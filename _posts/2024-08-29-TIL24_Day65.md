---
title: "Day65 ML Review - Ensemble Method (4)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, ensembleMethod, TIL_24]
toc: true 
---

# (Editing) Bagging: Basic Concepts & Code Implementation

<img src="/blog/images/2024-08-29-TIL24_Day65/BAD51C87-04C0-4138-A121-F8B29E7D989E-4967013.jpeg"><br><br>

>  In contrast to using the identical training dataset for fitting each classifier in the ensemble, we utilize **bootstrap samples**, <u>random samples drawn with replacements from the original training dataset</u>. This approach is why bagging is also referred to as **bootstrap aggregating.**

https://www.scaler.com/topics/machine-learning/bagging-in-machine-learning/

<center>
  <bR>
    <img src="/blog/images/2024-08-29-TIL24_Day65/image-20240911170350775.png" width="80%"><br><Br><font size=3pt><I>Image from: <a href="https://www.scaler.com/topics/machine-learning/bagging-in-machine-learning/">Scaler Topics "Bagging in Machine Learning"</a></I></font> <br><Br>
</center>




* Bagging, which stands for **Bootstrap Aggregating**, is a machine learning ensemble technique used to <u>improve the accuracy and reliability of predictive models</u>. It involves creating **multiple subsets of the training data** by using random sampling with replacement. These subsets train multiple base learners, such as decision trees, neural networks, or other models.

For example, seven training instances are randomly sampled in each bagging round as below.

<center><br>
  <img src="/blog/images/2024-08-29-TIL24_Day65/image-20240911171446023.png" width="70%"><br><BR>
</center>



The process involves creating **subsets**, known as bootstrap samples, for training a classifier. Usually, an unpruned decision tree is referred to as $C_j$. **In this method, each classifier is trained using a random subset of examples from the training dataset, labeled as Bagging Round 1, Bagging Round 2,** and so on. <u>It's crucial to understand that each subset may contain duplicate examples, and some original examples may not appear at all in a resampled dataset because of the way sampling with replacement works.</u> Once the individual classifiers are trained using these bootstrap samples, their predictions are combined using a majority voting mechanism. 

<br><br>



## Code Implementation

The scikit-learn library already includes the `BaggingClassifier` algorithm, which we can import from the ensemble submodule. In this case, we will use <u>an unpruned decision tree</u> as the base classifier and create an ensemble of 500 decision trees trained on different bootstrap samples of the training dataset.

```python
>>> from sklearn.ensemble import BaggingClassifier
>>> tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=None)
>>> bag = BaggingClassifier(base_estimator=tree, n_estimators=500,
                            max_samples=1.0, max_features=1.0,
                            bootstrap=True, bootstrap_features=False, n_jobs=1, random_state=1)
```

<br>

Let's compare the accuracy score of the prediction on both the training and test datasets to evaluate the performance of the bagging classifier against a single unpruned decision tree.

```python
>>> from sklearn.metrics import accuracy_score
>>> tree = tree.fit(X_train, y_train)
>>> y_train_pred = tree.predict(X_train)
>>> y_test_pred = tree.predict(X_test)
>>> tree_train = accuracy_score(y_train, y_train_pred)
>>> tree_test = accuracy_score(y_test, y_test_pred)
>>> print("Decision Tree Train/Test Accuracies %.3f/%.3f" % (tree_train, tree_test))
Decision Tree Train/Test Accuracies 1.000/0.833
```

<br>

The training accuracies of the decision tree and bagging classifier are both 100 percent on the training dataset. However, the bagging classifier demonstrates slightly better generalization performance as estimated on the test dataset. Next, compare the decision regions between the decision tree and the bagging classifier.



```python
>>> bag = bag.fit(X_train, y_train)
>>> y_train_pred = bag.predict(X_train)
>>> y_test_pred = bag.predict(X_test)
>>> bag_train = accuracy_score(y_train, y_train_pred)
>>> bag_test = accuracy_score(y_test, y_test_pred)
>>> print('Bagging train/test accuracies %.3f/%.3f' % (bag_train, bag_test))
Bagging train/test accuracies 1.000/0.917
```



Although the training accuracies of the decision tree and bagging 
