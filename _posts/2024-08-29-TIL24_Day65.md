---
title: "Day65 ML Review - Ensemble Method (4)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, ensembleMethod, TIL_24]
toc: true 
---

# (Editing) Bagging: Basic Concepts & Code Implementation

<img src="/blog/images/2024-08-29-TIL24_Day65/BAD51C87-04C0-4138-A121-F8B29E7D989E-4967013.jpeg"><br><br>

>  In contrast to using the identical training dataset for fitting each classifier in the ensemble, we utilize **bootstrap samples**, <u>random samples drawn with replacements from the original training dataset</u>. This approach is why bagging is also referred to as **bootstrap aggregating.**

https://www.scaler.com/topics/machine-learning/bagging-in-machine-learning/

<center>
  <bR>
    <img src="/blog/images/2024-08-29-TIL24_Day65/image-20240911170350775.png" width="80%"><br><Br><font size=3pt><I>Image from: <a href="https://www.scaler.com/topics/machine-learning/bagging-in-machine-learning/">Scaler Topics "Bagging in Machine Learning"</a></I></font> <br><Br>
</center>




* Bagging, which stands for Bootstrap Aggregating, is a machine learning ensemble technique used to <u>improve the accuracy and reliability of predictive models</u>. It involves creating **multiple subsets of the training data** by using random sampling with replacement. These subsets train multiple base learners, such as decision trees, neural networks, or other models.

For example, seven training instances are randomly sampled in each bagging round as below.

<center><br>
  <img src="/blog/images/2024-08-29-TIL24_Day65/image-20240911171446023.png" width="70%"><br><BR>
</center>



The process involves creating subsets, known as bootstrap samples, for training a classifier, usually an unpruned decision tree referred to as $C_j$. In this method, each classifier is trained using a random subset of examples from the training dataset, labeled as Bagging Round 1, Bagging Round 2, and so on. <u>It's important to understand that each subset may contain duplicate examples, and some original examples may not appear at all in a resampled dataset because of the way sampling with replacement works.</u> Once the individual classifiers are trained using these bootstrap samples, their predictions are combined using a majority voting mechanism. 

<br><br>



## Code Implementation

