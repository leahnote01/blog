---
title: "Day65 ML Review - Ensemble Method (4)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, ensembleMethod, TIL_24]
toc: true 
---

# Bagging: Basic Concepts & Code Implementation

<img src="/blog/images/2024-08-29-TIL24_Day65/BAD51C87-04C0-4138-A121-F8B29E7D989E-4967013.jpeg"><br><br>

>  In contrast to using the identical training dataset for fitting each classifier in the ensemble, we utilize **bootstrap samples**, <u>random samples drawn with replacements from the original training dataset</u>. This approach is why bagging is also referred to as **bootstrap aggregating.**

https://www.scaler.com/topics/machine-learning/bagging-in-machine-learning/

<center>
  <bR>
    <img src="/blog/images/2024-08-29-TIL24_Day65/image-20240911170350775.png" width="80%"><br><Br><font size=3pt><I>Image from: <a href="https://www.scaler.com/topics/machine-learning/bagging-in-machine-learning/">Scaler Topics "Bagging in Machine Learning"</a></I></font> <br><Br>
</center>




* Bagging, which stands for **Bootstrap Aggregating**, is a machine learning ensemble technique used to <u>improve the accuracy and reliability of predictive models</u>. It involves creating **multiple subsets of the training data** by using random sampling with replacement. These subsets train multiple base learners, such as decision trees, neural networks, or other models.



For example, seven training instances are randomly sampled in each bagging round as below.

<center><br>
  <img src="/blog/images/2024-08-29-TIL24_Day65/image-20240911171446023.png" width="70%"><br><BR>
</center>



The process entails training a classifier by generating subsets, termed bootstrap samples. Typically, an unpruned decision tree is denoted as $C_j$. In this approach, each classifier is trained on a random subset from the training dataset, labeled as <u>Bagging Round 1, Bagging Round 2, and so forth.</u> It's important to note that each subset can include duplicate examples, and some original examples might need to be present in the resampled dataset due to the nature of sampling with replacement. <u>After training the individual classifiers on these bootstrap samples, their predictions are aggregated using a majority voting system.</u>

<br><br>



## Code Implementation

The scikit-learn library already includes the `BaggingClassifier` algorithm, which we can import from the ensemble submodule. In this case, we will use <u>an unpruned decision tree</u> as the base classifier and create an ensemble of 500 decision trees trained on different bootstrap samples of the training dataset.

```python
>>> from sklearn.ensemble import BaggingClassifier
>>> tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=None)
>>> bag = BaggingClassifier(base_estimator=tree, n_estimators=500,
                            max_samples=1.0, max_features=1.0,
                            bootstrap=True, bootstrap_features=False, n_jobs=1, random_state=1)
```

<br>

We will assess the accuracy score of predictions on both the training and test datasets to evaluate how the bagging classifier performs compared to a single unpruned decision tree.

```python
>>> from sklearn.metrics import accuracy_score
>>> tree = tree.fit(X_train, y_train)
>>> y_train_pred = tree.predict(X_train)
>>> y_test_pred = tree.predict(X_test)
>>> tree_train = accuracy_score(y_train, y_train_pred)
>>> tree_test = accuracy_score(y_test, y_test_pred)
>>> print("Decision Tree Train/Test Accuracies %.3f/%.3f" % (tree_train, tree_test))
Decision Tree Train/Test Accuracies 1.000/0.833
```

<br>

The decision tree and bagging classifier achieve 100 percent training accuracy on the training dataset. Nevertheless, the bagging classifier shows marginally improved generalization performance on the test dataset. Next, let's compare the decision regions of the decision tree and the bagging classifier.<br><br>

```python
>>> bag = bag.fit(X_train, y_train)
>>> y_train_pred = bag.predict(X_train)
>>> y_test_pred = bag.predict(X_test)
>>> bag_train = accuracy_score(y_train, y_train_pred)
>>> bag_test = accuracy_score(y_test, y_test_pred)
>>> print('Bagging train/test accuracies %.3f/%.3f' % (bag_train, bag_test))
Bagging train/test accuracies 1.000/0.917
```

<br>

While both the decision tree and bagging classifier achieve identical training accuracies (100 percent each) on the training dataset, the bagging classifier demonstrates slightly superior generalization performance when evaluated on the test dataset.

<u>Complex classification tasks</u> and a dataset's high dimensionality can quickly result in overfitting with single decision trees. This is <u>where the bagging algorithm demonstrates its strengths.</u> However, bagging **does not effectively reduce model bias**, which occurs when models are overly simplistic and fail to capture data trends accurately. Therefore, we aim to implement bagging on a group of classifiers with low bias, such as unpruned decision trees.



