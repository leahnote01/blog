---
title: "Day95 Deep Learning Lecture Review - Lecture 13 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, CLIP, ResNet, SBERT, fine-tuning, prompt engineering, TIL_24]
toc: true 
---

# (Editing) Llama 3: Post-Training

![73103FEF-CED1-4907-9AA3-1304DBAE6BA1](/images/2024-10-31-TIL24_Day95_DL/73103FEF-CED1-4907-9AA3-1304DBAE6BA1.jpeg)



### Post-Training

![image-20241031132038480](/images/2024-10-31-TIL24_Day95_DL/image-20241031132038480.png)

The image outlines the post-training stages for Llama 3, which refine the model **for better performance and alignment with human preferences**. Hereâ€™s a brief explanation of each stage:

1. **Reward Modeling**:
   - **Data**: Uses human-annotated preference data, where responses are labeled based on quality or preference.
   - **Algorithm**: A binary classification task where the model learns to distinguish between preferred and less preferred responses.
   - **Model**: The output is a ***Reward Model (RM)*,** initialized from the base model, <u>which scores responses based on quality.</u>
2. **Supervised Fine Tuning (SFT)**:
   - **Data**: Uses prompt/response pairs to demonstrate ideal responses.
   - **Algorithm**: The model is trained to predict the next token within the response context, focusing on generating high-quality responses.
   - **Model**: An *SFT model*, initialized from the base model, which is fine-tuned to respond accurately to specific prompts.
3. **Direct Preference Optimization (DPO)**:
   - **Data**: Uses triplets consisting of a prompt, a good response, and a bad response.
   - **Algorithm**: The model is trained to maximize rewards by learning to prefer good responses over bad ones, using DPO (Direct Preference Optimization).
   - **Model**: This results in an *Instruct model*, initialized from the SFT model, which is aligned to follow user instructions better.

These stages help Llama 3 generate responses that are high-quality, aligned with human preferences, and contextually relevant.
