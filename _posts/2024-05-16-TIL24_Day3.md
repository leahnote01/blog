---
title: "Day03 ML Statistics Review"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statreview, TIL_24]
toc: true
---

# Basic Mathematics Concepts (3) - Eigendecomposition, Symmetric Matrix and Eigendecomposition

<img src="/blog/images/2024-05-16-TIL24_Day3/360F2DDF-9B34-4EE4-8A79-D4802F54D179.jpeg" alt="day03in">



*(Korean Explanation from https://darkpgmr.tistory.com/105)*

#### Eigendecomposition

> Eigenvalue decomposition is a mathematical process used in linear algebra to decompose <u>a square matrix into its constituent parts.</u> This process reveals many of the matrix's properties, such as whether it can be inverted, its determinant, and its rank. 

Eigendecomposition plays a crucial role in engineering, physics, and data science. It's a key tool in PCA(Principal Component Analysis), linear transformations, and systems of differential equations. 

<br>

If a square matrix $A$ of size $n \times n$ has $n$ **(1) linearly independent** **(2) eigenvectors**, then $A$ can be decomposed into $A= PDP^{-1}$ where $P$ is the matrix formed by placing the <u>eigenvectors as its columns</u>, $D$ is a diagonal matrix whose <u>diagonal elements are the corresponding eigenvalues</u>, and $P^{-1}$ is the inverse of the matrix $P$.

<br>

###### *Reference in Korean*

즉, 행렬 A의 고유벡터eigenvectors들을 열벡터로 하는 행렬을 P, 고유값eigenvalues들을 대각원소diagonal elements 하는 대각행렬diagonal matrix을 $D$라 하면 다음 식이 성립한다. <br>
$$
AP = PD \\
A = PDP^{-1}
$$

<br>이와 같이 행렬 A는 자신의 고유벡터들을 열벡터로 하는 행렬과 고유값을 대각원소로 하는 행렬의 곱으로 대각화 분해가 가능한데, 이러한 대각화 분해를 eigendecomposition 이라고 한다. 행렬 A의 eigendecomposition을 알면 행렬식 값 $det(A)$, A의 거듭제곱, 역행렬, 대각합(trace), 행렬의 다항식 등을 매우 손쉽게 계산할 수 있다.)

<br><br>

#### **Linear (in)dependence**

- Linear independence: No vector is a linear combination of the other vectors
- Common cause: One of the vectors is a <u>null vector</u>, or the vectors are <u>perpendicular to each other</u> <br>

(**어떤 벡터도 다른 벡터들의 상수배 합으로 표현될 수 없으면** 이 벡터들은 <u>서로 일차독립(linearly independent)이라고 한다</u>. 주의사항: 어떤 행렬에 대해 고유값은 유일하게 결정되지만 고유벡터는 유일하지 않다. 따라서 고유벡터는 몇가지 제약조건을 만족하는 벡터들 중에서 어느 벡터를 사용해도 무방하나 보통은 벡터의 크기를 1로 정규화한(normalized) 단위벡터를 고유벡터로 잡는 것이 일반적이다.)

<br><br>

#### **Symmetric Matrix and Eigendecomposition**

A symmetric matrix i<u>s a square matrix equal to its transpose</u> $A = PDP^{-1}$​. The eigenvalue decomposition of symmetric matrices has some special properties that make it useful in practical applications. 
$$
A = PDP^{-1}  \\
  = PDP^{T}\\
for \ PP^{T} = E \ (P^{-1}=P^{T})
$$




- Properties of Symmetric Matrices in Eigendecomposition
  
  - **Real Eigenvalues**: The eigenvalues of a symmetric matrix are always <u>real numbers</u>, even though the matrix might contain complex numbers. This property is particularly useful because it simplifies many problems in physics and engineering where real solutions are required.
  
    <br>
  
  - **Orthogonal** **Eigenvectors**: For any pair of eigenvectors $v_i$ and $v_j$ from a symmetric matrix corresponding to different eigenvalues, the eigenvectors are orthogonal. That is, **the dot product** $v_i \times v_j = 0$ for $i \neq j$​.  When eigenvectors are orthogonal, they lie 90 degrees to each other in the vector space. 
    
    - Dimensionality: Each eigenvector defines <u>a unique dimension in the space.</u> The fact that these dimensions are orthogonal ensures that they are **independent of one another**. This is a highly desirable property in many applications, like PCA, where we want to capture independent directions of variance.
    - Simplification: Orthogonal directions simplify computation and reduce numerical errors in calculations, especially <u>*transformations and projections in high-dimensional spaces.*</u>
    
    <br>
    
  - **Orthogonal Matrix**: For a symmetric matrix $A$, if $P$ is the matrix whose columns are the eigenvectors of $A$, then $P$ is an orthogonal matrix. An orthogonal matrix has a special property: $P^T = P^{-1}$. This equality implies that transposing the matrix $P$​ is equivalent to inverting it.

<br><br>

###### *Reference in Korean*

먼저, 벡터에 대해 얘기를 해 보면 두 벡터 $v_1$, $v_2$가 서로 수직이면(즉, $v_1·v_2=0$) 두 벡터 $v_1$, $v_2$는 서로 orthogonal 하다고 한다. 그리고 $v^{'}$ = $\frac{v} {\Vert v \Vert}$와 같이 어떤 벡터를 크기가 1인 단위벡터로 만드는 것을 정규화(normalization)라고 한다. orthonormal이라는 말은 orthogonal과 normal이 합쳐진 말로서 두 벡터 $v_1$, $v_2$가 모두 단위벡터(unit vector)이면서 $v_1$, $v_2$는 orthonormal(정규직교)하다고 한다.

orthogonal: $v_1·v_2= 0$

orthonormal: $v_1·v_2 = 0$  &  $\lVert v_1 \rVert =1$, $\Vert v_2 \Vert = 1$



즉, orthogonal, orthonormal은 벡터들 사이의 관계를 나타내는 말인데, 이게 행렬로 넘어가면 조금 의미가 달라진다.



직교행렬(orthogonal matrix)의 수학적 정의는 자신의 전치행렬(transpose)를 역행렬로 갖는 정방행렬이다.  

$$A^{-1} = A^T$$

이와 같이 직교행렬(orthogonal matrix)은 transpose를 시키면(행렬의 열과 행 원소들을 서로 바꾸면) 자신의 역행렬이 되기 때문에 다양한 선형대수학 계산에서 매우 편리한 성질을 가진 행렬이다.



그런데, 직교행렬의 열벡터들은 서로 orthonomal(정규직교)한 성질을 가지고 있다. 즉, 직교 행렬를 구성하는 열벡터들을 $v_1, v_2, .., v_n$이라 했을 때 이들은 모두 **단위벡터(unit vector)이면서 또한 서로 서로 수직인 성질을 갖는다**. 이러한 성질은 열벡터가 아닌 행벡터들에 대해서도 동일하게 성립한다 (즉, 행벡터들도 서로 orthonormal 하다).



즉, 직교행렬(orthogonal matrix)은 그 행렬을 구성하는 열벡터(행벡터)들이 서로 수직(orthogonal)이면서 크기가 1인 (normal한) 행렬로도 정의될 수 있다.



<img src="/blog/images/2024-05-16-TIL24_Day3/63841F63-292D-47F9-A914-317B42FE3350.jpeg" alt="day03out">

