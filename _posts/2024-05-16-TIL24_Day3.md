---
title: "Day03 ML Statistics Review"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statreview, TIL_24]
toc: true
---

# Basic Mathematics Concepts - Eigendecomposition, Symmetric Matrix and Eigendecomposition

<img src="/blog/images/2024-05-16-TIL24_Day3/360F2DDF-9B34-4EE4-8A79-D4802F54D179.jpeg" alt="day03">

#### Eigendecomposition

> Eigenvalue decomposition is a mathematical process used in linear algebra to decompose a square matrix into its constituent parts. This process reveals many of the matrix's properties, such as whether it can be inverted, its determinant, and its rank. 

Eigendecomposition is particularly important in many areas of engineering, physics, and data science, especially in analyses involving PCA(Principal Component Analysis), linear transformations, and systems of differential equations. \(A = B\)

<img src = "/blog/images/2024-05-16-TIL24_Day3/image-20240516160815690.png" alt="explanation">

<img src = "/blog/images/2024-05-16-TIL24_Day3/image-20240516162524184.png" alt="kor_explanation">

![image-20240516162524184](/images/2024-05-16-TIL24_Day3/image-20240516162524184.png)

(이와 같이 행렬 A는 자신의 고유벡터들을 열벡터로 하는 행렬과 고유값을 대각원소로 하는 행렬의 곱으로 대각화 분해가 가능한데, 이러한 대각화 분해를 eigendecomposition 이라고 한다. 행렬 A의 eigendecomposition을 알면 행렬식 값 det(A), A의 거듭제곱, 역행렬, 대각합(trace), 행렬의 다항식 등을 매우 손쉽게 계산할 수 있다.)



#### **Linear (in)dependence**

- Linear independence: No vector is a linear combination of the other vectors
- Common cause: One of the vectors is a <u>null vector</u>, or the vectors are <u>perpendicular to each other</u>

(어떤 벡터도 다른 벡터들의 상수배 합으로 표현될 수 없으면 이 벡터들은 서로 일차독립(linearly independent)이라고 한다. 주의사항: 어떤 행렬에 대해 고유값은 유일하게 결정되지만 고유벡터는 유일하지 않다. 따라서 고유벡터는 몇가지 제약조건을 만족하는 벡터들 중에서 어느 벡터를 사용해도 무방하나 보통은 벡터의 크기를 1로 정규화한(normalized) 단위벡터를 고유벡터로 잡는 것이 일반적이다.)



#### **Symmetric Matrix and Eigendecomposition**

A symmetric matrix is a square matrix equal to its transpose![equation.pdf](blob:file:///068a3423-df10-4b77-aeda-cbe1607d5374). The eigenvalue decomposition of symmetric matrices has some special properties that make it useful in practical applications. 





















As we confirm the above equation, all symmetric matrices can do Eigendecomposition with the orthogonal matrices. 



- Properties of Symmetric Matrices in Eigendecomposition

  - **Real Eigenvalues**: The eigenvalues of a symmetric matrix are always real numbers, even though the matrix might contain complex numbers. This property is particularly useful because it simplifies many problems in physics and engineering where real solutions are required.

  - **Orthogonal** **Eigenvectors**: For any pair of eigenvectors ![equation_1.pdf](blob:file:///db091643-3f7b-48fd-99c9-b293cfeb16d3) and ![equation_2.pdf](blob:file:///b249ac72-7e73-4d29-9b7f-b3cbd0dfa233) from a symmetric matrix corresponding to different eigenvalues, the eigenvectors are orthogonal. That is, **the dot product** **![equation_3.pdf](blob:file:///0247857a-c9a4-4133-b1f5-2f9b7c59e5c4)** **for** **![equation_4.pdf](blob:file:///a62840a3-43f3-41e3-9f59-fc7344fef6af)**. This orthogonality property holds for eigenvectors corresponding to distinct eigenvalues. When eigenvectors are orthogonal, it means they lie at 90 degrees to each other in the vector space

    - **Dimensionality**: Each eigenvector defines a unique dimension in the space. The fact that these dimensions are orthogonal ensures that they are independent of one another. This is a highly desirable property in many applications, like PCA, where we want to capture independent directions of variance. 

    - **Simplification**: Orthogonal directions simplify computation and reduce numerical errors in calculations, especially *transformations and projections in high-dimensional spaces*.

  - **Orthogonal Matrix ![equation_5.pdf](blob:file:///26a4b18f-1ea1-44d7-92ac-472d1338c709)**: For a symmetric matrix ![equation_6.pdf](blob:file:///44704ab1-df9f-4469-8a6c-1fa9ea5f9dc3), if **![equation_7.pdf](blob:file:///cb91b250-39ab-45f9-a53e-e6898f7599a3)** is the matrix whose columns are the eigenvectors of ![equation_8.pdf](blob:file:///f70e7f0a-a314-4531-8436-9d2226e6e549), then **![equation_9.pdf](blob:file:///e0c476d0-9a4d-4a9c-b8fe-ceabcdb00cc7)** is an orthogonal matrix. An orthogonal matrix has a special property: ![equation_10.pdf](blob:file:///8d6cf34d-f944-40e6-9d7f-bd5e5d20eb6f). This equality implies that transposing the matrix ![equation_11.pdf](blob:file:///b18b1f22-f51a-44c5-9d5d-4fa47ae5f951) is equivalent to inverting it. 





\* 참고사항 Reference in Korean

먼저, 벡터에 대해 얘기를 해 보면 두 벡터 v1, v2가 서로 수직이면(즉, v1·v2 = 0) 두 벡터 v1, v2는 서로 orthogonal 하다고 한다. 그리고 v' = v/∥v∥와 같이 어떤 벡터를 크기가 1인 단위벡터로 만드는 것을 정규화(normalization)라고 한다. orthonormal이라는 말은 orthogonal과 normal이 합쳐진 말로서 두 벡터 v1, v2가 모두 단위벡터(unit vector)이면서 서로 수직이면 두 벡터 v1, v2는 orthonormal(정규직교)하다고 한다.

orthogonal: v1·v2 = 0

orthonormal: v1·v2 = 0  &  ∥v1∥ = 1, ∥v2∥ = 1



즉, orthogonal, orthonormal은 벡터들 사이의 관계를 나타내는 말인데, 이게 행렬로 넘어가면 조금 의미가 달라진다.



직교행렬(orthogonal matrix)의 수학적 정의는 자신의 전치행렬(transpose)를 역행렬로 갖는 정방행렬이다. 

![equation_11.pdf](blob:file:///7869a633-80d2-4bb6-a638-09800ed9d863)



이와 같이 직교행렬(orthogonal matrix)은 transpose를 시키면(행렬의 열과 행 원소들을 서로 바꾸면) 자신의 역행렬이 되기 때문에 다양한 선형대수학 계산에서 매우 편리한 성질을 가진 행렬이다.



그런데, 직교행렬의 열벡터들은 서로 orthonomal(정규직교)한 성질을 가지고 있다. 즉, 직교 행렬를 구성하는 열벡터들을 v1, v2, ..., vn이라 했을 때 이들은 모두 단위벡터(unit vector)이면서 또한 서로 서로 수직인 성질을 갖는다. 이러한 성질은 열벡터가 아닌 행벡터들에 대해서도 동일하게 성립한다 (즉, 행벡터들도 서로 orthonormal 하다).



즉, 직교행렬(orthogonal matrix)은 그 행렬을 구성하는 열벡터(행벡터)들이 서로 수직(orthogonal)이면서 크기가 1인 (normal한) 행렬로도 정의될 수 있다.

