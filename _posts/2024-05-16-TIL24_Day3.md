---
title: "Day03 ML Statistics Review"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statreview, TIL_24]
toc: true
---

# Basic Mathematics Concepts - Eigendecomposition, Symmetric Matrix and Eigendecomposition

<img src="/blog/images/2024-05-16-TIL24_Day3/360F2DDF-9B34-4EE4-8A79-D4802F54D179.jpeg" alt="day03">

#### Eigendecomposition

> Eigenvalue decomposition is a mathematical process used in linear algebra to decompose a square matrix into its constituent parts. This process reveals many of the matrix's properties, such as whether it can be inverted, its determinant, and its rank. 

Eigendecomposition is particularly important in many areas of engineering, physics, and data science, especially in analyses involving PCA(Principal Component Analysis), linear transformations, and systems of differential equations. 



<img src = "/blog/images/2024-05-16-TIL24_Day3/image-20240516160815690.png" alt="explanation">

<img src = "/blog/images/2024-05-16-TIL24_Day3/image-20240516162524184.png" alt="kor_explanation">

![image-20240516162524184](/images/2024-05-16-TIL24_Day3/image-20240516162524184.png)

(이와 같이 행렬 A는 자신의 고유벡터들을 열벡터로 하는 행렬과 고유값을 대각원소로 하는 행렬의 곱으로 대각화 분해가 가능한데, 이러한 대각화 분해를 eigendecomposition 이라고 한다. 행렬 A의 eigendecomposition을 알면 행렬식 값 $det(A)$, A의 거듭제곱, 역행렬, 대각합(trace), 행렬의 다항식 등을 매우 손쉽게 계산할 수 있다.)



#### **Linear (in)dependence**

- Linear independence: No vector is a linear combination of the other vectors
- Common cause: One of the vectors is a <u>null vector</u>, or the vectors are <u>perpendicular to each other</u>

(**어떤 벡터도 다른 벡터들의 상수배 합으로 표현될 수 없으면** 이 벡터들은 <u>서로 일차독립(linearly independent)이라고 한다</u>. 주의사항: 어떤 행렬에 대해 고유값은 유일하게 결정되지만 고유벡터는 유일하지 않다. 따라서 고유벡터는 몇가지 제약조건을 만족하는 벡터들 중에서 어느 벡터를 사용해도 무방하나 보통은 벡터의 크기를 1로 정규화한(normalized) 단위벡터를 고유벡터로 잡는 것이 일반적이다.)



#### **Symmetric Matrix and Eigendecomposition**

A symmetric matrix i<u>s a square matrix equal to its transpose</u> $A = PDP^{-1}$. The eigenvalue decomposition of symmetric matrices has some special properties that make it useful in practical applications. 

![image-20240516205911117](/images/2024-05-16-TIL24_Day3/image-20240516205911117.png)



- Properties of Symmetric Matrices in Eigendecomposition

  - **Real Eigenvalues**: The eigenvalues of a symmetric matrix are always <u>real numbers</u>, even though the matrix might contain complex numbers. This property is particularly useful because it simplifies many problems in physics and engineering where real solutions are required.

  - **Orthogonal** **Eigenvectors**: For any pair of eigenvectors $v_i$ and $v_j$ from a symmetric matrix corresponding to different eigenvalues, the eigenvectors are orthogonal. That is, **the dot product** $v_i \times v_j = 0$ for $i \neq j$​.  When eigenvectors are orthogonal, they lie 90 degrees to each other in the vector space. 

    - Dimensionality: Each eigenvector defines <u>a unique dimension in the space.</u> The fact that these dimensions are orthogonal ensures that they are **independent of one another**. This is a highly desirable property in many applications, like PCA, where we want to capture independent directions of variance.
    - Simplification: Orthogonal directions simplify computation and reduce numerical errors in calculations, especially <u>*transformations and projections in high-dimensional spaces.*</u>

    









