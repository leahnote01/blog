---
title: "Day44 ML Review - K-Nearest Neighbors "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,kNearestNeighbors,TIL_24]
toc: true 
---

# Basic Concepts

<img src="/blog/images/2024-08-01-TIL24_Day44/IMG_1430.JPG">

> The K-Nearest Neighbor (KNN) classifier is especially interesting because it is fundamentally different from the learning algorithms we've discussed so far. KNN is a typical example of a "*lazy learner*." It is called "lazy" not because of its apparent simplicity but because it **doesn't learn** a discriminative **function** from the training data; instead, **it memorizes the training dataset.**







<Br>



#### Side note

In machine learning, algorithms can be broadly classified into **parametric** and **nonparametric** models based on their approaches to learning and prediction.

**Parametric** models are designed <u>to estimate parameters from the training dataset to learn a function that can classify new data points without relying on the original training data.</u> Once the parameters are estimated, the original training dataset is no longer needed for classifying new data. Examples of parametric models include the perceptron, logistic regression, and the linear Support Vector Machine (SVM).

In contrast, <u><b>nonparametric</b> models do not adhere to a fixed set of parameters, and the number of parameters grows with the training data.</u> Two examples of nonparametric models that we have encountered are the decision tree classifier/random forest and the kernel SVM.

K Nearest Neighbors (KNN) falls into a subcategory of nonparametric models known <u>as instance-based learning.</u> In this approach, the model memorizes the training dataset, and lazy learning is a special case of instance-based learning that incurs zero cost during the learning process.



<br><br>

