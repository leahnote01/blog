---
title: "Day44 ML Review - K-Nearest Neighbors "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,kNearestNeighbors,TIL_24]
toc: true 
---

# Basic Concepts

<img src="/blog/images/2024-08-01-TIL24_Day44/IMG_1430.JPG">

> The K-Nearest Neighbor (KNN) classifier is especially interesting because it is fundamentally different from the learning algorithms we've discussed so far. KNN is a typical example of a "*lazy learner*." It is called "lazy" not because of its apparent simplicity but because it **doesn't learn** a discriminative **function** from the training data; instead, **it memorizes the training dataset.**











#### Side note

In the field of machine learning, algorithms can be broadly classified into **parametric** and **nonparametric** models based on their approaches to learning and prediction.

Parametric models are designed to estimate parameters from the training dataset in order to learn a function that can classify new data points without relying on the original training data. This means that once the parameters are estimated, the original training dataset is no longer needed for classifying new data. Common examples of parametric models include the perceptron, logistic regression, and the linear Support Vector Machine (SVM).

In contrast, nonparametric models do not adhere to a fixed set of parameters, and the number of parameters grows with the training data. Two examples of nonparametric models that we have encountered are the decision tree classifier/random forest and the kernel SVM.

K Nearest Neighbors (KNN) falls into a subcategory of nonparametric models known as instance-based learning. In this approach, the model memorizes the training dataset, and lazy learning is a special case of instance-based learning that incurs zero cost during the learning process.
