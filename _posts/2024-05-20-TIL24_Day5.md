---
title: "Day05 ML Review - Principle Component Analysis PCA (1)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlreview, TIL_24, PCA]
toc: true
---

# PCA Mathematics & Algorithms 

<img src="/blog/images/2024-05-20-TIL24_Day5/888160AF-DBAC-4787-964C-EE0752BA4D59.jpeg" alt="day05in" width="80%">

<br>

>PCA is a method for **analyzing the main components of a distribution** formed by multiple data sets rather than analyzing all of the components of each piece of data individually. It is widely used for dimensionality reduction in Machine Learning.

**All explanation is mainly from The Dark Programer’s blog (**[**https://darkpgmr.tistory.com/110**](https://darkpgmr.tistory.com/110)**)**

<img src="/blog/images/2024-05-20-TIL24_Day5/image-20240521180649324.png" alt="01" width="80%">

The best way to describe the distribution characteristics of these datasets is to explain them using two vectors, **e1 and e2**, as shown in the figure. Knowing the ***direction and size of e1 and e2*** is the simplest and most effective way to understand this data distribution. <br>

The main component refers to **the direction vector** where the data's variance is most significant. In Figure 1, the data's dispersion, or degree of dispersion, is greatest along the e1 direction. The direction perpendicular to e1 and with the subsequent greatest dispersion of data is e2.

<br>



#### Straight-line approximation in 2-dimensional area

<img src="/blog/images/2024-05-20-TIL24_Day5/image-20240521194246138.png" alt="02" width="80%">

The PCA method for approximating a straight line involves <u>deriving a line parallel to the first principal component vector</u> from PCA **while passing through the average position of the data**.



The straight line obtained from the PCA method differs from the one obtained from the Least Squares (LS) Method. The LS method minimizes the distance between a straight line and the data, while the PCA method identifies the direction in which the data has the greatest variance.



In the figure above, the LS approximation $y=ax+b$ minimizes the $y-axis$ distance from the straight line, and the other LS approximation $ax+by+c=0$ minimizes the $z-axis$ distance from plane $z=ax+by+c$. We need to select a straight line that **maximizes** the **variance** of the data among the lines that *pass through the data's average point* <u>when the data is projected.</u>



#### PCA Mathematics

We need to determine the Covariance and Covariance Matrix before we can examine the detailed mathematics of the PCA. 

- **Covariance**: Covariance measures the joint variability of two random variables. With the two variables, $x$ & $y$, $cov(X, Y)$​​ is as below.
  
  
  $$
  cov(X,Y) = E[(X-E[X])(Y-E[Y])]
  $$
  <br>
  
  For two jointly distributed real-valued random variables $X$ and $Y$, the covariance is defined as the expected value (or mean) of the product of their derivations from their individual expected values.
   (https://en.wikipedia.org/wiki/Covariance)
  
- **Covariance Matrix**: A covariance matrix is a square matrix giving the covariance between each pair of elements of a given random vector. Any covariance matrix is <u>symmetric</u> and <u>positive semi-definite,</u> and its main diagonal contains <u>variances</u>.

  
  $$
  C = \begin{pmatrix}
  \text{cov}(x,x) & \text{cov}(x,y) \\
  \text{cov}(x,y) & \text{cov}(y,y)
  \end{pmatrix} \\
  $$

  $$
  = \begin{pmatrix}
  \frac{1}{n} \sum (x_i - m_x)^2 & \frac{1}{n} \sum (x_i - m_x)(y_i - m_y) \\
  \frac{1}{n} \sum (x_i - m_x)(y_i - m_y) & \frac{1}{n} \sum (y_i
   - m_y)^2
  \end{pmatrix} \\
  $$

  <br>
  PCA can be viewed as the process of eigendecomposition of the covariance matrix of input data into its eigenvectors and eigenvalues.



#### How the PCA works (e.g., in two dimensions)

1. **Standardize** the data:

   Subtract the mean of each feature in the dataset from all data points and then divide by the standard deviation. This ensures that each feature contributes equally to the analysis.<br>

2. Compte the **covariance matrix**:

   For instance, for two dimensions, the covariance matrix $C$ is a $2 \times 2$ matrix where each element represents the covariance between two variables. The diagonal elements represent the variance of each variable, and the off-diagonal elements are the covariance as above.<br>

3. 





<img src="/blog/images/2024-05-20-TIL24_Day5/FBA913B0-2E9B-4D6F-961F-D7426A107AAE.jpeg" alt="day05out" width="80%">

