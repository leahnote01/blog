---
title: "Day05 ML Review - Principle Component Analysis PCA (1)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlreview, TIL_24, PCA]
toc: true
---

# PCA Mathematics & Algorithms 

<img src="/blog/images/2024-05-20-TIL24_Day5/888160AF-DBAC-4787-964C-EE0752BA4D59.jpeg" alt="day05in" width="50%">

<br>

>PCA is a method for **analyzing the main components of a distribution** formed by multiple data sets rather than analyzing all of the components of each piece of data individually. It is widely used for dimensionality reduction in Machine Learning.

**All explanation is mainly from The Dark Programerâ€™s blog (**[**https://darkpgmr.tistory.com/110**](https://darkpgmr.tistory.com/110)**)**

![image-20240521180649324](/images/2024-05-20-TIL24_Day5/image-20240521180649324.png)

The best way to describe the distribution characteristics of these datasets is to explain them using two vectors, **e1 and e2**, as shown in the figure. Knowing the ***direction and size of e1 and e2*** is the simplest and most effective way to understand this data distribution. <br>

The main component refers to **the direction vector** where the data's variance is most significant. In Figure 1, the data's dispersion, or degree of dispersion, is greatest along the e1 direction. The direction perpendicular to e1 and with the subsequent greatest dispersion of data is e2.

<br>



#### Straight-line approximation in 2-dimensional area

![image-20240521194246138](/images/2024-05-20-TIL24_Day5/image-20240521194246138.png)

The PCA method for approximating a straight line involves <u>deriving a line parallel to the first principal component vector</u> from PCA **while passing through the average position of the data**.



The straight line obtained from the PCA method differs from the one obtained from the Least Squares (LS) Method. The LS method minimizes the distance between a straight line and the data, while the PCA method identifies the direction in which the data has the greatest variance.



In the figure above, the LS approximation $y=ax+b$ minimizes the $y-axis$ distance from the straight line, and the other LS approximation $ax+by+c=0$ minimizes the $z-axis$ distance from plane $z=ax+by+c$. We need to select a straight line that **maximizes** the **variance** of the data among the lines that *pass through the data's average point* <u>when the data is projected.</u>



#### PCA Mathematics

We need to determine the Covariance and Covariance Matrix before we can examine the detailed mathematics of the PCA. 

- Covariance: Covariance measures the joint variability of two random variables. With the two variables, $x$ & $y$, $cov(X, Y)$ is as below.
  



<img src="/blog/images/2024-05-20-TIL24_Day5/FBA913B0-2E9B-4D6F-961F-D7426A107AAE.jpeg" alt="day05out" width="50%">

