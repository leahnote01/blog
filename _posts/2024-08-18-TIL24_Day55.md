---
title: "Day55 ML Review - Model Evaluation "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, modelEvaluation, TIL_24]
toc: true 
---

# Model Evaluation and HyperParameter Tuning

<img src="/blog/images/2024-08-18-TIL24_Day55/230B0EC3-D77D-4768-B30F-7AC2D3A2A5CE_1_105_c.jpeg">

> When dealing with several datasets, utilizing the parameters acquired from fitting the training data to standardize and compress fresh data is crucial. This process entails implementing these parameters to instances in a distinct test dataset. The scikit-learn `Pipeline` class enables us to train a model containing any number of transformation stages and use it to generate predictions for new data.



### Loading the Dataset

We will begin by implementing the Breast Cancer Wisconsin dataset from the UCI website directly and incorporating it into a `Pipeline`.

```python
import pandas as pd
df = pd.read_csv('https://archive.ics.uci.edu/ml/' 'machine-learning-databases' '/breast-cancer-wisconsin/wdbc.data', header=None)
```

<Br>

First, we will collect the 30 features and organize them into a NumPy array called `X`. Next, we will use a LabelEncoder object to convert the class labels, originally represented as `'M'` and ``'B,'`` into integer values. After encoding the class labels (diagnosis) into an array called `y`, the malignant tumors (`'M'`) will be represented as class `1`, and the benign tumors will be represented as class `0`.

```python
from sklearn.preprocessing import LabelEncoder
X = df.loc[:, 2:].values
y = df.loc[:, 1].values
le = LabelEncoder()
y = le.fit_transformer(y)
le.classes_array(['B', 'M'], dtype=object)
```

<br>

Start with dividing the dataset into a separate training dataset(80 percent of the data) and a separate test dataset(20 percent of the data).

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
```

<Br>

### Combining Transformers and Estimators in a Pipeline

The features in the Breast Cancer Wisconsin dataset are measured using different scales. It's important to **standardize** the columns before using them in a linear classifier like logistic regression. Additionally, suppose we aim to **reduce the dimensionality** of the data from the original 30 dimensions to a two-dimensional subspace using principal component analysis (PCA).

Instead of going through the model fitting and data transformation steps for the training and test datasets separately, we can chain the `Standard-Scaler`, `PCA`, and `LogisticRegression` objects in a pipeline.

```python
```







<br><br>

