---
title: "Day32 ML Review - Support Vector Machine (1)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,supportVectorMachine,TIL_24]
toc: true 
---

# Basic Concepts & Mathematical Formulations of Support Vector Machine

> Another powerful and widely used learning algorithm is the **support vector machine (SVM)**, which can be considered an extension of the perceptron. 

<img src="/blog/images/2024-07-23-TIL24_Day32/DD8DFD26-A0CC-4E5B-A18A-8332363758E6_1_105_c.jpeg">

<br><br>

## Basic Concepts

Using a support vector machine, <u>we aim to maximize the margin</u>, which is the distance between the separating hyperplane (decision boundary) and the training examples closest to this hyperplane, known as **support vectors**.

#### 1. Hyperplane and decision boundary

- In a binary classification problem, SVM aims to find the optimal hyperplane that separates the data points of different classes.
- A hyperplane in an $n$-dimensional space is a flat affine subspace of dimension $n-1$.  (For a 2D space, a hyperplane is a line; for a 3D space, it is a plane. )



#### 2. Support Vectors

- Support vectors are <u>the data points closest to the hyperplane and influence its position and orientation</u>; they lie on the edge of the margin.
- The optimal hyperplane <u>maximizes the margin</u>, which is the distance between it and the nearest support vectors from both classes. 

<br>

## Mathematical Formulation - Maximum Margin Intuition

The rationale for having decision boundaries with large margins is that they are likely to have lower generalization errors, while models with small margins are more prone to overfitting.

When we see those positive and negative hyperplanes that are parallel to the decision boundary, which can be expressed as follows:

<center>
  $$
  \begin{align}
w_0 + \mathbf{w}^T \mathbf{x}_{\text{pos}} &= 1 \quad (1) \\
w_0 + \mathbf{w}^T \mathbf{x}_{\text{neg}} &= -1 \quad (2)
\end{align}
  $$
</center>
<br>

If we subtract those two linear equations (1) and (2) from each other, we get:

<center>
  $$
  \Rightarrow \mathbf{w}^T (\mathbf{x}_{\text{pos}} - \mathbf{x}_{\text{neg}}) = 2
  $$
</center>

<br>

We can normalize this equation by the length of the vector $w$, which is defined as follows: <br>

<center>
  $$
    \|\mathbf{w}\| = \sqrt{\sum_{j=1}^m w_j^2}
  $$
</center>

<br>

So, we delivered the following question:

<center>
  $$
  \frac{\mathbf{w}^T (\mathbf{x}_{\text{pos}} - \mathbf{x}_{\text{neg}})}{\|\mathbf{w}\|} = \frac{2}{\|\mathbf{w}\|}
  $$
</center>

<br>

The left side of <u>the preceding equation represents the distance between the positive and negative hyperplane</u>, known as the margin, which we aim to **maximize**.

<br>

Now, the objective function of the SVM becomes the maximization of this margin by maximizing $\frac{2}{\Vert \mathbf{w} \rVert}$ under the constraint that the examples are classified correctly, which can be written as:

<br>

<center>
  $$
   \begin{cases}
w_0 + \mathbf{w}^T \mathbf{x}^{(i)} \geq 1 & \text{if } y^{(i)} = 1 \\
w_0 + \mathbf{w}^T \mathbf{x}^{(i)} \leq -1 & \text{if } y^{(i)} = -1
\end{cases}
  $$
</center>

<br>

<center>
  $$
    \quad \text{for } i = 1, \ldots, N
  $$
</center>

<br>$N$ is the number of examples in our dataset.

In practice, **it is easier to minimize the reciprocal term, $\frac{1}{2}\Vert W \Vert^2$.**

<br><br>
