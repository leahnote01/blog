---
title: "Day56 ML Review - Cross Validation "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, kFold, TIL_24]
toc: true 
---

# Using K-fold Cross Validation to Assess Model Performance

<img src="/blog/images/2024-08-19-TIL24_Day56/A77B832A-568D-4275-8B7F-FFA21E129330.jpeg"><br><br>

> When constructing a machine learning model, evaluating its performance based on data on which it has yet to be trained is essential. For example, if we train our model using a particular dataset, <u>we need to assess its performance when presented with new data that it has yet to see.</u>

As we've discussed, a model can suffer from underfitting (high bias) if it's too simple or overfitting the training data (high variance) if it's too complex. The key is to evaluate our model meticulously to strike an acceptable balance between bias and variance.

We will check **holdout cross-validation** and **k-fold cross-validation**, which can help us to obtain reliable estimates of the model's generalization performance, that is, how well the model performs on unseen data.

<br>

## The Holdout Method

Estimating the generalization performance of machine learning models often involves using <u>the holdout cross-validation approach</u>, a classic and popular method. This technique includes splitting the initial dataset into separate training and test datasets. <u>The training dataset is then used to train the model, while the test dataset assesses its generalization performance.</u>

<center>
  <img src="/blog/images/2024-08-19-TIL24_Day56/image-20240826150944655.png" width="70%"><br><br>
  <font><I>(Image from: https://vitalflux.com/hold-out-method-for-training-machine-learning-model/)</I></font>
</center>





In typical machine learning scenarios, it's crucial to **fine-tune and compare different parameter configurations** to enhance predictive performance on new data. This process, known as **model selection**, involves <u>identifying the best tuning parameters</u> (also called **hyperparameters**) for a specific classification problem. However, reusing the same test dataset multiple times during model selection can lead to it becoming part of our training data, <u>increasing the risk of overfitting</u>. Despite this challenge, many practitioners still utilize the test dataset for model selection, which is considered poor machine learning practice.



A better way of using the holdout method for model selection is to separate the data into three parts 





<center>
  <img src="/blog/images/2024-08-19-TIL24_Day56/image-20240826135323827.png" width="70%"><br>
  <font><I>(Image from: https://vitalflux.com/hold-out-method-for-training-machine-learning-model/)</I></font>
</center>






<br><br>

