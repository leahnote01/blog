---
title: "Day56 ML Review - Cross Validation "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, kFold, TIL_24]
toc: true 
---

# Using K-fold Cross Validation to Assess Model Performance

<img src="/blog/images/2024-08-19-TIL24_Day56/A77B832A-568D-4275-8B7F-FFA21E129330.jpeg"><br><br>

> When constructing a machine learning model, it is essential to evaluate its performance based on data it has not been trained on. For example, if we train our model using a particular dataset, <u>we need to assess its performance when presented with new data that it has yet to see.</u>

As we've discussed before, a model can suffer from underfitting (high bias) if it's too simple or overfitting the training data (high variance) if it's too complex. The key is to evaluate our model meticulously to strike an acceptable balance between bias and variance.

We will check **holdout cross-validation** and **k-fold cross-validation**, which can help us to obtain reliable estimates of the model's generalization performance, that is, how well the model performs on unseen data.

<br>

## The Holdout Method

In machine learning, we must adjust and <u>compare different parameter settings</u> to improve predictions on new data. This helps us choose the best parameter values for a given classification problem.

Using the same test dataset multiple times during model selection can cause overfitting, where the model is too closely matched to the training data and may need to work better with new data.

We split the data into <u>training, validation, and test datasets to avoid this.</u> The training dataset is used to fit different models, and the performance of the validation dataset helps with model selection.

One downside of the holdout method is that the <u>performance estimate may change based on how we split the training dataset into subsets</u>, leading to varying estimates. In the next section, we will explore a more reliable method for performance estimation called k-fold cross-validation, where we repeat the holdout method on subsets of the training data.



![image-20240826135323827](/images/2024-08-19-TIL24_Day56/image-20240826135323827.png)

<br><br>

