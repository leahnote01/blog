---
title: "Day56 ML Review - Cross Validation "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, kFold, TIL_24]
toc: true 
---

# Model Selection and K-Fold Cross Validation

<img src="/blog/images/2024-08-19-TIL24_Day56/A77B832A-568D-4275-8B7F-FFA21E129330.jpeg"><br><br>

> When constructing a machine learning model, evaluating its performance based on data on which it has yet to be trained is essential. For example, if we train our model using a particular dataset, <u>we need to assess its performance when presented with new data that it has yet to see.</u>

As we've discussed, a model can suffer from underfitting (high bias) if it's too simple or overfitting the training data (high variance) if it's too complex. The key is to evaluate our model meticulously to strike an acceptable balance between bias and variance.

We will check **holdout cross-validation** and **k-fold cross-validation**, which can help us to obtain reliable estimates of the model's generalization performance, that is, how well the model performs on unseen data.

<br>

## The Holdout Method

Estimating the generalization performance of machine learning models often involves using <u>the holdout cross-validation approach</u>, a classic and popular method. This technique includes splitting the initial dataset into separate training and test datasets. <u>The training dataset is then used to train the model, while the test dataset assesses its generalization performance.</u>

<center>
  <img src="/blog/images/2024-08-19-TIL24_Day56/image-20240826150944655.png" width="70%"><br><br>
  <font><I>(Image from: https://vitalflux.com/hold-out-method-for-training-machine-learning-model/)</I></font>
</center>

<br>

In typical machine learning scenarios, it's crucial to **fine-tune and compare different parameter configurations** to enhance predictive performance on new data. This process, known as **model selection**, involves <u>identifying the best tuning parameters</u> (also called **hyperparameters**) for a specific classification problem. However, reusing the same test dataset multiple times during model selection can lead to it becoming part of our training data, <u>increasing the risk of overfitting</u>. Despite this challenge, many practitioners still utilize the test dataset for model selection, which is considered poor machine learning practice.



A more effective approach to employing the holdout method for **model selection** involves dividing the data into **three distinct parts**: a training dataset, a validation dataset, and a test dataset. (1) The training dataset is utilized to train various models, and (2) <u>the validation dataset is then employed to select the best model based on performance.</u> The diagram below depicts the concept of holdout cross-validation, <u>where a validation dataset is used to assess the model's performance after training with various hyperparameter values.</u> 

(3)Once the hyperparameter values are appropriately fine-tuned, <u>the model's overall performance is evaluated using the test dataset.</u>



<center>
  <img src="/blog/images/2024-08-19-TIL24_Day56/image-20240826135323827.png" width="70%"><br>
  <font><I>(Image from: https://vitalflux.com/hold-out-method-for-training-machine-learning-model/)</I></font>
</center>

<br>

## K-Fold Cross-Validation

> Using the holdout method, we can estimate the performance of a model by repeating it $k$ times on $k$ subsets of the training data. 



In $k$-fold cross-validation, <u>the training dataset is randomly split into $k$ folds without replacement.</u> Out of these folds, $k-1$ is used for model training, and <u>the remaining $1$ fold is used for performance evaluation.</u> This process is repeated k times to create $k$ models and performance estimates.

To obtain a performance estimate that is <u>less sensitive to the sub-partitioning of the training data</u> compared to the holdout method, we calculate the average performance of the models based on different independent test folds. We typically use k-fold cross-validation for model tuning, which involves finding the optimal hyperparameter values that result in satisfactory generalization performance. This is estimated by evaluating the model performance on the test folds.

![image-20240826155432591](/images/2024-08-19-TIL24_Day56/image-20240826155432591.png)

After finding suitable hyperparameter values, we can retrain the model using the entire training dataset. This allows us to obtain a final performance estimate using the independent test dataset. The reason for fitting the model to the entire training dataset after k-fold cross-validation is that providing more training examples to a learning algorithm generally leads to a more accurate and reliable model.





<br><br>

