---
title: "Day27 ML Review - Training Perceptron (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,TIL_24]
toc: true 
---

# **Choosing a classification algorithm step by step** - Training Perceptron (2)

![9A24D9FE-384D-47B2-A010-DC1A8384D5AF_1_105_c](/images/2024-07-17-TIL24_Day27/9A24D9FE-384D-47B2-A010-DC1A8384D5AF_1_105_c-1503743.jpeg)

<br><br>

*(Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2, 3rd Edition)*

> Choosing the right classification algorithm for a particular problem involves practice and experience. Each algorithm has its <u>distinct characteristics</u> and is based on <u>specific assumptions</u>.

We will review the following steps for these classification algorithms, starting with the Perceptron. 

1. Selecting features and collecting labeled training examples.
2. Choosing a performance metric.
3. Choosing a classifier and optimization algorithm.
4. Evaluating the performance of the model.
5. Tuning the algorithm.

<br>

```python
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(X_train)
X_train_std = sc.transform(X_train)
X_test_std = sc.transform(X_test)
```

Continuing from the last posting, we will proceed with the above steps for standardization.

Using the `fit` method, `StandardScaler` estimated the parameters $\mu$ and $\delta$. 

Having standardized the training data, we can now train a perceptron model. Most algorithms in scikit-learn already support multiclass classification by default via the `one-vs.-rest (OVR)` method, which allows us to feed the three flower classes to the perceptron all at once. 



```python
from sklearn.linear_model import Perceptron
ppn = Perceptron(eta0=0.1, random_state=1)
ppn.fit(X_train_std, y_train)
```



The model parameter, `eta0`, is equivalent to <u>the learning rate</u>, `eta`, that we used in our perceptron implementation, and the `n_iter` parameter defines the number of epochs (passes over the training dataset).

Finding an appropriate learning rate requires some experimentation. 

The algorithm will overshoot the global cost minimum <u>if the learning rate is too large.</u> **The updates can overshoot the optimal solution, causing the algorithm to oscillate and potentially never converge.** 

On the other hand, <u>if the learning rate is too small</u>, the algorithm will require more epochs until convergence, making the learning slow - especially for large datasets. **The algorithm makes very small updates, which can slow down convergence.** Also, we used the `random_state` parameter to ensure the reproducibility of the initial shuffling of the training dataset after each epoch.

Having trained a model in scikit-learn, we can make predictions via the `predict` method.



```python
y_pred = ppn.predict(X_test_std)
print(‘Misclassified examples: %d’ % (y_test 1 != y_pred).sum()
```

<br>

