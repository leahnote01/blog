---
title: "Day08 ML Review - Linear Regression (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, TIL_24]
toc: true
---

# Cost Function of Linear Regression

<img src="/blog/images/2024-05-23-TIL24_Day8/20D36117-06D4-4077-9EDC-68C0B43EE283.jpeg" alt="day08in">

Every day is a new day. Spring has just bloomed in upstate New York.

As I sit here in the Science Library on campus, I can't help but feel a sense of accomplishment. The summer semester has just started, and while others are enjoying their time, I'm here, learning and growing. It's so productive and fills me with <font color="#0384fc">**joy and wisdom.**</font> haha! I truly appreciate my Lord Jesus with all my heart.



<img src="/blog/images/2024-05-23-TIL24_Day8/4C4610FF-B8AD-4674-95E2-2F7E432692FF_1_102_a.jpeg" alt="lib">



### Cost Function of Linear Regression

> Linear regression aims to find the best-fitting line through a set of data points. This line is defined by its parameters (coefficients): the slope & the intercept.  The cost function helps determine how well the line fits the data by measuring the difference between the observed values and the values predicted by the model. 

<br>

In linear regression, the cost function is critical in evaluating the model's performance. Linear regression aims to find the best-fitting line through a set of data points. This line is defined by its parameters (coefficients): the slope and the intercept. The cost function helps determine how well the line fits the data by measuring the difference between the observed values and the values predicted by the model. The most commonly used cost function in linear regression is the Mean Squared Error (MSE).

<br><br>

#### Mean Squared Error (MSE)

The MSE is calculated by taking **the average of the squared differences** between the <u>observed values</u> (actual y-values of the data points) and the <u>predicted values</u> (predicted y-values obtained from the regression line). Mathematically, it can be expressed as: <br>

<center> $\text {MSE} = \frac{1}{n} \sum_{i=1}^n(y_i-\hat{y_i})^2   \\
\\$  </center>


<br>

Where : 

* $n$ is the number of data points,
* $y_i$ is the actual value at $i^{th}$ point,
* $\hat{y_i}$ is the predicted value at $i^{th}$ point.

<br>

##### Purpose of the MSE

- **Performance Measurement**: MSE provides a quantifiable measure of how far the predictions deviate from the actual outcomes, which directly reflects the performance of the model.

- **Model Fitting**: The aim of linear regression is to minimize the MSE to achieve the best fit. The model can find the line that best fits the data by adjusting the coefficients (slope and intercept) to reduce MSE.

- **Gradient Descent**: MSE is particularly useful because it is differentiable, allowing techniques like gradient descent to be used for optimization. Gradient descent is an algorithm that iteratively adjusts the parameter to minimize the MSE.

<br>

##### Characteristics of MSE

- **Sensitive to Outliers**: MSE increases significantly when there are outliers in the data, as it squares the differences. This sensitivity can be beneficial for detecting outliers but can also skew the model if outliers are not addressed.

- **Always Non-negative**: MSE is always non-negative because it involves the squares of differences, and the square of any real number is non-negative. The best value (minimum) is 0 when all predictions perfectly match the actual values.



![BF26A6E3-700A-4C37-BA1E-2F0DA46AEE40](/images/2024-05-23-TIL24_Day8/BF26A6E3-700A-4C37-BA1E-2F0DA46AEE40.jpeg)
