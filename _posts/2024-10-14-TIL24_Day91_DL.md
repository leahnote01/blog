---
title: "Day91 Deep Learning Lecture Review - HW1"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, hyperparameter tuning, mlops, w&b, TIL_24]
toc: true 
---

# Optimizing and Monitoring Hyperparameters with W&B for ResNet-18 Training

<img src="/blog/images/2024-10-14-TIL24_Day91_DL/C38292DC-87CA-412E-BCEE-7415218C78CE_1_105_c.jpeg"><br><br>

I explored hyperparameter tuning, network fine-tuning, and monitoring model performance with Weights and Biases (W&B) in this latest assignment. The primary goal was to train ResNet-18 for image classification on the Oxford-IIIT Pet Dataset while optimizing different model parameters. Here’s a summary of my workflow, the results, and the insights I gained throughout the process.<br><br>

## W&B for Training Monitoring

Training neural networks involves balancing various model architectures, hyperparameters, and optimization strategies. To make these decisions effectively, monitoring the training process and understanding the impact of changes on model performance is essential. I used **W&B to log metrics** such as <u>training loss, validation accuracy, test accuracy, and other performance indicators.</u>   <br>

\*I utilized Google Colab since W&B does not cater to some Mac environment users. 



1. **Dataset Preparation**: I was provided the <u>Oxford-IIIT Pet Dataset</u> by **resizing** the images (as required by ResNet-18) and applying **normalization**. This setup aligns with the requirements for training ResNet-18, which expects input data similar to the <u>ImageNet</u> Dataset.

2. **Implementing W&B Logging**: I integrated W&B into the training script by logging:

   - Training loss and learning rate after each mini-batch.
   - Validation loss and accuracy after each epoch.
   - Final test loss and accuracy.

   ```python
   import wandb
   wandb.init(project="resnet-oxford-pets")
   
   # Inside of training loop
   # Log training metrics to W&B
   wandb.log({
     'train_loss': loss.item(),
     'learning_rate': optimizer.param_groups[0]['lr'],
     'seen_examples': seen_examples,
     'global_step': global_step
             })
   
   if global_step % batch_steps == 0:
     val_acc, val_loss = evaluate(model, val_loader, device)
   
     
   # Log validation metrics to W&B
   wandb.log({
     'val_loss': val_loss,
     'val_accuracy': val_acc,
     'epoch': epoch,
     'seen_examples': seen_examples
   })
   
   if best_val_loss > val_loss:
     best_val_loss = val_loss
     os.makedirs(os.path.join(save_dir, f'{run_name}_{rid}'), exist_ok=True)
     state_dict = model.state_dict()
     torch.save(state_dict, os.path.join(save_dir, f'{run_name}_{rid}', 'checkpoint.pth'))
     print(f'Checkpoint at step {global_step} saved!')
   
   
   # Log metrics to W&B
   wandb.log({
     'loss (batch)': loss.item(),
     'global_step': global_step,
     'seen_examples': seen_examples
   })
   ```

   <br><br>

3. Result- Observations

   - **Training Loss**: The training loss consistently decreased, indicating that the model learned from the data effectively. 
   - **Validation Loss**: In contrast to the training loss, the validation loss increased over time, <u>suggesting that the model overfitted the training data.</u> **This highlights the importance of validation monitoring to maintain generalizability.** 
   - **Test Accuracy**: The final test accuracy was relatively low, around **2%**, indicating poor generalization. This led me to re-evaluate the model's settings and consider additional techniques. 

<br><br>

<center>
  <img src="/blog/images/2024-10-14-TIL24_Day91_DL/Screenshot 2024-10-23 at 1.09.48 PM.png" width="80%"><img src="/blog/images/2024-10-14-TIL24_Day91_DL/Screenshot 2024-10-23 at 1.09.19 PM.png" width="80%">
<img src="/blog/images/2024-10-14-TIL24_Day91_DL/Screenshot 2024-10-23 at 1.08.55 PM.png" width="80%">
  <img src="/blog/images/2024-10-14-TIL24_Day91_DL/Screenshot 2024-10-23 at 1.08.36 PM.png" width="80%"> </center> <br><br>



## Tuning Hyperparameters

I concentrated on tuning hyperparameters, particularly <u>the learning rate</u> during this phase, **utilizing a W&B sweep.** The goal was to identify the best settings for enhancing the model's performance. 



### Learning Rate Tuning

- I ran multiple experiments using learning rates: $1e-2$, $1e-3$, $1e-4$,  $1e-5$, with the default being $1e-3$

  ```python
  sweep_config = {
    'method' : 'grid',
    'parameters' : {
      'learning_rate': {
        'values': [1e-2, 1e-3, 1e-4, 1e-5]
      }
    }
  }
  sweep_id = wandb.sweep(sweep_config, project="resnet-oxford-pets")
  wandb.agent(sweep_id, function=train)
  ```

  <center>
    <img src="/blog/images/2024-10-14-TIL24_Day91_DL/image-20241023142956741.png"><br><br>
  </center>

  

- Key Insights

  - The learning rate of <u>$1e-3$</u> consistently showed the best convergence and generalization performance, leading to lower training loss and higher **validation and test accuracy**. 
  - Higher learning rates led to oscillation without convergence, whereas very low rates caused slow learning and the risk of underfitting.

  - Charts comparing the learning rate impacts demonstrated that 0.001 was optimal, balancing efficient learning and model stability.



### Learning Rate Scheduler with OneCycleLR

To improve training efficiency and convergence, I implemented the **OneCycleLR scheduler**. This scheduler dynamically adjusts the learning rate, starting with a low value, increasing it to a peak, and gradually reducing it.


```python
from torch.optim.lr_scheduler import OneCycleLR

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
scheduler = OneCycleLR(optimizer, max_lr=1e-2, steps_per_epoch = len(train_loader),
                      epochs = num_epochs)


# In the Train loop
if use_scheduler:scheduler = lr_scheduler.OneCycleLR(
  optimizer,
  max_lr=kwargs.get('max_lr', 0.01),
  total_steps=kwargs.get('total_steps', 1000),
  pct_start=kwargs.get('pct_start', 0.3),
  anneal_strategy=kwargs.get('anneal_strategy', 'linear'), 
  final_div_factor=kwargs.get('final_div_factor', 10)
)
else:
  scheduler = None
  return scheduler

```



![image-20241023144101190](/images/2024-10-14-TIL24_Day91_DL/image-20241023144101190.png)

- Results with OneCycleLR
  - The training process with **OneCycleLR** showed faster convergence compared to the baseline with a static learning rate. The model benefited from higher initial leraning rates to explore the parameter spze, followed by reduced rates for finer adjustments.
  - However, validation accuracy fluctuated more compared to the baseline, highlighting that a dynamic learning rate can sometimes introduce instability but ultimately led to better test accuracy.

<br><br>

### **Fine-Tuning a Pretrained ResNet-18**

Lastly, I fine-tuned a ResNet-18 model pre-trained on ImageNet. Fine-tuning leverages the pre-learned features and adapts them to the specific task, in this case, **classifying pet images**. 

```python
import torchvision.models as models

# Load a pretrained ResNet-18 model
model = models.resnet18(pretrained=True)

# modify the final layer to match the number of classes in the pet dataset
model.fc = nn.Linear(model.fc.in_features, num_classes)

# Freeze all layers except the final one
for param in model.parameters():
  param.requires_grad = False
for param in model.fc.parameters():
  param.requires_grad = True
```

<br>

![image-20241025115707470](/images/2024-10-14-TIL24_Day91_DL/image-20241025115707470.png)

![image-20241025115720697](/images/2024-10-14-TIL24_Day91_DL/image-20241025115720697.png)

- Comparison with Training from Scratch
  - **Training Loss**: The pre-trained model showed significantly faster convergence, reducing the loss in fewer epochs compared to the baseline model.
  - **Validation and Test Accuracy**: The fine-tuned model also outperformed the baseline in validation and test accuracy, underscoring the benefit of using transfer learning to quickly adappt strong feature representations from large datasets like ImageNet.

<br><br>

## Model Testing

### Pre-Train Testing

>Pre-Train Test: Conducted before training, these tests identify potential issues in the model’s architecture, data preprocessing, or other components, preventing wasted resources on flawed training.

- **Data Leakage Check**: I checked for potential data leakage between training, validation, and test sets by comparing image hashes.
  - Observation: no overlap between training and validation, but <u>5,000 images overlapped between training and test sets</u>. This indicates a serious data leakage problem, which could lead to inflated test performance.
  - Solution: Re-split the dataset to eliminate any overlap, ensuring each set is unique to maintain fair evaluation.
- **Learning Rate Tuning:** I used a **learning rate range test** to identify the optimal learning rate for training the model. The graph suggested an optimal learning rate around **0.026**, where the loss decreased rapidly.
  - **Analysis**: Choosing an optimal learning rate is crucial for **efficient training and stability**. A value too high leads to unstable training, while a value too low causes slow convergence.

![image-20241025160646543](/images/2024-10-14-TIL24_Day91_DL/image-20241025160646543.png)



- **Model Architecture Check**: to verify that the model's output shape matches the label format;
  - Initialized the model and identified an incorrect output shape of $[1, 128]$ instead of the expected $[1, 10]$.
  - <u>Modified the final fully connected layer to output the correct number of features (10).</u>
- **Gradient Descent Validation**: to verify that all the models' trainable parameters are updated after a single gradient step on a batch of data. 
  - Detected that parameters in the downsample layers were not receiving gradients.
  - Implemented a function to check for missing gradients and reviewed the architecture for necessary adjustments.<Br><br>



### Post Train Testing

I examined the trained model's robustness by focusing on **post-training evaluations**, particularly on detecting common issues such as **Dying ReLU** and assessing the model's **robustness to brightness variations**. Below are the key areas and findings.

- **Dying ReLU Examination**: Dying ReLU occurs when a ReLU activation function <u>consistently outputs zero,</u> effectively making a neuron non-contributive. This can lead to ineffective learning since such "dead" neurons no longer help the model learn meaningful features.

  - Approach: I registered hooks on the ReLU layers in my model to capture their outputs during a forward pass through the test dataset.

  - Observation: After analyzing the ReLU activations, I found **slight significant Dying ReLU neurons** in my trained model, which is a positive sign, indicating that the ReLU layers are functioning properly and contributing to the model's learning.

    ``` python
    # Hook to store ReLU outputs
    relu_outputs = []
    
    def relu_hook(module, input, output):
        relu_outputs.append(output.clone().detach())
    
    # Register hooks on ReLU activations
    for layer in model.modules():
        if isinstance(layer, torch.nn.ReLU):
            layer.register_forward_hook(relu_hook)
    
    # Forward pass to check activations
    with torch.no_grad():
        for images, labels in test_loader:
            _ = model(images)
    
    # Analyzing the collected ReLU outputs
    dying_relu_neurons = sum(torch.sum(relu_output == 0).item() for relu_output in relu_outputs)
    print(f"Dying ReLU neurons: {dying_relu_neurons} detected.")
    ```

<br>

- **Model Robustness to Brightness changes**: To further evaluate the model's robustness, I tested how well the model could classify images under different brightness levels.

  - **Brightness Factor Evaluation**: I adjusted the brightness of the test images by multiplying pixel values with a factor **λ**, ranging from **0.2 to 1.0** in increments of 0.2. This allowed me to observe how the model performed across varying lighting conditions.

  - **Results**:

    - Highest Accuracy at Low Brightness: The model performed best at a low brightness factor of λ = 0.2, achieving an accuracy of around 17.48%.

    - Performance Degradation: As brightness increased, accuracy dropped, with the lowest accuracy recorded at λ = 1.0 (original brightness level), indicating that the model was not well-optimized for well-lit images

      ``` python
      # Adjust brightness levels and evaluate
      brightness_levels = [0.2, 0.4, 0.6, 0.8, 1.0]
      for level in brightness_levels:
          transformed_images = apply_brightness_factor(test_images, level)
          accuracy = evaluate_model(model, transformed_images, labels)
          print(f"Accuracy at brightness level {level}: {accuracy:.2f}%")
      ```

<br>

-  **Normalization Mismatch**: An analysis was conducted to assess normalization differences between the training and test sets. 

  - **Calculated vs. Expected Statistics**: The test set's mean and standard deviation showed significant deviation from the expected values. For example, the calculated mean was close to zero, while the expected values were approximately 0.49, 0.48, and 0.44 for the RGB channels.

  - **Impact**: This inconsistency in normalization may lead to the model encountering inputs that fall outside the range it was trained on, which could result in **poor generalization** and incorrect predictions during inference.

  

  ![image-20241025163056677](/images/2024-10-14-TIL24_Day91_DL/image-20241025163056677.png)



<br><br>Throughout the entire journey of processing whole procedures, from fine-tuning a model to executing post-tests, I gained a profound understanding of several critical aspects of machine learning. The significance of model validation became abundantly clear, showcasing how essential it is to ensure a model's performance is robust and reliable. 

<br><Br>
