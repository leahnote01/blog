---
title: "Day68 Deep Learning Lecture Review - Lecture 3"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [DL review, transformers, TIL_24]
toc: true 
---

# Neural Net Zoo: Transformers, Recurrent Neural Networks (RNNs) and Graph Neural Networks (GNNs)

<img src="/blog/images/2024-09-05-TIL24_Day68_DL/9C0DB8D1-E5F6-40F6-9C08-6C572349A4D4_1_105_c.jpeg"><br><br>

We will review several high-level neural network architectures, which are all based on the MLP.

### Fully-Connected Networks (MLPs) -- Revisiting

- MLP is <u>a class of fully connected feedforward neural networks.</u> It consists of **input** layers, one or more **hidden** layers, and an **output** layer. Every node (neuron) in one layer is connected to every node in the next layer, hence the term "fully connected."
- **Critical Characteristics of MLP:**
  - <u>General Purpose</u>: MLPs apply to a range of tasks, including classification and regression. However, they do not naturally leverage data structures like images or time series.
  - <u>Fixed Input Size</u>: MLPs require fixed-size input data (flattened into a vector), so they don't naturally handle high-dimensional data like images without preprocessing.
  - <u>Lack of Spatial Awareness</u>: MLPs do not consider the spatial structure of the data, making them less efficient for tasks like image recognition, where local relationships between pixels are essential.

- MLPs have been extended to handle other forms of data.
  - It is often used in other networks.
- Sequential inputs and sets (text, audio, etc.): RNNs (many types), Transformers (many types)
- Graphs: Graph neural networks (many types)
- Images and Video: Convolutional Neural Network,  Vision Transformers
- Every type of neural network architecture features layers with specific characteristics.
  - You can combine them freely.



### Transformers

- Transformers for Variable Length Sequences
  - It can be used for sets of data and sequential data.
  - Each input consists of N $d$-dimensional vectors, where $N$ is variable (sequence length).
  - Uses weight sharing (same weights for each sequence element) and self-attention.

<br>

- Transformers Preserve Input Topology
  - If we give a transformer layer $N$ vectors in a sequence, it will produce $N$ corresponding coutputs.
  - The self-attention layer modulates the processing such that all other locations in the sequence modulate each location. <br><br>

<center>
  <img src="/blog/images/2024-09-05-TIL24_Day68_DL/image-20240906204246221.png"><br><br>
</center>





- Transformer vs. Fully connected MLP Network

  | Features     | Fully Connected MLP                          | Transformer                                                  |
  | ------------ | -------------------------------------------- | ------------------------------------------------------------ |
  | Unit Input   | Vector                                       | $N$ $d$-dim Vectors                                          |
  | Unit Output  | 1 number (scalar)                            | $N$ $b$-dim Vectors                                          |
  | Architecture | Layers of densely connected neurons.         | Based on self-attention mechanisms. Encoder-decoder structure. |
  | Processing   | Processes all sequence parts simultaneously. | Processes inputs independently; lacks sequence awareness.    |

  The $N$ inputs are often called "tokens"

<br><br><br>

### Recurrent Neural Networks 

- Recurrent Neural Networks (RNNs)
  - Used for sequential data, just like transformers
  
  - Hard to parallelized compared to transformers
  
    

<center>
  <img src="/blog/images/2024-09-05-TIL24_Day68_DL/image-20240906210350522.png"><br><br>
</center>
Source: [IBM-What is RNNs?](https://www.ibm.com/topics/recurrent-neural-networks)

> Recurrent Neural Networks (**RNNs**) are a class of neural networks specifically <u>designed for handling sequential data</u>. 

- Unlike feedforward neural networks, which assume inputs are independent of each other, RNNs have connections that form cycles, allowing them to retain information from previous time steps. <br><br>



#### Key Features of RNNs

- <u>Sequential Data Processing</u>: RNNs are designed to handle **sequential or time-dependent data**, such as time series, natural language, or video frames. They maintain a hidden state that captures information about previous time steps, enabling them to model dependencies over time.
- <u>Hidden State</u>: The key feature of RNNs is the hidden state, which acts as memory. <u>The hidden state at each time step is updated based on the input at that step and the hidden state from the previous step</u>, allowing the network to carry forward important information.
- <u>Shared Weights</u>: RNNs use the **same weights** across all time steps, which makes them more efficient when processing sequences of arbitrary length. This weight sharing allows them to generalize across different time steps in the sequence.
- <u>Feedback Loops</u>: RNNs have feedback loops that enable the network <u>to incorporate information from previous time steps.</u> This looping mechanism gives RNNs the ability to **remember past inputs**, making them suitable for tasks where context from earlier inputs is important.<br><Br>



#### How RNNs Work

RNNs process inputs sequentially, maintaining a hidden state that carries information from previous steps. At each time step, the network performs the following operations:

1. **Input**: Receives an input vector (e.g., a word in a sentence).
2. **Hidden State Update**: Updates the hidden state based on the current input and the previous hidden state.
3. **Output**: Produces an output (e.g., predicting the next word in a sequence or classifying the sentiment of a sentence).

For long sequences, RNNs face challenges such as the vanishing gradient problem, which LSTMs and GRUs aim to address by adding mechanisms to retain and forget information selectively.



![image-20241020180015423](/images/2024-09-05-TIL24_Day68_DL/image-20241020180015423.png)



#### Comparison with Other Neural Networks

- **RNN vs. CNN**: While CNNs are good at processing data with grid-like structures (e.g., images), RNNs are better suited for sequential data (e.g., text or time series). <u>CNNs are spatially focused, whereas RNNs are temporally focused.</u>
- **RNN vs. MLP**: <u>MLPs assume that the inputs are independent, which makes them unsuitable for sequential data</u>. RNNs, on the other hand, explicitly model dependencies between inputs over time.<br><br>



#### Limitations of RNNs

- **Vanishing Gradient Problem**: When training on long sequences, the gradients during backpropagation can shrink, making it hard for RNNs to learn long-term dependencies.
- **Training Difficulty**: RNNs are harder to train and often require specialized architectures like LSTMs or GRUs for better performance on long sequences.<br><br>



### Graph Neural Networks (GNNs)

>  **Graph Neural Networks (GNNs)** are a type of neural network designed specifically to work with **graph-structured data**. 

In contrast to traditional neural networks like Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), which work with grid-like data structures (e.g., images or sequences), GNNs are designed to process and learn from data where **relationships** <u>between entities can be represented as nodes and edges in a graph.</u> This makes GNNs well-suited for tasks that involve understanding complex relationships, <u>such as social networks, molecular structures, or knowledge graphs.</u>

#### Key Concepts in GNNs

1. **Graph Structure**:
   - A **graph** consists of **nodes** (or vertices) and **edges** (connections between nodes). Nodes represent entities, and edges represent relationships between them. GNNs learn to propagate and aggregate information over this graph structure.
2. **Message Passing**:
   - GNNs operate based on a process called **message passing**. At each layer of the network, nodes update their representations by exchanging information (messages) with their neighboring nodes. The updated node representation is derived from both the nodeâ€™s own features and the features of its neighbors.
   - Message passing occurs iteratively across several layers, allowing information to propagate across the entire graph and enabling the model to capture both local and global dependencies.
3. **Node Embeddings**:
   - After multiple layers of message passing, each node in the graph has a learned representation or **embedding**. This embedding contains both the features of the node itself and information aggregated from its neighboring nodes, allowing the GNN to capture the graph structure and relationships.
4. **Graph-Level and Node-Level Tasks**:
   - GNNs can be used for a variety of tasks, including:
     - **Node classification**: Predicting the labels of individual nodes (e.g., classifying users in a social network based on their connections).
     - **Link prediction**: Predicting whether a connection (edge) exists or will exist between two nodes (e.g., recommending friendships in a social network).
     - **Graph classification**: Assigning a label to an entire graph (e.g., determining the chemical properties of a molecule based on its atomic structure).





### Transfer Learning





<br><br>

