---
title: "Day68 Deep Learning Lecture Review - Lecture 2 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, transformers, TIL_24]
toc: true 
---

# Lecture 2: Transformers, Recurrent Neural Networks (RNNs) and Graph Neural Networks

<img src="/blog/images/2024-09-05-TIL24_Day68_DL/9C0DB8D1-E5F6-40F6-9C08-6C572349A4D4_1_105_c.jpeg"><br><br>

### Transformers

- Transformers for Variable Length Sequences
  - It can be used for sets of data and sequential data.
  - Each input consists of N $d$-dimensional vectors, where $N$ is variable (sequence length).
  - Uses weight sharing (same weights for each sequence element) and self-attention.

<br>

- Transformers Preserve Input Topology
  - If we give a transformer layer $N$ vectors in a sequence, it will produce $N$ corresponding coutputs.
  - The self-attention layer modulates the processing such that all other locations in the sequence modulate each location. <br><br>

![image-20240906204246221](/images/2024-09-05-TIL24_Day68_DL/image-20240906204246221.png)



- Transformer vs. Fully connected MLP Network

  | Features     | Fully Connected MLP                          | Transformer                                                  |
  | ------------ | -------------------------------------------- | ------------------------------------------------------------ |
  | Unit Input   | Vector                                       | $N$ $d$-dim Vectors                                          |
  | Unit Output  | 1 number (scalar)                            | $N$ $b$-dim Vectors                                          |
  | Architecture | Layers of densely connected neurons.         | Based on self-attention mechanisms. Encoder-decoder structure. |
  | Processing   | Processes all sequence parts simultaneously. | Processes inputs independently; lacks sequence awareness.    |

  The $N$ inputs are often called "tokens"

<br><br><br>

### Recurrent Neural Networks 

- Recurrent Neural Networks (RNNs)
  - Used for sequential data, just like transformers
  - Hard to parallelized compared to transformers
  - Hard to make them "``ep" in the same ways as transformers

![image-20240906210350522](/images/2024-09-05-TIL24_Day68_DL/image-20240906210350522.png)

