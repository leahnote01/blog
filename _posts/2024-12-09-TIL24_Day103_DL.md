---
title: "Day103 Deep Learning Lecture Review - Lecture 18 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Data-Centric AI: Active Learning, SEALS(Similarity Search for Efficient Active Learning), Dataset Pruning, and Data Engine

<img src="/blog/images/2024-12-09-TIL24_Day103_DL/DAC8C068-1943-4123-AD4C-09C1F6368AD2_1_105_c.jpeg"><br><br>



### Selecting Data to Label

#### Active Learning for Efficiently Labeling Datasets

> Active Learning is an iterative process to label data points to maximize the model's performance. Instead of randomly labeling data, it identifies the most **informative samples** to label next.

- Method for determining which samples from a large unlabeled data pool to label would maximally improve models.
- Active learning is a method for efficiently getting labels for datasets.<br><Br>

<center>
  <img src="/blog/images/2024-11-29-TIL24_Day102_DL/image-20241221212000077.png" width="80%"><br><br>
</center>


- **Pool-Based Active Learning:**
  
  1. **Unlabeled Data Pool**
     - Begin with a large pool of unlabeled data.
  2. **Initial Labeling**
     - Label <u>a small subset of the data</u> to train an initial model.
  3. **Select Informative Samples**
     - Use the trained model <u>to rank unlabeled samples by uncertainty</u> (e.g., entropy or margin sampling).
     - <u>Choose the most uncertain samples</u> to label in the next round.
  4. **Iterate**
     - Retrain the model with the newly labeled data and repeat the process.<br><br>
  
  

- **Determining Which Samples to Label**
  - For each round of active learning, we <u>need to identify the samples we want to label.</u>
  - Most methods use uncertainty sampling, where we select samples to label what the model is most uncertain about.
  - Two standard methods for scoring data are **maximum entropy (MaxEnt)** and **Information Density (ID)**. 
    - MaxEnt: $\phi_{\text{MaxEnt}}(z, A_r, P_r) = - \sum_{\hat{y}} P(\hat{y} \vert z; A_r) \log P(\hat{y} \vert z; A_r)$ 
    - ID: It uses MaxEnt and also cosine similarity between embeddings. $\phi_{\text{ID}}(\textbf{z}, A_r, P_r) = \phi_{\text{MaxEnt}}(z) \times \left( \frac{1}{\vert P_r \vert} \sum_{z_p \in P_r} \text{sim}(z, z_p) \right)^\beta$
      - $A_r$ is the model at round $r$, $\textbf{z}$ is an embedding, and $P_r$ is the set containing the unlabeled pool.<br>



#### Key Benefits & Challenges

- **Advantages**
  - **Efficiency**: Saves resources by focusing on the most impactful samples.
  - **Cost Reduction**: Reduces the need to label large datasets.
- **Challenges**
  - Determining which samples to label to maximize performance
  - New emerging challenges in active learning during the era of deep learning. 
- **Applications**
  - Annotating medical data, where expert labeling is expensive (e.g., radiology images).
  - Selecting rare concepts in large datasets using **SEALS** (Similarity search for Efficient Active Learning).<br><Br>



#### SEALS (Similarity search for Efficient Active )

Image Source: [MIT- Growing or Compressing Datasets](https://dcai.csail.mit.edu/2023/growing-compressing-datasets/)

![image-20250114221148914](/images/2024-12-09-TIL24_Day103_DL/image-20250114221148914.png)

![image-20250114221221655](/images/2024-12-09-TIL24_Day103_DL/image-20250114221221655.png)

![image-20250114221254263](/images/2024-12-09-TIL24_Day103_DL/image-20250114221254263.png)

- **SEALS** starts the same way as conventional active learning, but uses similiarity ssearch such that it doesn't need to run the deep learning system on all of the unlabeled data. 
- Take newly labeled samples and find their neighbors.
- Procedures in SEAL
  1. **Embedding Extraction**
     - Compute embeddings (vector representations) ***for all*** unlabeled data points using a pretrained model.
     - Embeddings capture <u>the semantic meaning</u> of the data, making similarity searches more meaningful.
  2. **Similar Matrix Computation**
     - Use <u>locality-sensitive hashing (LSH)</u> to create a fast approximate simliarity matrix.
     - LSH groups similar embeddings together, enabling quick nearest neighbor searches.
  3. **Candidate Selection**
     - Select the **k-nearest neighbors** of the labeled points based on the similiarity matrix.
     - These neighbors form a smaller candidate pool for active learning.
  4. **Uncertainty Scoring**
     - Apply uncertainty metrics (e.g., MaxEnt or ID) only to the candidate pool.
     - Selecte the most uncertain points for labeling.
  5. **Repeat**
     - After labeling the selected points, update the model and repeat the process.







<Br><br>
