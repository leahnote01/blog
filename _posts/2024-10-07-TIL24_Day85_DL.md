---
title: "Day85 Deep Learning Lecture Review - Lecture 7-8"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, CNN, TIL_24]
toc: true 
---

# Large Language Model - Perplexity, Tokenizers, Data Cleaning, and Much More

![3F8B9BAE-5982-44D3-962F-13437C646816_1_105_c](/images/2024-10-07-TIL24_Day85_DL/3F8B9BAE-5982-44D3-962F-13437C646816_1_105_c.jpeg)

<Br><br>

#### General Advice for LLMs

- Start small- Try to use super large models when you need to. 
- Try to repurpose existing models over building your own 
- Test ideas in papers for yourself



#### Perplexity

> **Perplexity** is a metric commonly used in Natural Language Processing (NLP) to evaluate language models, especially probabilistic models like those used in text generation tasks (e.g., GPT). It measures how well a model predicts a sequence of words, and it gives insight into how "uncertain" or "surprised" the model is when making predictions. (Source: https://klu.ai/glossary/perplexity)

- It measures how well a model can predict the next word based on the preceding context.

- The lower the score, the better the model's ability to predict the next word accurately.

  ![image-20241008131026818](/images/2024-10-07-TIL24_Day85_DL/image-20241008131026818.png)

- It means the exponent of the cross-entropy loss (mean of log-likelihood) obtained from the model on the next word prediction task.
- **Interpretation**:

  - If the model assigns <u>high probabilities to the correct words (or the next word in a sequence), the perplexity will be low,</u> meaning the model is doing well.
  - Conversely, if the model assigns low probabilities to the correct words, the perplexity will be high, indicating that the model is "perplexed" or uncertain.

  

![image-20241008132007167](/images/2024-10-07-TIL24_Day85_DL/image-20241008132007167.png)

- **Limitations:**
  - Length Sensitivity: Since perplexity is normalized by the number of words in the sequence, it may not always directly correspond to how "good" a model is for long or short sequences. Sometimes, it can be less intuitive than other metrics like accuracy. (**For shorter text is larger than longer text**)
  - Limited Scope: Perplexity focuses solely on the likelihood of predicting the next word, which may not capture other important aspects of language generation, such as fluency, coherence, or factual correctness.



#### Preprocessing Datasets for LLMs



