---
title: "Day58 ML Review - Cross Validation (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, TIL_24]
toc: true 
---

# Grid Search for Fine-Tuning Machine Learning Models

<img src="/blog/images/2024-08-21-TIL24_Day58/ADAAFFD8-C6A4-45F8-9DAF-B6CA0DB02644.jpeg"><br><Br>

## Grid Search

> Grid search is a method <u>for fine-tuning the hyperparameters of machine learning models.</u> It involves defining **a grid of hyperparameter values** and systematically searching through the grid **to find the combination of parameters** that results in the best model performance, as determined by a specific metric. <br>



### How it works

1. **Define the Parameters**: We start by defining a list or grid of values for various hyperparameters we want to tune. This might include parameters like learning rate, number of hidden layers, number of trees in a decision forest, etc.
2. **Cross-Validation Setup**: Grid search is often used in conjunction with cross-validation to ensure that the tuning process does not overfit to a particular subset of the data. In $k$-fold cross-validation, the training set is split into $k$ smaller sets (or folds). The model is trained on $k-1$ of these folds, with the remaining part used as a pseudo-test set to compute a performance measure such as accuracy, F1 score, etc.
3. **Exhaustive Search**: The method then systematically trains models using every combination of parameters in your grid and evaluates each model using cross-validation. For each combination, the average score across all folds is computed.
4. **Select Best Parameters**: <u>Once all combinations have been evaluated, the grid search selects the best parameter combination in the cross-validation process.</u>
5. **Final Model**: A final model is often trained on the entire training set using the best parameter settings.

<Br>

This method is exhaustive but can also be <u>computationally expensive</u>, especially with large datasets, a wide range of parameter values, or many parameters to tune. As an alternative, <u>methods like random search or Bayesian optimization provide faster but less exhaustive approaches to hyperparameter tuning.</u><br>



```python
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
pipe_svc = make_pipeline(StandardScaler(), SVC(random_state=1))
param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]
param_grid = [{'svc__C': param_range,
              'svc__kernel': ['linear']},
             {'svc__C': param_range,
             'svc__gamma': param_range,
             'svc__kernel': ['rbf']}]
gs = GridSearchCV(estimator=pipe_svc,
                 param_grid=param_grid,
                 scoring='accuracy',
                 cv=10,
                 refit=True,
                 n_jobs=-1)
gs = gs.fit(X_train, y_train)
print(gs.best_score)
print(gs.best_params_)
```

In the previous code, we used the `GridSearchCV` object from the `sklearn.model_selection` module to train and optimize an SVM pipeline. We specified the parameters we wanted to tune by setting the `param_grid` parameter of `GridSearchCV` to a list of dictionaries. For the linear SVM, we only focused on tuning the inverse regularization parameter, `C`, while for the RBF kernel SVM, we tuned both the `svc __C` and `svc__gamma`parameters. It's important to note that the `svc__gamma` parameter is specific to kernel SVMs.

**We found the score of the best-performing model using the `best_score_` attribute and examined its parameters using the `best_params_` attribute.** In this instance, the RBF kernel SVM model with `svc__C` = 100.0 achieved the highest $k$-fold cross-validation accuracy of 98.5%. 

<br><br>

### Side Note: Nested Cross-Validation

> Another recommendation for hyperparameter tuning is nested cross-validation. This method involves <u>two layers of cross-validation: an outer loop and an inner loop.</u> The purpose is to ensure that the model evaluation is as unbiased as possible, particularly in scenarios involving hyperparameter tuning.<Br>

We use an **outer** k-fold cross-validation loop to <u>split the data into training and test folds.</u> Within this, there is an **inner** loop that <u>selects the model using k-fold cross-validation on the training fold.</u> 

Once the model is selected, the test fold is used to evaluate the model's performance. The diagram below illustrates the concept of nested cross-validation. This approach is particularly useful for large datasets where computational performance is crucial.

![image-20240827203529283](/images/2024-08-21-TIL24_Day58/image-20240827203529283.png)



<br><br>

