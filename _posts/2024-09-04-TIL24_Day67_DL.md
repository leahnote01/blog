---
title: "Day67 Deep Learning Lecture Review"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, TIL_24]
toc: true 
---

# Lecture 2: Paradigms Beyond Supervised Learning and Neural Network

<img src="/blog/images/2024-09-04-TIL24_Day67_DL/E078B106-915B-47B6-B14F-816FCBFA5E13.jpeg"><br><br>



## 1. Paradigms Beyond Supervised Learning

### Types of Learning

- Supervised Learning
  - Training data includes desired output.
  - classification & regression



- Unsupervised Learning
  - Training data does not include the desired output.
  - clustering, dimensionality reduction, density estimation



- Self-Supervised Learning
  - Training data labels itself somehow.



- Reinforced Learning
  - A type of machine learning where an agent <u>learns to behave in an environment by performing actions and seeing the results, often as rewards or punishments</u>
  - In supervised learning, a function approximation such as 'f(x)=y' is given to the model, but in RL, within the environment, the agent can take actions at each time step, and <u>it is not told exactly what to do.</u>
  - The model gets a scaler reward signal.
    - ex) Won the game $\rightarrow$ +1, or lost the game -1
  - Wants to maximize the reward
  - Examples
    - Gaming:
      - AlphaGo: The program developed by DeepMind that learns to play the board game Go and can compete against top human players.
      - Video Game Play: Agents developed to play and master complex video games like those from the Atari game suite.
    - Robotics:
      - Autonomous Vehicles: Developing driving policies for autonomous cars that can navigate complex traffic scenarios.
      - Robotic Manipulation: Robots learning to manipulate objects through trial and error, improving their ability to perform tasks such as assembly-line work or surgical operations.

<br>

## 2. A Variety of Neural Network

> We will reivew a high-level neural network architectures, which are <u> all based on the MLP.</u> The goal is to get to know deeper with the Inputs and Outputs of each of the number of nerural network systems. 



### Fully Connected Networks (MLPs)

- Cannot do  $\rightarrow$ Extend to handle other forms
  - efficently handle images and videos $\rightarrow$ Convolutional Neural Networks
  - sequntial inputs and sets (text, audio, etc.) $\rightarrow$ RNNs, Transformers
  - exotic data structures, (e.g.m graphs as input) $\rightarrow$ Graph Neural Networks

- Each kind of neural network arcitecture has layers that have particular properties
  - Can mix and match them
- Ofthen MLP layers are used near the top of many network architectures to facilitate classification / regression

<br>

### Inductive Bias

- Inductive Bias: <u>Assumptions built into the learning algorithm</u>
- Crucial in shaping how models learn and generalize. The architecture and configuration of a deep neural network itself introduces specific biases that affect the learning outcomes.
  - Using convlutions is an inductive bias, since they assume the same filter is useful across the entire image
- MLPs lack a strong inductive bias in the architecture
  - Can include some weak form in some losses or regularizers

<br>

### Convolutional Neural Networks

- Definition
  - A form of neural network that has an inductive bias
    - Architecture Design: CNNs are specifically <u>biased towards handling grid-like topology data</u>, such as <u>images</u>. This bias is introduced through the use of convolutional layers that enforce a local connectivity pattern, meaning <u>each unit in a layer is connected to a small region of the layer before it, mimicking the receptive fields of neurons in the human visual cortex.</u> This design assumes that local patterns are crucial and that these patterns are translation-invariant, making CNNs particularly effective for tasks where this assumption holds true (e.g., image and video recognition).
