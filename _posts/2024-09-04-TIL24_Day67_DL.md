---
title: "Day67 Deep Learning Lecture Review - Lecture 2 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, TIL_24]
toc: true 
---

# Lecture 2: Paradigms Beyond Supervised Learning and Neural Network

<img src="/blog/images/2024-09-04-TIL24_Day67_DL/E078B106-915B-47B6-B14F-816FCBFA5E13.jpeg"><br><br>



## 1. Paradigms Beyond Supervised Learning

### Types of Learning

- Supervised Learning
  - Training data includes desired output.
  - classification & regression



- Unsupervised Learning
  - Training data does not include the desired output.
  - clustering, dimensionality reduction, density estimation



- Self-Supervised Learning
  - Training data labels itself somehow.



- Reinforced Learning
  - A type of machine learning where an agent <u>learns to behave in an environment by performing actions and seeing the results, often as rewards or punishments</u>
  - In supervised learning, a function approximation such as 'f(x)=y' is given to the model, but in RL, within the environment, the agent can take actions at each time step, and <u>it is not told exactly what to do.</u>
  - The model gets a scaler reward signal.
    - ex) Won the game $\rightarrow$ +1, or lost the game -1
  - Wants to maximize the reward
  - Examples
    - Gaming:
      - AlphaGo: The program developed by DeepMind that learns to play the board game Go and can compete against top human players.
      - Video Game Play: Agents developed to play and master complex video games like those from the Atari game suite.
    - Robotics:
      - Autonomous Vehicles: Developing driving policies for autonomous cars that can navigate complex traffic scenarios.
      - Robotic Manipulation: Robots learning to manipulate objects through trial and error, improving their ability to perform tasks such as assembly-line work or surgical operations.

<br>

## 2. A Variety of Neural Network

> We will reivew a high-level neural network architectures, which are <u> all based on the MLP.</u> The goal is to get to know deeper with the Inputs and Outputs of each of the number of nerural network systems. 



### Fully Connected Networks (MLPs)

- Cannot do  $\rightarrow$ <u>Extend to handle other forms</u>
  - efficently handle images and videos $\rightarrow$ Convolutional Neural Networks
  - sequntial inputs and sets (text, audio, etc.) $\rightarrow$ RNNs, Transformers
  - exotic data structures, (e.g.m graphs as input) $\rightarrow$ Graph Neural Networks

- Each kind of neural network arcitecture has layers that have particular properties
  - Can mix and match them
- Ofthen MLP layers are used near the top of many network architectures to facilitate classification / regression

<br>

### Inductive Bias

- Inductive Bias: <u>Assumptions built into the learning algorithm</u>
- Crucial in shaping how models learn and generalize. The architecture and configuration of a deep neural network itself introduces specific biases that affect the learning outcomes.
  - Using convlutions is an inductive bias, since they assume the same filter is useful across the entire image
- MLPs lack a strong inductive bias in the architecture
  - Can include some weak form in some losses or regularizers

<br>

### Convolutional Neural Networks

- Definition
  - A form of neural network that <u>has an inductive bias</u>
    - Use convolutional units (spatially tied weights) implemented via linear filtering

- A deep learning neural network architecture <u>that learns directly from data</u>. CNNs are particularly useful for finding patterns in images for object, class, and category recognition.



- Fully Connected MLP network vs. Convolutional Neural Network

  <center>
    <img src="/blog/images/2024-09-04-TIL24_Day67_DL/image-20240905193316994.png" width="80%"><br>
    <Font size= 3pt><I>Perceptron <br>
      (Image from: https://www.simplilearn.com/tutorials/deep-learning-tutorial/multilayer-perceptron#:~:text=A%20fully%20connected%20multi%2Dlayer,a%20feedforward%20artificial%20neural%20network)</I></Font><br><br>
  </center>
  
  
  - Perceptron
    - Perceptron rule and Adaline rule were used to train a single-layer neural network
    - <u>Weights are updated based on a unit function in perceptron rule</u> or on a linear function in Adaline Rule.
  
  <center>
    <img src="/blog/images/2024-09-04-TIL24_Day67_DL/image-20240905193807751.png" width="80%"><br>
    <Font size= 3pt><I>Multi-Layered ANN<br>
      (Image from: https://www.simplilearn.com/tutorials/deep-learning-tutorial/multilayer-perceptron#:~:text=A%20fully%20connected%20multi%2Dlayer,a%20feedforward%20artificial%20neural%20network)</I></Font> <br><br>
  </center>
  

  - Multi-Layer Perceptron

    - <u>A fully connected multi-layer neural network</u> is called a Multilayer Perceptron (MLP).

    - It has 3 layers including one hidden layer. If it has more than 1 hidden layer, it is called a deep ANN. An MLP is a typical example of a feedforward artificial neural network.

    - To optimize a neural network, it's important to <u>adjust the number of layers and neurons</u>, which are referred to as **hyperparameters**. This process involves using cross-validation techniques to identify the most effective values for these parameters.
  
    - Weight adjustment training in neural networks involves **backpropagation**, a process in which the network adjusts the weights of its connections <u>to reduce errors.</u> Deeper neural networks can encounter <u>vanishing gradient problems</u>, where the gradient becomes extremely <u>small</u>, hindering the learning process. 
  
      | Feature         | MLP (Fully Connected) | CNN (Convolutional Neural Network)              |
      | --------------- | --------------------- | ----------------------------------------------- |
      | **Basic Unit**  | Neuron                | Convolutional Unit                              |
      | **Unit Input**  | Vector                | Tensor (3D for images: width, height, channels) |
      | **Unit Output** | 1 number (scalar)     | Feature Map (tensor)                            |

<br>

- How does Convolutional Network work?<br>
  
  <center>
    <img src="/blog/images/2024-09-04-TIL24_Day67_DL/image-20240905202317255.png" width="80%"><br><br><br>
  </center>
