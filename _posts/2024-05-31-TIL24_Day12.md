---
title: "Day12 ML Review - Gradient Descent (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, TIL_24]
toc: true
---

# Overview of Gradient Descent (2)

<img src="/blog/images/2024-05-31-TIL24_Day12/D5C9EEA5-F38D-4FE1-85B0-F5B45667D129.jpeg">

from https://angeloyeo.github.io/2020/08/16/gradient_descent.html

<br>

### Derivation of formula for gradient descent

Intuitively, the gradient descent method, also known as the steepest descent method, is a technique for finding the value of the independent variable that <u>minimizes the function value by adjusting the variable's value to decrease the function value.</u> <br>

If the amount of <u>data is very large,</u> the solution can be obtained more efficiently in terms of computational amount by finding the solution through an iterative method such as gradient descent.

<br>

![Screenshot 2024-05-31 at 2.09.13 PM](/images/2024-05-31-TIL24_Day12/Screenshot 2024-05-31 at 2.09.13 PM.png)

Gradient descent involves using the function's gradient to determine whether it reaches its minimum value when the value is adjusted.

- If the slope is positive, as the value increases, the function value also increases.
- Conversely, if the slope is negative, this means that as the value increases, the function value decreases.

Also, <u>a large slope value indicates a steep incline</u>, but it also signifies being far from the coordinates, while the position corresponds to the minimum/maximum value. 

<Br><br>

#### The direction component of the gradient

If the function's value increases as x increases (i.e., the slope is positive), you should move x in the negative direction. Conversely, if the function's value decreases as x increases (i.e., the slope is negative), you should move x in the positive direction.
<center>
  $$
  x_{i+1} = x_i - distance \times slope
  $$
</center>







If the function value increases as x increases at a specific point (the slope is positive), we need to move x in the negative direction. Conversely, if the function value at a specific point decreases as x increases (the slope is negative), we move x in the positive direction.

<Br><Br>

#### **Reference in Korean**

gradient descent는 함수의 기울기(즉, gradient)를 이용해 $x$의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법이라고 할 수 있다. <br>

기울기가 양수라는 것은 $x$ 값이 커질 수록 함수 값이 커진다는 것을 의미하고, 반대로 기울기가 음수라면 $x$값이 커질 수록 함수의 값이 작아진다는 것을 의미한다고 볼 수 있다.

또, 기울기의 값이 크다는 것은 가파르다는 것을 의미하기도 하지만, 또 한편으로는 $x$의 위치가 최소값/최댓값에 해당되는 $x$ 좌표로부터 멀리 떨어져있는 것을 의미하기도 한다.

<Br>

