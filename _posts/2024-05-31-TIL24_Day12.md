---
title: "Day12 ML Review - Gradient Descent (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, TIL_24]
toc: true
---

# Overview of Gradient Descent (2)

<img src="/blog/images/2024-05-31-TIL24_Day12/D5C9EEA5-F38D-4FE1-85B0-F5B45667D129.jpeg">

from https://angeloyeo.github.io/2020/08/16/gradient_descent.html

<br>

### Derivation of formula for gradient descent

At its core, the gradient descent method is a technique used in machine learning to **find the value of the independent variable that minimizes the function value.** It does this by adjusting the variable's value to decrease the function value. Think of it as a way to 'descend' to the lowest point of a function, much like a hiker navigating a mountainous terrain.

If <u>the data is very large,</u> finding the solution through an iterative method such as gradient descent can be more computationally efficient.

<br>

<img src="/blog/images/2024-05-31-TIL24_Day12/Screenshot 2024-05-31 at 2.09.13 PM.png">

Gradient descent involves using the function's gradient to determine whether it reaches its minimum value when the value is adjusted.

- If the slope is positive, the function value also increases as the value increases.
- Conversely, if the slope is negative, the function value decreases as the value increases.

Also, <u>a large slope value indicates a steep incline</u>, but it also signifies being far from the coordinates, while the position corresponds to the minimum/maximum value. 

If the function value increases as x increases at a specific point (the slope is positive), we need to move x in the negative direction. Conversely, if the function value at a specific point decreases as x increases (the slope is negative), we move x in the positive direction.

<Br>

#### **Reference in Korean**

gradient descent는 함수의 기울기(즉, gradient)를 이용해 $x$의 값을 어디로 옮겼을 때 함수가 최소값을 찾는지 알아보는 방법이라고 할 수 있다. <br>

기울기가 양수라는 것은 $x$ 값이 커질 수록 함수 값이 커진다는 것을 의미하고, 반대로 기울기가 음수라면 $x$값이 커질 수록 함수의 값이 작아진다는 것을 의미한다고 볼 수 있다.

또, 기울기의 값이 크다는 것은 가파르다는 것을 의미하기도 하지만, 또 한편으로는 $x$의 위치가 최소값/최댓값에 해당되는 $x$ 좌표로부터 멀리 떨어져있는 것을 의미하기도 한다.

<Br><br>

#### The direction component of the gradient

If the function's value increases as x increases (i.e., the slope is positive), we should move x in the negative direction. Conversely, if the function's value decreases as x increases (i.e., the slope is negative), you should move x in the positive direction.
<center>
  $$
  x_{i+1} = x_i - distance \times slope
  $$
</center>

<br>

Here, $x_i$ and $x_{i+1}$ are the coordinates of the $i^{th}$ and $i^{th+1}$ calculated x, respectively.

So, how should we consider the distance here? You can use the magnitude of the gradient.

<Br><br>

#### The Step of the Gradient



