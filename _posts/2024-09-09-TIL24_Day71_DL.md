---
title: "Day71 Deep Learning Lecture Review - Lecture 5 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, logisticRegression, TIL_24]
toc: true 
---

# Lecture 5: Transformers and Foundation Models (2)

<img src="/blog/images/2024-09-09-TIL24_Day71_DL/602ACAEF-72C9-4F1A-AA70-3DFE801A5607_1_105_c.jpeg"><br><br>

### Benefits of the Transformer:

1. **Parallelization**:
   - Unlike RNNs, which process input sequentially, the Transformer can process the entire input sequence in parallel, which significantly speeds up training and inference, especially on large datasets.
2. **Long-Range Dependencies**:
   - Self-attention allows the Transformer to capture long-range dependencies in the data. RNNs can struggle with this due to vanishing gradients, but the Transformer can easily attend to words that are far apart in a sequence.
3. **Scalability**:
   - Transformers scale effectively to very large datasets and models with billions of parameters. This scalability has led to models like GPT-3 and BERT, which achieve state-of-the-art results on numerous NLP tasks.
4. **Versatility**:
   - The Transformer architecture is highly versatile and has been adapted for a wide range of tasks beyond NLP, such as image processing (Vision Transformers) and speech recognition.

<br>

### Limitations of the Transformer:

1. **Computationally Expensive**:
   - The self-attention mechanism has a quadratic complexity in relation to the input sequence length. This means that for long sequences, the computation required can become very expensive.
2. **Lack of Inductive Bias**:
   - Unlike RNNs or CNNs, which have an inherent understanding of temporal order (RNNs) or spatial locality (CNNs), the Transformer relies on positional encodings to provide this information. This lack of inherent inductive bias can make the Transformer less effective when training on smaller datasets.

<Br>

### Popular Transformer-Based Models:

1. **BERT (Bidirectional Encoder Representations from Transformers)**:
   - BERT is a pre-trained Transformer model that uses the encoder portion of the Transformer architecture and focuses on bidirectional learning from text. It is pre-trained on masked language modeling tasks and has achieved state-of-the-art results in a wide range of NLP tasks.
2. **GPT (Generative Pre-trained Transformer)**:
   - GPT uses the decoder portion of the Transformer to generate text. Models like GPT-3 are extremely powerful at text generation and have been used in many applications, including conversation agents and text completion.
3. **T5 (Text-To-Text Transfer Transformer)**:
   - T5 treats every NLP task as a text-to-text problem, where both the input and output are represented as text strings. It uses the full Transformer architecture for a wide range of NLP tasks like translation, summarization, and more.
4. **Vision Transformer (ViT)**:
   - The Transformer architecture has also been applied to image data through Vision Transformers, which treat image patches as a sequence of tokens and apply self-attention to model global dependencies in the image.

<br><br>





