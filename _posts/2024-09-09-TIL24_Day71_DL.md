---
title: "Day71 Deep Learning Lecture Review - Lecture 5 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, classifier, transformer, selfAttention, TIL_24]
toc: true 
---

# Lecture 5: Self-Attention in Transformer Models

<img src="/blog/images/2024-09-09-TIL24_Day71_DL/602ACAEF-72C9-4F1A-AA70-3DFE801A5607_1_105_c.jpeg"><br><br>

<I><Font size="3pt">Explanation From: <a href="https://www.cloudthat.com/resources/blog/attention-mechanisms-in-transformers#introduction">Cloud That "Attention Mechanisms in Transformers"</a></Font></I>



## Step by Step of Self Attention

>**Self-attention**, called <u>scaled dot-product attention</u>, is a mechanism within the Transformer architecture that enables <u>the model to calculate the importance of each word in a sentence while processing a particular word.</u> It can be visualized as a spotlight, directing its focus onto various parts of the sentence as the model analyzes each word. Mathematically, this mechanism is defined as follows:

- **Query, Key, and Value:** For a given word, the self-attention mechanism computes three vectors: Query (Q), Key (K), and Value (V). These vectors are learned during training.
- **Attention Scores:** The model calculates attention scores by taking the dot product of the Query vector for the current word and the Key vectors for all the words in the input sequence. These scores indicate how much focus each word should receive.
- **Softmax and Scaling:** The attention scores are passed through a softmax function to get a probability distribution. This distribution is then used to weigh the Value vectors, deciding how much each word’s information should contribute to the current word’s representation.
- **Weighted Sum:** Finally, the Value vectors are weighted by the attention scores and summed to create the new representation of the current word.<br><Br>



Let's continue to dive into the structure of Encoder and Decoder.



![image-20240913202608716](/images/2024-09-09-TIL24_Day71_DL/image-20240913202608716.png)

The Transformer stacks <u>several encoder layers</u>, a hyperparameter referred to as num_layers. If we consider the encoder as a single layer, one encoder layer is primarily divided into two sublayers. These are **self-attention** and **feed-forward neural networks**.

<br>

The attention function <u>computes the similarity between a given 'query' and each 'key.'</u> This computed similarity is then applied <u>as a weight to each 'value' mapped to the key</u>. Finally, it returns the weighted sum of all 'values' reflecting the similarities.

![image-20240913203658790](/images/2024-09-09-TIL24_Day71_DL/image-20240913203658790.png)

The hidden state of the decoder cell is Q, and the hidden state of the encoder cell is K, indicating that Q and K had different values. **In self-attention, Q, K, and V are all the same.** In the self-attention of the transformer, Q, K, and V are as follows:

\<Values in Attention\>

- Q = Queries: Hidden states of the decoder cell at all time steps
- K = Keys: Hidden states of the encoder cell at all time steps
- V = Values: Hidden states of the encoder cell at all time steps

\<Values in 'Self-Attention'\>

- Q: Word vectors of all words in the input sentence
- K: Word vectors of all words in the input sentence
- V: Word vectors of all words in the input sentence

<br>

### 1. Q, K, V Vector

Let's explore the mechanism of self-attention in Transformers. For example, let's consider converting the word vector for "student" into the vectors for Q, K, and V.

![image-20240913204451297](/images/2024-09-09-TIL24_Day71_DL/image-20240913204451297.png)

A smaller vector is produced <u>by multiplying the existing vector by the weight matrix.</u> Each weight matrix has a size of $d_{model} \times (d_{model}/num\_heads)$. <u>This weight matrix is learned during the training process</u>. For example, if $d_{model}$=512 and $num\_heads=8$, each vector is **multiplied by three different weight matrices** to obtain Q, K, and V vectors, each of size 64. The above figure shows the process of obtaining Q, K, and V vectors from the 'student' vector among the word vectors. Following the same process for all word vectors, "I, am, a, student." each word will acquire its corresponding Q, K, and V vectors.



### 2. Scaled dot-product Attention

If we obtained the Q, K, and V vectors, then from this point on, the process is the same as the conventional attention mechanism. Each Q vector computes **attention scores** **for all K vectors** and then determines the **attention distribution**, using this <u>to create a weighted sum of all V vectors,</u> which results in the attention value or context vector. This process is repeated for all Q vectors.

However, as previously mentioned in the attention chapter, there are various types of attention functions. In Transformers, *(the attention function does not solely use the dot product, which was used in the attention chapter, score(q,k)=q⋅k; instead,)* it uses **a modified attention function $score(q,k)=q⋅k/ \sqrt{n}$**, where it is divided by a specific value. 

This type of attention function is referred to as **scaled dot-product attention**, as it includes the scaling of values that was discussed in the dot-product attention from the attention chapter. Let's understand this through a diagram below.



![image-20240913205847789](/images/2024-09-09-TIL24_Day71_DL/image-20240913205847789.png)

As in the diagram above, the attention score shows ***how relevant*** the word "I" is to the words "I," "am," "a," and "student." In transformers, the value obtained from scaling the inner product of two vectors is chosen by using the square root of $d_k$ $\sqrt{d_k}$, which represents **the dimension of the K vector**. As previously mentioned, $d_k$ has a value of 64 according to the formula $d_{model}/num\_heads$, making $\sqrt{d_k}$ equal to 8.



![image-20240913210245340](/images/2024-09-09-TIL24_Day71_DL/image-20240913210245340.png)

Now, we use the **softmax** function on the attention score to obtain the attention distribution and calculate the attention value by performing a weighted sum with each V vector. This can also be called the **attention value** or context vector for word I. This process would be applied to each word, but there is a more straightforward way to calculate this. 





### 3. Batch processing through matrix operations

To process the Q, K, and V vectors for each word and perform scaled dot-product attention, we can use **matrix operations** instead of vector operations, which allows us to process in batches. Let's dive into the process using matrix operations. Instead of multiplying the weight matrix by each individual word vector, we can multiply the matrix of the entire sentence by the weight matrix to obtain the Q, K, and V matrices.



### Input Representation:

- **Input vectors**: <u>We start with <b>T</b> input vectors</u> $(x_1, x_2, ..., x_T)$, each having **d dimensions**. These vectors could represent word embeddings in an NLP task, or feature vectors in other contexts.

- **Matrix ($X$)**: The input vectors <u>are stacked together into a matrix $X$</u>. Each row in $X$ corresponds to one of the $T$ input vectors, so the matrix has a shape $(T,D)$ where:
  
  - $T$ is the number of input vectors (e.g., the number of words in a sentence),
  
  - $D$ is the dimensionality of each input vector.
  
    <br>
  

The three matrices **$ W_Q, W_K, W_V $** are **learned parameters** of the model, and the **self-attention mechanism** uses them to calculate the attention scores and output for each input vector.

This process allows each input vector to attend to all other vectors in the sequence in a **parallelizable** way, a vital feature of the Transformer model's efficiency.

To perform self-attention, we need to transform each input vector into three new vectors: **Key (K)**, **Query (Q)**, and **Value (V)**. <span style="background-color: #0080FE; color: white">These vectors are obtained by **linearly projecting** the input vectors through three separate learned weight matrices.</span>

1. **Key Vector $ K $**: 
   
   - The **Key** vectors help determine <u>how much attention one input should give to another.</u> They capture the information contained in each input vector.
   - Each input vector is multiplied by a weight matrix $W_K$ (with dimensions $ d \times d_k$) to produce the Key vectors. The equation is:
     
     <center>
       $ K = X \cdot W_K$<br><br>
     </center>
     
     where $ W_K $ is the matrix of learned weights.
   
   <Br>
   
2. **Query Vector $Q$**:
   
   - The **Query** vectors represent what the model is "querying" or "looking for" in other vectors. They are responsible <u>for finding relevant information in other input vectors.</u>
   - Each input vector is multiplied by a different weight matrix $ W_Q $ (with dimensions $ d \times d_q $) to produce the Query vectors. The equation is:
     <center>
       $Q = X \cdot W_Q$ <br><br>
     </center>
     
     where $ W_Q $ is another learned matrix of weights. 
   
   <br>
   
3. **Value Vector $ V $**:
   - The **Value** vectors are the actual content or information that will be passed to the output. After the attention scores are computed, the output is a weighted sum of the Value vectors.
   - Each input vector is multiplied by a third weight matrix $ W_V $ (with dimensions $d \times d_v $) to produce the Value vectors. The equation is:
     
     <center>
     $  V = X \cdot W_V$ <br><br>
     </center>
     
     
     
     where $ W_V $ is yet another learned matrix of weights.
   
   <Br><Br>

### How These Vectors Work Together:
- **Keys** and **Queries** are used to compute **attention scores**:
  - The attention mechanism compares the Query vector of one input vector to the Key vectors of all other input vectors. This comparison helps the model decide how much attention one input should give to another.
  - The attention score is usually computed as the **dot product** of the Query and Key vectors, and then normalized using the softmax function.

- **Values** are weighted by attention scores:
  - Once the attention scores are computed, they are used to weigh the Value vectors. These weighted Value vectors are then combined to produce the final output for each input vector.
  





<br><br>

### Benefits of the Transformer:

1. **Parallelization**:
   - Unlike RNNs, which process input sequentially, the Transformer can process the entire input sequence in parallel, which significantly speeds up training and inference, especially on large datasets.
2. **Long-Range Dependencies**:
   - Self-attention allows the Transformer to capture long-range dependencies in the data. RNNs can struggle with this due to vanishing gradients, but the Transformer can easily attend to words that are far apart in a sequence.
3. **Scalability**:
   - Transformers scale effectively to very large datasets and models with billions of parameters. This scalability has led to models like GPT-3 and BERT, which achieve state-of-the-art results on numerous NLP tasks.
4. **Versatility**:
   - The Transformer architecture is highly versatile and has been adapted for a wide range of tasks beyond NLP, such as image processing (Vision Transformers) and speech recognition.

<br>

### Limitations of the Transformer:

1. **Computationally Expensive**:
   - The self-attention mechanism has a quadratic complexity in relation to the input sequence length. This means that for long sequences, the computation required can become very expensive.
2. **Lack of Inductive Bias**:
   - Unlike RNNs or CNNs, which have an inherent understanding of temporal order (RNNs) or spatial locality (CNNs), the Transformer relies on positional encodings to provide this information. This lack of inherent inductive bias can make the Transformer less effective when training on smaller datasets.

<Br>

### Popular Transformer-Based Models:

1. **BERT (Bidirectional Encoder Representations from Transformers)**:
   - BERT is a pre-trained Transformer model that uses the encoder portion of the Transformer architecture and focuses on bidirectional learning from text. It is pre-trained on masked language modeling tasks and has achieved state-of-the-art results in a wide range of NLP tasks.
2. **GPT (Generative Pre-trained Transformer)**:
   - GPT uses the decoder portion of the Transformer to generate text. Models like GPT-3 are extremely powerful at text generation and have been used in many applications, including conversation agents and text completion.
3. **T5 (Text-To-Text Transfer Transformer)**:
   - T5 treats every NLP task as a text-to-text problem, where both the input and output are represented as text strings. It uses the full Transformer architecture for a wide range of NLP tasks like translation, summarization, and more.
4. **Vision Transformer (ViT)**:
   - The Transformer architecture has also been applied to image data through Vision Transformers, which treat image patches as a sequence of tokens and apply self-attention to model global dependencies in the image.

<br><br>



