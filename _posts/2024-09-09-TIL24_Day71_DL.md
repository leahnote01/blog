---
title: "Day71 Deep Learning Lecture Review - Lecture 5 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, classifier, transformer, selfAttention, TIL_24]
toc: true 
---

# Lecture 5: Self-Attention in Transformer Models

<img src="/blog/images/2024-09-09-TIL24_Day71_DL/602ACAEF-72C9-4F1A-AA70-3DFE801A5607_1_105_c.jpeg"><br><br>

<I><Font size="3pt">Explanation From: <a href="https://www.cloudthat.com/resources/blog/attention-mechanisms-in-transformers#introduction">Cloud That "Attention Mechanisms in Transformers"</a></Font></I>



>**Self-attention**, referred to as <u>scaled dot-product attention</u>, is a mechanism within the Transformer architecture that enables <u>the model to calculate the importance of each word in a sentence while processing a particular word.</u> It can be visualized as a spotlight directing its focus onto various parts of the sentence as the model analyzes each word. Mathematically, this mechanism is defined as follows:

- **Query, Key, and Value:** For a given word, the self-attention mechanism computes three vectors: Query (Q), Key (K), and Value (V). These vectors are learned during training.
- **Attention Scores:** The model calculates attention scores by taking the dot product of the Query vector for the current word and the Key vectors for all the words in the input sequence. These scores indicate how much focus each word should receive.
- **Softmax and Scaling:** The attention scores are passed through a softmax function to get a probability distribution. This distribution is then used to weigh the Value vectors, deciding how much each word’s information should contribute to the current word’s representation.
- **Weighted Sum:** Finally, the Value vectors are weighted by the attention scores and summed to create the new representation of the current word.<br><Br>



Let's continue to dive into the structure of Encoder and Decoder.



![image-20240913202608716](/images/2024-09-09-TIL24_Day71_DL/image-20240913202608716.png)

The Transformer stacks a number of encoder layers, which is a hyperparameter referred to as num_layers. If we consider the encoder as a single layer, one encoder layer is largely divided into two sublayers. These are self-attention and feed-forward neural networks. In the illustration above, it mentions multi-head self-attention and position-wise feed-forward neural networks, but multi-head self-attention means that self-attention is used in parallel, and position-wise feed-forward neural networks refer to the standard feed-forward neural networks that we are familiar with.











## Step by Step of Self-Attention

![image-20240913133624204](/images/2024-09-09-TIL24_Day71_DL/image-20240913133624204.png)

### Input Representation:

- **Input vectors**: <u>We start with <b>T</b> input vectors</u> $(x_1, x_2, ..., x_T)$, each having **d dimensions**. These vectors could represent word embeddings in an NLP task, or feature vectors in other contexts.

- **Matrix ($X$)**: The input vectors <u>are stacked together into a matrix $X$</u>. Each row in $X$ corresponds to one of the $T$ input vectors, so the matrix has a shape $(T,D)$ where:
  
  - $T$ is the number of input vectors (e.g., the number of words in a sentence),
  
  - $D$ is the dimensionality of each input vector.
  
    <br>
  

### Linear Projections: Key, Query, and Value
> **Key (K)**: Encodes the content of each input, determining the attention weights.
>
> **Query (Q)**: Encodes what each input is looking for in other inputs.
>
> **Value (V)**: Holds the information that will be passed on, weighted by the attention scores.



The three matrices **$ W_Q, W_K, W_V $** are **learned parameters** of the model, and the **self-attention mechanism** uses them to calculate the attention scores and output for each input vector.

This process allows each input vector to attend to all other vectors in the sequence in a **parallelizable** way, a vital feature of the Transformer model's efficiency.

To perform self-attention, we need to transform each input vector into three new vectors: **Key (K)**, **Query (Q)**, and **Value (V)**. <span style="background-color: #0080FE; color: white">These vectors are obtained by **linearly projecting** the input vectors through three separate learned weight matrices.</span>

1. **Key Vector $ K $**: 
   
   - The **Key** vectors help determine <u>how much attention one input should give to another.</u> They capture the information contained in each input vector.
   - Each input vector is multiplied by a weight matrix $W_K$ (with dimensions $ d \times d_k$) to produce the Key vectors. The equation is:
     
     <center>
       $ K = X \cdot W_K$<br><br>
     </center>
     
     where $ W_K $ is the matrix of learned weights.
   
   <Br>
   
2. **Query Vector $Q$**:
   
   - The **Query** vectors represent what the model is "querying" or "looking for" in other vectors. They are responsible <u>for finding relevant information in other input vectors.</u>
   - Each input vector is multiplied by a different weight matrix $ W_Q $ (with dimensions $ d \times d_q $) to produce the Query vectors. The equation is:
     <center>
       $Q = X \cdot W_Q$ <br><br>
     </center>
     
     where $ W_Q $ is another learned matrix of weights. 
   
   <br>
   
3. **Value Vector $ V $**:
   - The **Value** vectors are the actual content or information that will be passed to the output. After the attention scores are computed, the output is a weighted sum of the Value vectors.
   - Each input vector is multiplied by a third weight matrix $ W_V $ (with dimensions $d \times d_v $) to produce the Value vectors. The equation is:
     
     <center>
     $  V = X \cdot W_V$ <br><br>
     </center>
     
     
     
     where $ W_V $ is yet another learned matrix of weights.
   
   <Br><Br>

### How These Vectors Work Together:
- **Keys** and **Queries** are used to compute **attention scores**:
  - The attention mechanism compares the Query vector of one input vector to the Key vectors of all other input vectors. This comparison helps the model decide how much attention one input should give to another.
  - The attention score is usually computed as the **dot product** of the Query and Key vectors, and then normalized using the softmax function.

- **Values** are weighted by attention scores:
  - Once the attention scores are computed, they are used to weigh the Value vectors. These weighted Value vectors are then combined to produce the final output for each input vector.
  







<br><br>

### Benefits of the Transformer:

1. **Parallelization**:
   - Unlike RNNs, which process input sequentially, the Transformer can process the entire input sequence in parallel, which significantly speeds up training and inference, especially on large datasets.
2. **Long-Range Dependencies**:
   - Self-attention allows the Transformer to capture long-range dependencies in the data. RNNs can struggle with this due to vanishing gradients, but the Transformer can easily attend to words that are far apart in a sequence.
3. **Scalability**:
   - Transformers scale effectively to very large datasets and models with billions of parameters. This scalability has led to models like GPT-3 and BERT, which achieve state-of-the-art results on numerous NLP tasks.
4. **Versatility**:
   - The Transformer architecture is highly versatile and has been adapted for a wide range of tasks beyond NLP, such as image processing (Vision Transformers) and speech recognition.

<br>

### Limitations of the Transformer:

1. **Computationally Expensive**:
   - The self-attention mechanism has a quadratic complexity in relation to the input sequence length. This means that for long sequences, the computation required can become very expensive.
2. **Lack of Inductive Bias**:
   - Unlike RNNs or CNNs, which have an inherent understanding of temporal order (RNNs) or spatial locality (CNNs), the Transformer relies on positional encodings to provide this information. This lack of inherent inductive bias can make the Transformer less effective when training on smaller datasets.

<Br>

### Popular Transformer-Based Models:

1. **BERT (Bidirectional Encoder Representations from Transformers)**:
   - BERT is a pre-trained Transformer model that uses the encoder portion of the Transformer architecture and focuses on bidirectional learning from text. It is pre-trained on masked language modeling tasks and has achieved state-of-the-art results in a wide range of NLP tasks.
2. **GPT (Generative Pre-trained Transformer)**:
   - GPT uses the decoder portion of the Transformer to generate text. Models like GPT-3 are extremely powerful at text generation and have been used in many applications, including conversation agents and text completion.
3. **T5 (Text-To-Text Transfer Transformer)**:
   - T5 treats every NLP task as a text-to-text problem, where both the input and output are represented as text strings. It uses the full Transformer architecture for a wide range of NLP tasks like translation, summarization, and more.
4. **Vision Transformer (ViT)**:
   - The Transformer architecture has also been applied to image data through Vision Transformers, which treat image patches as a sequence of tokens and apply self-attention to model global dependencies in the image.

<br><br>





