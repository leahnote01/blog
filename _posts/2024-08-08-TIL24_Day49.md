---
title: "Day49 ML Review - Data Preprocessing (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,logisticRegression,TIL_24]
toc: true 
---

# Partitioning a Dataset into Training & Test Datasets, 

<img src="/blog/images/2024-08-08-TIL24_Day49/A65867D9-34DE-4631-8310-3385D650DA00.jpeg">

<br><br>

## Partitioning a Dataset

> A convenient way to randomly split this dataset into separate test and training datasets is to use the train_test_split function from scikit-learn's `model_selection `submodule.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)
```

Providing the class label array `y` as an argument to `stratify` ensures that both the training and test datasets have the same class proportions as the original dataset.

<br>

### Choosing an Appropriate Ratio

When deciding on the division of a dataset into training and test datasets, it's essential to carefully consider the <u>tradeoff</u> involved. The commonly employed splits include the **60:40, 70:30, or 80:20** ratios, which are chosen <u>based on the initial dataset's size</u>. For larger datasets, options such as <u>90:10 or 99:1</u> splits are frequently utilized and are considered appropriate due to their ability to ensure a robust evaluation of the model's performance.

After training and evaluating a model, it is often beneficial t<u>o retrain the classifier with the entirety</u> of the dataset rather than discarding the allocated test data. This approach has the potential to enhance the model's predictive performance. However, <u>when dealing with a small dataset or one that contains outliers in the test data, retraining on the entire dataset could result in poorer generalization performance.</u> Furthermore, after retraining the model with the complete dataset, there will be no independent data remaining to assess its performance.

<br>

## Feature Scaling

