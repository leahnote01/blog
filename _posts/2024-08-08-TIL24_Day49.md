---
title: "Day49 ML Review - Data Preprocessing (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,logisticRegression,TIL_24]
toc: true 
---

# Partitioning a Dataset into Training & Test Datasets, Feature Scaling, and How To Select Meaningful Features

<img src="/blog/images/2024-08-08-TIL24_Day49/A65867D9-34DE-4631-8310-3385D650DA00.jpeg">

<br><br>

## Partitioning a Dataset

> A convenient way to randomly split this dataset into separate test and training datasets is to use the train_test_split function from scikit-learn's `model_selection `submodule.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)
```

Providing the class label array `y` as an argument to `stratify` ensures that the training and test datasets have the same class proportions as the original dataset. <br>

### Choosing an Appropriate Ratio

The <u>tradeoff</u> involved must be carefully considered when dividing a dataset into training and test datasets. The commonly employed splits include the **60:40, 70:30, or 80:20** ratios, chosen <u>based on the initial dataset's size</u>. For larger datasets, options such as <u>90:10 or 99:1</u> splits are frequently utilized and are considered appropriate due to their ability to ensure a robust evaluation of the model's performance.

After training and evaluating a model, it is often beneficial t<u>o retrain the classifier with the entirety</u> of the dataset rather than discarding the allocated test data. This approach has the potential to enhance the model's predictive performance. However, retraining on the entire dataset could result in poorer generalization performance <u>when dealing with a small dataset or one that contains outliers in the test data.</u> Furthermore, no independent data will remain to assess its performance after retraining the model with the complete dataset. <br>



## Feature Scaling

It's essential to ensure that features are on the same scale for optimal performance when working with machine learning and optimization algorithms. This is especially crucial, for example, when using the gradient descent optimization algorithm, as it can significantly impact its behavior.

**Normalization** and **standardization** are common approaches to bringing different features onto the same scale.  <br><br>

### Normalization

Normalization usually involves rescaling the features to a range of [0,1] and can be thought of as a form of **min-max scaling.**

To normalize our data, we can apply the min-max scaling to each feature column, where the new value, $x^{(i)}_{norm}$ of example, $x^{(i)}$can be calculated as follows. 

<center>
  $x^{(i)}_{norm} = \frac{x^{(i)}-x_{min}}{x^{max} - x^{min}}$ <br><br>
</center>

Here, $x^{(i)}$ is a particular example, $x_{min}$ is the smallest value in a feature column, and $x_{max}$ is the largest value. <br>

The min-max scaling procedure is implemented in scikit-learn and can be used as follows.

```python
from sklearn.preprocessing import MinMaxScaler
mms = MinMaxScaler()
X_train_norm = mms.fit_transform(X_train)
X_test_norm = mms.transform(X_test)
```

<br>

### Standardization

In various linear models, such as logistic regression and Support Vector Machines (SVM), the process begins by setting the weights to `0` or <u>small random values close to 0</u>. Standardization involves adjusting the feature columns to have a **mean of 0** and **a standard deviation of 1** (standard normal distribution). This transformation facilitates the learning of weights in the model. Moreover, standardization maintains <u>essential information about outliers and diminishes the model's susceptibility to these extreme values.</u> This sets it apart from min-max scaling, compressing the data into a specific range of values.

The following equation can express the standardization:

<center>
  $x^{i}_{std} = \frac{x^{(i)}-\mu_{x}}{\sigma_{x}}$ <br><br>
  Here, $\mu_{x}$ is the sample mean of a particular feature column,<br>
  and, $\sigma_{x}$ is the corresponding standard deviation. <br><br>
</center>

Again, it is also important to highlight that we fit the `StandardScaler` class **only once**—on the training data—and use those parameters to transform the test dataset or any new data point.

Scikit-learn offers advanced feature scaling methods like `RobustScaler`, which is ideal for small datasets with outliers. It operates on each feature column independently, removing the median and scaling the data based on the 1st and 3rd quartile, minimizing the impact of extreme values and outliers.<br>



### Selecting Meaningful Features

When a model performs significantly better on a training dataset than the test dataset, it's a vital sign of overfitting. Overfitting occurs when a model tightly fits the parameters to the specific observations in the training dataset but fails to generalize well to new data, resulting in **high variance**. The overfitting is because our model is too complex for the training data. There are two ways to reduce overfitting: <u>regularization</u> and <u>dimensionality reduction</u>. 

We already handled these from here:

- Regularization: https://leahnote01.github.io/blog/til_24/TIL24_Day17/
- Dimensionality reduction: (partly) https://leahnote01.github.io/blog/til_24/TIL24_Day46/

<br>

**L2 regularization** is a method used to <u>decrease the complexity of a model</u> <u>by penalizing large individual weights</u>. Another approach to reduce model complexity is **L1 regularization**, which involves replacing the square of the weights <u>with the sum of the absolute values of the weights.</u> In contrast to L2 regularization, L1 regularization typically results in <u>sparse feature vectors,</u> with most feature weights being zero. This sparsity can be beneficial when dealing with high-dimensional datasets containing many irrelevant features, especially <u>when there are more irrelevant dimensions than training examples</u>. From this perspective, L1 regularization is a technique for feature selection.

<br>

#### Dimensionality Reduction

