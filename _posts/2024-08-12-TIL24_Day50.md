---
title: "Day50 ML Review - Compressing Data (1)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, dimensionalityReduction, TIL_24]
toc: true 
---

# Compressing Data via Dimensionality Reduction, and Summary of PCA

<img src="/blog/images/2024-08-12-TIL24_Day50/0505D9E2-E0A1-486D-BA1B-1492FB424D59.jpeg"><br><br>

In my earlier articles, I talked about different methods for decreasing the dimensionality of a dataset through various feature selection techniques. Another way to achieve dimensionality reduction is through feature extraction.

To conduct feature extraction, it's crucial to convert the data in a dataset **into a new feature subspace with lower dimensionality** than the original. Data compression is vital, enabling us to store and analyze the growing volume of generated and gathered data.

In summary, there are three principal methodologies for data compression:

- **Principal Component Analysis (PCA)**: for unsupervised data compression. We discussed it before.
- **Linear Discriminant Analysis(LDA)**: as a supervised dimensionality reduction technique for maximizing class separability.
- Nonlinear dimensionality reduction via **Kernel Principal Component Analysis (KPCA)**. <br>



## Principal Component Analysis (PCA)

We have discussed this subject frequently in my previous posts here:

- [Day05 ML Review - Principle Component Analysis (1)](https://leahnote01.github.io/blog/til_24/TIL24_Day5/)
- [Day06 ML Review - Principle Component Analysis (2)](https://leahnote01.github.io/blog/til_24/TIL24_Day6/)
- [Day18 ML Review - Principle Component Analysis (3)](https://leahnote01.github.io/blog/til_24/TIL24_Day18/)



To recap those ideas, just like feature selection, we have various feature extraction methods at our disposal to decrease the number of features in a dataset. <u>Feature selection involves retaining the original features,</u> as seen in algorithms like Sequential Backward Selection, <u>while feature extraction entails transforming or projecting the data onto a new feature space.</u> The feature extraction process enhances data storage capacity and predictive accuracy by addressing the curse of dimensionality, particularly in the case of non-regularized models.

PCA enables us to recognize data patterns by examining the relationships between features. Simply put, PCA is designed to discover <u>the most variable directions in data</u> with many dimensions and then <u>map the data onto a new space with the same number or fewer dimensions.</u> The perpendicular axes (principal components) in the new space represent the most variable directions, considering that the new feature axes are perpendicular to each other, as shown in the illustration below:

![image-20240819155638052](/images/2024-08-12-TIL24_Day50/image-20240819155638052.png)

When PCA is employed for reducing dimensionality, we create a transformation matrix, $W$, with dimensions $d \times k$. This matrix enables us to transform a vector, $x$, representing the features of a training example, **in a new $k$-dimensional feature subspace** <u>with fewer dimensions</u> compared to the original d-dimensional feature space.

After transforming the original $d$-dimensional data to a new $k$-dimensional subspace, **PC1** will have the <u>highest variance possible</u>. <u>All subsequent principal components will have the greatest variability while being uncorrelated (orthogonal)</u> to the other principal components. Even if the input features are correlated, the resulting principal components will be mutually orthogonal (uncorrelated). *PCA directions are greatly influenced by data scaling, and it's necessary to standardize the features before PCA if the features were measured on different scales and equal importance needs to be assigned to all features.*

PCA follows the steps below.

1. Standardize the d-dimensional dataset. 
2. Construct the covariance matrix. 
3. Decompose the covariance matrix into its eigenvectors and eigenvalues. 
4. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
5.  Select k eigenvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace ().
6. Construct a projection matrix, W, from the "top" k eigenvectors. 
7. Transform the d-dimensional input dataset, X, using the projection matrix, W, to obtain the new k-dimensional feature subspace.



<br><br>

