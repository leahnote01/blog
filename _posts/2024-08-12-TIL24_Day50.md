---
title: "Day50 ML Review - Compressing Data (1)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, dimensionalityReduction, TIL_24]
toc: true 
---

# Compressing Data via Dimensionality Reduction, PCA, and LDA

<img src="/blog/images/2024-08-12-TIL24_Day50/0505D9E2-E0A1-486D-BA1B-1492FB424D59.jpeg"><br><br>

In my earlier articles, I talked about different methods for decreasing the dimensionality of a dataset through various feature selection techniques. Another way to achieve dimensionality reduction is through feature extraction.

To conduct feature extraction, it's crucial to convert the data in a dataset **into a new feature subspace with lower dimensionality** than the original. Data compression is vital, enabling us to store and analyze the growing volume of data being generated and gathered.

In summary, there are three principal methodologies for data compression:

- **Principal Component Analysis (PCA)**: for unsupervised data compression. We discussed it before.
- **Linear Discriminant Analysis(LDA)**: as a supervised dimensionality reduction technique for maximizing class separability.
- Nonlinear dimensionality reduction via **Kernel Principal Component Analysis (KPCA)**



## Principal Component Analysis (PCA)

We have discussed this subject frequently in my previous posts here:

- [Day05 ML Review - Principle Component Analysis (1)](https://leahnote01.github.io/blog/til_24/TIL24_Day5/)
- [Day06 ML Review - Principle Component Analysis (2)](https://leahnote01.github.io/blog/til_24/TIL24_Day6/)
- [Day18 ML Review - Principle Component Analysis (3)](https://leahnote01.github.io/blog/til_24/TIL24_Day18/)



To recap those ideas, just like feature selection, we have various feature extraction methods at our disposal to decrease the number of features in a dataset. <u>Feature selection involves retaining the original features,</u> as seen in algorithms like Sequential Backward Selection, <u>while feature extraction entails transforming or projecting the data onto a new feature space.</u>









<br><br>

