---
title: "Day93 Deep Learning Lecture Review - Fine-Tuning Models (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, LoRA, fine-tuning, TIL_24]
toc: true 
---

# (Editing) Comparing Pre-trained model embeddings (ResNet+SBERT vs. CLIP) 

![65DF05AD-D9C7-42F4-9268-0FAAEACDEE43_1_105_c](/images/2024-10-23-TIL24_Day93_DL/65DF05AD-D9C7-42F4-9268-0FAAEACDEE43_1_105_c.jpeg)

<br><br>

<I>In my previous post, I employed two pre-trained model configurations—ResNet and SBERT—to create embeddings for **Visual Question Answering (VQA) tasks**, allowing for comparison with the CLIP model. I will resume my analysis in this post by implementing the CLIP model.</I>

<br>

**4. Embedding Extraction Using CLIP**

I utilized the CLIP model in the second setup, incorporating a visual encoder (ViT-B/32) and a textual encoder. Both yielded embeddings of dimension 512. I noted that CLIP's joint training of visual and textual encoders delivered slightly improved performance compared to the ResNet + SBERT setup, achieving a final test accuracy of 0.3767. Below is the detailed code for extracting embeddings and training with CLIP.<br>

- **CLIP** (Contrastive Language–Image Pretraining) is designed to align visual and textual representations in the same space. It jointly trains a visual encoder and a textual encoder on a massive dataset of image-text pairs, making it well-suited for tasks like VQA, where both image and text information are critical.

<center>
  <img src="/blog/images/2024-10-17-TIL24_Day92_DL/image-20241027133803173.png"><br><br><br>
</center>



```python
from transformers import CLIPProcessor, CLIPModel

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Extract image embedding
image = Image.open("path_of_the_image")
inputs = processor(images=image, return_tensors="pt")
with torch.no_grad():
  image_features = clip_model.get_image_features(**inputs)
  
# Extract text embedding
text = "This is a sample question."
inputs = processor(text=text, return_tenxors="pt")
with torch.no_grad():
  text_features = clip_model.get_text_features(**inputs)
```

The visual and textual embeddings produced by CLIP were both of dimension 512, resulting in a concatenated embedding of dimension. This more balanced representation provided better alignment between visual and textual information, likely contributing to higher accuracy.

- **CLIP?** CLIP models (in my experiment, `ViT-B/32`) are trained to match images and text representations, which makes them <u>inherently better at understanding the relationship between the two modalities.</u> By using both encoders simultaneously, CLIP generates embeddings that are more likely to be aligned meaningfully, improving performance for multimodal tasks like VQA.
- **Embedding dimensionality (512):** CLIP's visual and textual embeddings are of dimension 512. 



**5. Comparison of Setups**

- **ResNet+SBERT**: This setup involves extracting features independently from both modalities (image and text) and concatenating them. Since these models were not trained together, the embeddings might not be perfectly aligned, which could lead to slightly lower performance. Nevertheless, combining a powerful CNN for image embeddings(ResNet-50) and a strong sentence embedding model (SBERT) still provides reasonably good results.
- **CLIP**: CLIP's joint training of visual and textual encoders ensures that the image and text embeddings are more naturally aligned. This results in better performance on tasks like VQA, where the relationship between the image and the question is crucial. The fact that both embeddings have the same dimensionality (512) also makes it easier to combine them and train a downstream classifier without additional dimensionality reduction. 

<br>

In conclusion, CLIP outperforms the ResNEt + SBERT setup because it was designed to create more harmonious and aligned embeddings for text and images, directly benefiting tasks like VQA. Additionally, its balanced dimensionality(512) for image and text embeddings makes the classification task more manageable than the larger combined dimensionality in the ResNet + SBERT setup. 

<br><br>

