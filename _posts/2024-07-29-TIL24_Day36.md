---
title: "Day36 ML Review - Decision Tree (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,decisionTree,TIL_24]
toc: true 
---

#  Information Gain (2) - Classification Error &  Building a Decision Tree 

<img src="/blog/images/2024-07-29-TIL24_Day36/ED99A730-8EE3-4156-A6F2-9554CE5EC257.jpeg">

<br><br>

### Maximizing IG: Getting the Best Possible Value with the Best Efficiency

> We must establish an objective function to optimize through the tree learning algorithm to <u>split the nodes effectively based on the most informative features.</u> 

<br>

#### 2. Gini Impurity ($I_G$)

The Gini impurity can be understood as a criterion to minimize the probability of misclassification:

<center>
  $I_G(t) = \sum_{i=1}^{c} p(i \mid t)(1-p(i \mid t)) = 1 - \sum_{i=1}^{c}p(i \mid t)^2$ <br><br><br>
</center>


Like entropy, the Gini impurity is maximal when the classes are perfectly mixed. This occurs, for example, in a binary class setting $(c=2)$.

<center>
  $I_G(t) = 1-\sum_{i=1}^{c}0.5^2=0.5$
<Br><Br><br>
</center>


In practice, <u>both Gini impurity and entropy usually yield similar results.</u> It's not worth spending much time evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.



##### Calculation Example

Assume we have the same node as above:

- Class 0: 30 instances
- Class 1: 70 instances

The probabilities are:

<center>
  $p_0 = \frac{30}{100} = 0.3$, $\hspace{20 mm}$
  $p_1 = \frac{70}{100} = 0.7$ <br><br><br>
</center>




Then, the entropy is:

<center>
  $\text{Entropy} = - (0.3 \log_2 0.3 + 0.7 \log_2 0.7) $ <br>
  $= - (0.3 \cdot (-1.737) + 0.7 \cdot (-0.514)) $ <br>
  $= - (-0.521 - 0.360) $ <br>
  $= 0.881 $ <br><br>
</center>



<br><br>

#### 3. Classification Error

Classification error measurement is as follows.

<center>
  $I_E(t) = 1 - max\{ p(i \mid t) \} $
<br><br><Br>
</center>


This criterion is useful for pruning, but not recommended for growing a decision tree because it is less sensitive to changes in the class probabilities of the nodes.





<br><br>

