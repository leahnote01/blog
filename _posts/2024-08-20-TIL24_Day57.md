---
title: "Day57 ML Review - Cross Validation (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, TIL_24]
toc: true 
---

# Learning & Validation Curves, and Bias & Variance

<img src="/blog/images/2024-08-20-TIL24_Day57/B65A0D34-51FD-418F-A096-9FB5D8CCE1CE.jpeg"><br><br>

> By analyzing learning and validation curves, we can assess and diagnose the model and enhance the performance of a learning algorithm. **Learning curves** are instrumental in <u>determining whether a learning algorithm suffers from overfitting (high variance) or underfitting (high bias</u>). Additionally, the insights provided by **validation curves** enable us <u>to effectively address and rectify common issues encountered by a learning algorithm</u>.

A model with excessive parameters for a given training dataset can result in overfitting. This means the model becomes too tailored to the training data and performs poorly when exposed to new, unseen data. While one potential solution is acquiring more training examples, this approach can be financially prohibitive or unfeasible. By visually analyzing the changes in the model's training and validation accuracies as the size of the training dataset varies, we can effectively ascertain whether the model exhibits high variance or high bias and determine whether acquiring more data could effectively address this issue.<br><br>



<font size=4pt><I>Explanations from: <b>"Bias vs. Variance"</b>, Strictly By The Numbers. [Online]. Available: https://www.strictlybythenumbers.com/bias-vs-variance. [Accessed: 26-Aug-2024].</I></font>

### What is Bias?

Algorithmic bias, or bias in an ML model, occurs when the model **overemphasizes** certain features. This results in a complex model <u>because the model attempts to generalize the data.</u>

<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826172317531.png" width="70%"><br><br>
</center>



**The accuracy of our model is determined by analyzing the disparity between its average prediction and the actual value.**

When a model exhibits <u>high bias, it oversimplifies the data, reducing accuracy in both training and test datasets.</u>

Conversely, <u>a low-bias model indicates that it has not captured enough information about the target feature</u>, resulting in inaccuracies due to bias error.

- **Error due to bias**: Calculated as the most frequently observed difference between the expected prediction of our model and the correct value. (=ground truth)

<br><br>

### What is Variance?

As a measure of variability, variance helps us understand <u>how far apart a set of numbers is from its average value</u>.

<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826173226356.png" width="70%"><br><br>
</center>



The variance of a machine learning model refers to the <u>variability of the model's predictions for a specific data point, showing how spread out the data is.</u>

**High variance suggests that the model is overfitting and cannot generalize to new, unseen data.** While it may perform well on the training data, it will likely have significant inaccuracies due to variance error when applied to new data.

On the other hand, **low variance means that the predicted values align closely with the actual data, resulting in minor prediction errors**.

- **Error due to variance**: The variance shows how much the predictions for a given point vary between different realizations of the model [=expected variance for the predictions in different realizations of the model]

<br>

### **How are bias and variance related?**

The relationship between bias and variance can be described as **inverse**. When bias is high, variance tends to be low, and vice versa. Striking a balance between low bias and low variance in a model can be a complex task.

A model with low variance and high bias is advantageous as it lowers the risk of making inaccurate predictions. However, it <u>may require additional support</u> to properly generalize the dataset.

Conversely, a model with low bias and high variance has the ability to effectively generalize the dataset, but it may result in inaccurate predictions, ultimately adding complexity to the model.

<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826174138680.png"><br><br>
</center>



Looking at the graph above, it's evident that <u>when there is high bias, the model complexity is very low, but the error is very high, which affects the model's predictions</u>. On the other hand, <u>when there is low bias and high variance, the error is also high, making the model inaccurate.</u> In both cases, the training set needs to be generalized better.

To achieve a less complex model with low bias, we must find a balance between bias and variance. This trade-off should result in an optimized model complexity and minimized error. There are various methods to achieve this balance, which we will discuss in the following section.



<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826174735361.png" width="70%"><br><br>
</center>



In the graph above, we can observe the predictions and actual values for different scenarios of bias and variance. The optimal scenario is low bias and variance, but achieving this is impractical.

A scenario with high variance and low bias appears to be better than others, indicating a closer match between actual and predicted values. There are several ways to address this bias-variance trade-off.

One approach is to increase the model's complexity, which decreases bias and increases variance. However, maintaining an acceptable level of variance allows for good accuracy on the training data and minimizes total error, which is the sum of variance error and bias error.

Another method is to expand the training dataset to address overfitting. This also enables an increase in model complexity with minimal variance errors.

However, it's important to note that expanding the training dataset may not be adequate for low-bias models or underfitting situations. Therefore, this method should only be applied in high-variance scenarios.

<br><br>

