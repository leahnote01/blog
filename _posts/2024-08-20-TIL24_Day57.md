---
title: "Day57 ML Review - Cross Validation (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, TIL_24]
toc: true 
---

# Learning & Validation Curves, and Bias & Variance

<img src="/blog/images/2024-08-20-TIL24_Day57/B65A0D34-51FD-418F-A096-9FB5D8CCE1CE.jpeg"><br><br>

> By analyzing learning and validation curves, we can assess and diagnose the model and enhance the performance of a learning algorithm. **Learning curves** are instrumental in <u>determining whether a learning algorithm suffers from overfitting (high variance) or underfitting (high bias</u>). Additionally, the insights provided by **validation curves** enable us <u>to effectively address and rectify common issues encountered by a learning algorithm</u>.

A model with excessive parameters for a given training dataset can result in overfitting. This means the model becomes too tailored to the training data and performs poorly when exposed to new, unseen data. While one potential solution is acquiring more training examples, this approach can be financially prohibitive or unfeasible. By visually analyzing the changes in the model's training and validation accuracies as the size of the training dataset varies, we can effectively ascertain whether the model exhibits high variance or high bias and determine whether acquiring more data could effectively address this issue.<br><br>



<font size=4pt><I>Explanations from: <b>"Bias vs. Variance"</b>, Strictly By The Numbers. [Online]. Available: https://www.strictlybythenumbers.com/bias-vs-variance. [Accessed: 26-Aug-2024].</I></font>

### What is Bias?

Algorithmic bias, or bias in an ML model, occurs when the model **overemphasizes** certain features. This results in a complex model <u>because the model attempts to generalize the data.</u>

<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826172317531.png" width="70%"><br><br>
</center>



**The accuracy of our model is determined by analyzing the disparity between its average prediction and the actual value.**

When a model exhibits <u>high bias, it oversimplifies the data, reducing accuracy in both training and test datasets.</u>

Conversely, <u>a low-bias model indicates that it has not captured enough information about the target feature</u>, resulting in inaccuracies due to bias error.

- **Error due to bias**: Calculated as the most frequently observed difference between the expected prediction of our model and the correct value. (=ground truth)

<br><br>

### What is Variance?

As a measure of variability, variance helps us understand <u>how far apart a set of numbers is from its average value</u>.



<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826173226356.png" width="70%"><br><br>
</center>



The variance of a machine learning model refers to the <u>variability of the model's predictions for a specific data point, showing how spread out the data is.</u>

**High variance suggests that the model is overfitting and cannot generalize to new, unseen data.** While it may perform well on the training data, it will likely have significant inaccuracies due to variance error when applied to new data.

On the other hand, **low variance means that the predicted values align closely with the actual data, resulting in minor prediction errors**.

- **Error due to variance**: The variance shows how much the predictions for a given point vary between different realizations of the model [=expected variance for the predictions in different realizations of the model]

<br>

### **How are bias and variance related?**

The relationship between bias and variance can be described as **inverse**. When bias is high, variance tends to be low, and vice versa. Striking a balance between low bias and low variance in a model can be a complex task.

A model with low variance and high bias is advantageous as it lowers the risk of making inaccurate predictions. However, it <u>may require additional support</u> to properly generalize the dataset.

Conversely, a model with low bias and high variance has the ability to effectively generalize the dataset, but it may result in inaccurate predictions, ultimately adding complexity to the model.<br>



<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826174138680.png" width="70%"><br><br>
</center>




Looking at the graph above, it's evident that <u>when there is high bias, the model complexity is very low, but the error is very high, which affects the model's predictions</u>. On the other hand, <u>when there is low bias and high variance, the error is also high, making the model inaccurate.</u> In both cases, the training set needs to be generalized better.

We must balance bias and variance to achieve a less complex model with low bias. This <u>trade-off should result in an optimized model complexity and minimized error.</u> There are various methods to achieve this balance, which we will discuss in the following section.<Br>



<center>
  <img src="/blog/images/2024-08-20-TIL24_Day57/image-20240826174735361.png" width="70%"><br><br>
</center>



In the graph above, we can observe the predictions and actual values for different scenarios of bias and variance. The optimal scenario is low bias and variance, but achieving this is impractical.

A scenario **with high variance and low bias** is better than others, <u>indicating a closer match between actual and predicted values.</u> There are several ways to address this bias-variance trade-off.

One approach is to increase the model's complexity, which decreases bias and increases variance. However**, maintaining an acceptable level of variance allows for good accuracy on the training data and minimizes total error, which is the sum of variance error and bias error.**

Another method is to expand the training dataset to address overfitting. This also enables an increase in model complexity with minimal variance errors.

However, expanding the training dataset may need to be improved for low-bias models or underfitting situations. Therefore, this method should only be applied in high-variance scenarios.





### Summary

**Bias** is an error that occurs when a machine-learning model makes **wrong assumptions**. In simple terms, a high bias indicates a large gap between the actual data distribution and the data distribution that the model predicts. If the bias is too high, the model is likely to underfit.

**Variance** is the error that occurs due to **fluctuations in the training data**. In other words, if the training data has large variations and doesn't follow a consistent pattern, there's a good chance that a model well-trained on the data won't predict new data accurately. This is known as overfitting, and it is more likely to happen when the variance is high.

The **bias-variance trade-off** describes a relationship in which it's difficult to simultaneously improve both bias and variance in a model. Therefore, it's important to consider both factors when choosing a model.

- When <u>the bias is large and the variance is small, it's true for simpler models.</u> In this case, there is a significant difference between the data distribution and the model that fits it. However, even if the training data changes, the resulting model remains similar in shape.

- On the other hand, when <u>the bias is small and the variance is large</u>, it's more common in complex models. Here, the difference between the data distribution and the corresponding model is small, yet the shape of the resulting models varies as the training data changes.

<br><br>

