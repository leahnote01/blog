---
title: "Day89 ML Review - Ensemble Method (5)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, random forest, ensemble method, TIL_24]
toc: true 
---

# Revisiting Ensemble Method, Random Forest

<img src="/blog/images/2024-10-11-TIL24_Day89/82E621C9-1573-455E-8D34-E0340B9F2D1B.jpeg">

<I>Source: [Analytics Vidhya - Ensemble Learning Methods](https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/)</I>

As we further examine the Ensemble Method in machine learning, we find it integrates several machine learning models. One challenge in this field is that individual models often yield subpar results, and they are known as **weak learners**. To address this issue of low prediction accuracy, we merge multiple models to develop a more effective one.

As mentioned in earlier posts, a model with high bias or high variance leads to inaccurate predictions.

<center>
  <img src="/blog/images/2024-10-11-TIL24_Day89/image-20241012112953805.png">
  <img src="/blog/images/2024-10-11-TIL24_Day89/image-20241012112959670.png"> <br><Br>
</center>




- A **high-bias** model arises when the data isn't sufficiently learned or linked to its distribution. (Bias reflects how much a model’s prediction deviates from the target value based on training data. It results from simplifying assumptions <u>in the model</u>, making the target functions easier to approximate.)

- **High variance**, often overfitting, happens when a machine learning model or statistical algorithm pays too much attention to particular patterns in the training data, leading to poor generalization on new, unseen data. (Variance measures how much a random variable diverges from its expected value, using a single training set to evaluate prediction variability <u>across various training sets.</u>)



Ensemble learning aims to reduce the bias if we have a weak model with high bias and low variance. <br><br>



<I>Source: [IBM: What is Random Forest?](https://www.ibm.com/topics/random-forest#:~:text=Random%20forest%20is%20a%20commonly,Decision%20trees)</I>

## Random Forest

> The random forest algorithm builds on the **bagging method** by incorporating both bagging and feature randomness, resulting in a diverse collection of uncorrelated decision trees. Feature randomness allows for the creation of a random selection of features, which helps maintain low correlation between the decision trees.



- How it works

![image-20241012115441856](/images/2024-10-11-TIL24_Day89/image-20241012115441856.png)

<I>Image Source: [IBM: What is Random Forest?](https://www.ibm.com/topics/random-forest#:~:text=Random%20forest%20is%20a%20commonly,Decision%20trees)</I><br><br>

Random forest algorithms need **three** key hyperparameters before training: <u>node size, tree count, and sampled features.</u> After setting these, the classifier can tackle regression and classification tasks.

The random forest algorithm comprises multiple decision trees, where each tree is created from a bootstrap sample—a data sample taken from the training set with replacement.

The method of determining predictions depends on the type of problem. In **regression** tasks, the <u>predictions from individual decision trees are averaged,</u> while in **classification** tasks, the <u>most frequent categorical variable</u> is chosen through a majority vote. Then, the out-of-bag (OOB) sample is utilized for cross-validation to finalize the prediction.













<br><br>

