---
title: "Day89 ML Review - Ensemble Method (5)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, random forest, ensemble method, TIL_24]
toc: true 
---

# Revisiting Ensemble Method, Random Forest

<img src="/blog/images/2024-10-11-TIL24_Day89/82E621C9-1573-455E-8D34-E0340B9F2D1B.jpeg">

<I>Source: [Analytics Vidhya - Ensemble Learning Methods](https://www.analyticsvidhya.com/blog/2023/01/ensemble-learning-methods-bagging-boosting-and-stacking/)</I>

As we further examine the Ensemble Method in machine learning, we find it integrates several machine learning models. One challenge in this field is that individual models often yield subpar results, and they are known as **weak learners**. To address this issue of low prediction accuracy, we merge multiple models to develop a more effective one.

As mentioned in earlier posts, a model with high bias or high variance leads to inaccurate predictions.

![image-20241012112953805](/images/2024-10-11-TIL24_Day89/image-20241012112953805.png)

A high-bias model arises when the data isn't sufficiently learned, linked to its distribution. (Bias reflects how much a model’s prediction deviates from the target value based on training data. It results from simplifying assumptions in the model, making the target functions easier to approximate.)

![image-20241012112959670](/images/2024-10-11-TIL24_Day89/image-20241012112959670.png)

High variance, often refered as overfitting, happens when a machine learning model or statistical algorithm pays too much attention to particular patterns in the training data, leading to poor generalization on new, unseen data. (Variance measures how much a random variable diverges from its expected value, using a single training set to evaluate prediction variability across various training sets..)









<br><br>

