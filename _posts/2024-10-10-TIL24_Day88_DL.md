---
title: "Day88 Deep Learning Lecture Review - Lecture 10-12"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, LLM, NLP, TIL_24]
toc: true 
---

# Deep Learning & Numerical Precision(Floating Point), Hardware Considerations, and Distributed Model Training

![IMG_2981](/images/2024-10-10-TIL24_Day88_DL/IMG_2981.JPG)

<br><Br>

#### Bus Bottleneck

> A **bus bottleneck** refers to a situation in computer systems where the **data bus**, which connects different components (such as the CPU, memory, and peripherals), becomes a limiting factor for system performance. A bus is responsible for transmitting data between components, and when it cannot handle the volume of data being transferred, it becomes a **bottleneck**.

- Common **Causes** of Bus Bottlenecks
  - **Limited Bandwidth**: The data bus has a finite bandwidth, which defines how much data it can transfer per unit of time. Suppose the transmitted data exceeds the bus’s capacity, resulting in a bottleneck.
  - **High Traffic Load**: When multiple devices or components try to use the same bus simultaneously, the bus can become overwhelmed, leading to delays and slower data transmission.
- **Solutions** to Address Bus Bottlenecks:
  - <u>Increasing Bus Bandwidth</u>: Upgrading to buses with higher bandwidth can reduce bottlenecks. For example, **using PCIe (Peripheral Component Interconnect Express)** instead of older bus technologies like PCI or AGP can drastically improve component data transfer rates.
  - <u>Bus Architecture Optimizatio</u>n: Modern systems often use multiple buses (e.g., separate buses for CPU-memory communication and I/O devices) to reduce traffic on a single bus and avoid bottlenecks.
  - <u>Parallel Bus Systems</u>: Increasing the number of parallel data lines can improve throughput, as more data can be transmitted simultaneously.
- **SXM (Scalable Matrix Extension)** and **NVLink** can be used.<br><br>







#### Floating Point - FP32

> A floating point is a way to represent real numbers in a computer system using a finite number of bits.

- FP32 is the **32-bit floating-point** format commonly used in computer systems for numerical computations, following the IEEE 754 standard. FP32 (<u>also known as single-precision floating poin</u>t) uses 32 bits to represent real numbers, split into three components:

  - 1 bit for the **sign**: Indicates whether the number is positive (0) or negative (1).
  - 8 bits for the **exponent**: This encodes the scale of the number by representing the power of 2 multiplied by the number.
  - 23 bits for the **mantissa** (or significand): This represents the actual precision of the number.

- Half Precision: **FP16**

  - FP16 uses only <u>16 bit</u>s rather than 32 bits.
  - It has <u>less precision</u> and a smaller range of values.
  - FP16 is about **twice as fast as** FP32.

  <center>
    <img src="/blog/images/2024-10-10-TIL24_Day88_DL/image-20241009180016174.png"><br><br>
  </center>

  

  - Weights in FP16 require half as much VRAM.
  - If we can train in lower precision (fewer bits), we would have more GPU memory available, and
    arithmetic operations would be faster as FP16 uses half the amount of RAM as FP32.
  - Additionally, we could potentially use a bigger model and a bigger batch size, yielding an even greater speedup. 

- Remember that we must also have a VRAM dedicated to 2x your model size.

  - – 1x is your model
  - – 1x is your optimizer: Momentum, Adam, AdamW, etc.



- **Mixed Precision Training**
  - We often encounter numerical problems when using FP16 everywhere. Although it is okay for the forward pass, it is not sufficient for computing weight updates.
  - Mixed precision training combines FP32 and FP16 (or BFLOAT16 or TFLOAT32).
  - It enables shorter training times and lowers the memory requirements of training with bigger models, batch sizes, and inputs. 



- Automatic Mixed Precision (AMP)
  - Using mixed precision arithmetic could enhance computational efficiency. Instead of relying solely on 32-bit floating point (FP32) operations, <u>AMP dynamically scales between 16-bit floating point (FP16) and FP32 arithmetic during training.</u> 
  - Using FP16 for less critical calculations, AMP can reduce memory and speed up computations, especially on modern hardware like **NVIDIA GPUs** with **Tensor Cores** optimized for FP16.



- **BFLOAT16**
  - bfloat16 (Brain Floating Point 16) is a <u>16-bit floating-point</u> representation used primarily to accelerate deep learning tasks. Google introduced it, and hardware manufacturers like Intel and **NVIDIA** adopted it.
  - Unlike the standard 16-bit floating-point format (FP16), **bfloat16 retains the same range as a 32-bit floating-point (FP32) format but with <u>reduced precision</u>.** This makes it particularly useful in machine learning tasks, where having a wide dynamic range is essential, but precision can be sacrificed for faster computations.



- **Tensor Float 32**
  - **TF32** offers a middle ground between FP32 and **FP16** formats by balancing performance with precision.
  - It only offers **18 bits (NOT 32 bits)** and the same dynamic range as FP32 and BFLOAT16, with the same precision as FP16, whereas BFLOAT16 has less precision than FP16.



<center>
  <img src="/blog/images/2024-10-10-TIL24_Day88_DL/image-20241009181210116.png"><br><br>
</center>





- INT8

  - It refers to an 8-bit integer format used in machine learning and deep learning to represent numerical data.
  - **INT8 quantization** is the process of converting the weights and activations of a neural network from higher-precision formats (like **FP32**) into **8-bit integers**.
  - Two key methods are used:
    - **Post-training quantization:** Quantizing a model after it has been trained with floating-point values.
    - **Quantization-aware training**: <u>Training the model while simulating quantized arithmetic, leading to better accuracy after quantization</u>
  - **Accuracy Loss:** Since INT8 quantization reduces the precision of model weights and activations, it can lead to a slight degradation in model performance. This is particularly noticeable in models that rely on very fine-grained numerical precision.<br><br>

  

  

#### Hardware Considerations

- Different GPUs supports different precisions. Not all GPUs support bfloat16 or TF32. Some GPUs support special operations to enable mixed-precision training to be faster. 

- **Tensor Cores**
  - Tensor cores can <u>accelerate mixed precision training, but not all GPUs have them.</u>
  - Accelerated cores **for multiplying two FP16/BFLOAT16/TF32** matrices together and adding a FP32/FP16/BFLOAT16/TF32 matrix, but <u>returned value can be in FP32.</u>
  - When using Tensor cores for a speed up, this can greatly reduce the GPU utilization by having BatchNorm in the code.<br><br>



#### Prefetching

> **Prefetching** is a technique used in computer systems to improve performance by **loading data into the cache or memory before it is actually needed** by the CPU or other components.

- Prefetching **data loaders** have the data loaded and transformed in a <u>separate thread so that the GPU does not need to wait</u>. This is what PyTorch’s data loader does. 
- Dataloaders often allow for **parallel data loading** through multiple worker threads. This speeds up data retrieval, ensuring the model does not wait for data to be loaded during training and thus increasing training efficiency.
  - In PyTorch, setting `num_workers=4` allows for parallel data loading with four worker processes.
- NVIDIA's Data Loading Library (DALI) pipeline, where data is loaded on the CPU and then decoded to GPU, where it is augmented.
  - DALI supports data prefetching <u>transparently with a queue</u> to get batches ready ahead of time.



- FFCV 
  - **FFCV** is a fast and efficient computer vision data-loading framework designed to optimize the process of loading large datasets, especially for deep learning tasks. It aims to reduce bottlenecks caused by slow data loading, which often limits the performance of models on GPUs. FFCV <u>speeds up training and inference by using efficient data storage formats and parallel data loading strategies.</u>
  - In PyTorch, for example, FFCV can be used as an alternative to traditional data loaders like `torch.utils.data.DataLoader`. By converting datasets into FFCV's format and using FFCV's efficient data loader, training times can be significantly reduced, particularly when dealing with image datasets like **ImageNet**.<br><br>



#### Activation (Gradient) Checkpointing

> **Activation (Gradient) Checkpointing** is a memory optimization technique used in deep learning to reduce the memory footprint during model training, particularly for very deep neural networks or models with large batch sizes

- Normally, we store intermediate activation tensors in a forward pass. This requires a lot of VRAM. Activation checkpointing avoids storing intermediate activation tensors **by recomputing them during the backward pass by keeping track of the original input** .
- How Activation Checkpointing Works:
  - Forward Pass: Normally, during the forward pass of a neural network, all intermediate activations (outputs of each layer) are stored in memory for use during the backward pass to compute gradients. **In an activation checkpointing, only a subset of these activations (checkpoints) are stored in memory**. The remaining activations are recomputed on the fly during the backward pass when needed for gradient calculation.
  - Backward Pass: During backpropagation, **if an activation is not stored, it is recomputed by running a part of the forward pass again.** This recomputation consumes extra computational resources, but it saves significant memory. 
- Normal Backprop implementation: store all activations in a forward pass in memory, then re-use them during backprop. Uses lots of memory, but you don’t re-compute anything.
- Memory Poor Backprop: Compute only what you need, discard the rest, and recompute it if you need it again.

<br><br>



#### Distributed Model Training

> **Distributed Model Training** is a method in machine learning in which a model's training is split across multiple machines or devices. This enables faster and more scalable training, especially for large datasets and models.

- In the context of multi-GPU training, “**rank**” refers to the unique identifier assigned <u>to each process</u> or node in a distributed system
- Each process is assigned an integer 0 to N
  - N = World Size– 1
  -  World Size is the total number of distributed processes
- Rank 0 is commonly used for the “master” or “chief” process that
  coordinates the distributed work.



- **Distributed operations** are fundamental communication patterns used in parallel computing and distributed systems to efficiently manage data among multiple processes or nodes.
  - **AllReduce**: Aggregate and distribute the reduced result to all processes.
    - Perform an operation on data across all devices and writes the result in the receive buffers of every rank.
    - Example:
      - Process 1 has value 3.  
        Process 2 has value 5.  
        Process 3 has value 7.  
        Using sum as the reduction operation, all processes receive the result 15. 
  - **Broadcast**: Send data from one process to all others.
    - Send a tensor of data from one process (rank) to all other processes so that they all have the same data.
    - Example:
      - Process 0 (root) has a dataset or variable.  
        After broadcast, all processes have the same dataset or variable as Process 0.
  - **Reduce**: Aggregate data to a single process.
    - Add all tensors across all ranks and send it to rank 3.
    - Same as AllReduce, except the result is only sent to one rank.
    - Note: <u>Reduce followed by Broadcast is equivalent to AllReduce</u>.
    - Example: 
      - Process 1 has value 2.  
        Process 2 has value 4.  
        Process 3 has value 6.  
        Using max as the reduction operation, Process 0 (root) receives the result 6.    
  - **AllGather**: Collect and distribute all data chunks to all processes.
    - AllGather gathers a tensor from all ranks to create a stacked tensor that is then sent to all ranks.
    - Example:
      - Process 1 has data [A].  
        Process 2 has data [B].  
        Process 3 has data [C].  
        After AllGather, all processes have data [A, B, C].  
  - **ReduceScatter:** Reduce data and scatter the reduced portions to processes.
    - ReduceScatter is the same as Reduce except the result is scattered in equal blocks across ranks.
    - AllReduce is equilvaent to ReduceScatter followed by AllGather.
    - Example:
      - Process 1 has data [1, 2].   
        Process 2 has data [3, 4].   
        Process 3 has data [5, 6].   
        Using sum as the reduction operation:  
        Sum of first elements: 1 + 3 + 5 = 9.  
        Sum of second elements: 2 + 4 + 6 = 12.  
        The sums [9, 12] are scattered:  
        Process 1 receives 9.  
        Process 2 receives 12.  <br><BR>



#### Data Parallelism

- Multiple workers (GPUs), but only one model to train.
  - All GPUs have a version of the model and optimizer states. 
  - <u>Split data up such that each worker sees a different set of data.</u>
  - Once a forward pass is complete for all workers, each model
    computes a gradient.
  - Gradients are averaged and then model is updated.
- Data Parallel using **AllReduce**
  - Computation happens on all workers
  - Gradients are transferred directly to workers using AllReduce.
  - Model is transferred to all workers with AllReduce
- **Synchronous Data Parallelism**
  - All workers wait until all other GPUs finish calculating gradients and then average them to update the shared model.
  - <u>Synchronous is more often used</u> with strong devices with powerful communication links.
- **Asynchronous Data Parallelism**
  - Workers are not synchronized, instead updates happen asynchronously.
- **Data Parallel Pitfalls**
  - Keep the local batch size constant and scale the global batch size, but you must also change hyperparameters such as the learning rate because you are using a much larger effective batch size. <br><br>





#### Model Parallelism

- If our model won’t fit on one GPU, we can use model parallelism.
- Split the model across worker GPUs and it can be used to make
  <u>batches bigger.</u>



![image-20241010100459715](/images/2024-10-10-TIL24_Day88_DL/image-20241010100459715.png)

<br><Br>

