---
title: "Day88 Deep Learning Lecture Review - Lecture 10-12"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, LLM, NLP, TIL_24]
toc: true 
---

# Deep Learning & Numerical Precision(Floating Point), Hardware Considerations, 



<br><Br>

#### Bus Bottleneck

> A **bus bottleneck** refers to a situation in computer systems where the **data bus**, which connects different components (such as the CPU, memory, and peripherals), becomes a limiting factor for system performance. A bus is responsible for transmitting data between components, and when it cannot handle the volume of data being transferred, it becomes a **bottleneck**.

- Common **Causes** of Bus Bottlenecks
  - **Limited Bandwidth**: The data bus has a finite bandwidth, which defines how much data it can transfer per unit of time. Suppose the transmitted data exceeds the bus’s capacity, resulting in a bottleneck.
  - **High Traffic Load**: When multiple devices or components try to use the same bus simultaneously, the bus can become overwhelmed, leading to delays and slower data transmission.
- **Solutions** to Address Bus Bottlenecks:
  - <u>Increasing Bus Bandwidth</u>: Upgrading to buses with higher bandwidth can reduce bottlenecks. For example, **using PCIe (Peripheral Component Interconnect Express)** instead of older bus technologies like PCI or AGP can drastically improve component data transfer rates.
  - <u>Bus Architecture Optimizatio</u>n: Modern systems often use multiple buses (e.g., separate buses for CPU-memory communication and I/O devices) to reduce traffic on a single bus and avoid bottlenecks.
  - <u>Parallel Bus Systems</u>: Increasing the number of parallel data lines can improve throughput, as more data can be transmitted simultaneously.
- **SXM (Scalable Matrix Extension)** and **NVLink** can be used.<br><br>







#### Floating Point - FP32

> A floating point is a way to represent real numbers in a computer system using a finite number of bits.

- FP32 is the **32-bit floating-point** format commonly used in computer systems for numerical computations, following the IEEE 754 standard. FP32 (<u>also known as single-precision floating poin</u>t) uses 32 bits to represent real numbers, split into three components:

  - 1 bit for the **sign**: Indicates whether the number is positive (0) or negative (1).
  - 8 bits for the **exponent**: This encodes the scale of the number by representing the power of 2 multiplied by the number.
  - 23 bits for the **mantissa** (or significand): This represents the actual precision of the number.

- Half Precision: **FP16**

  - FP16 uses only <u>16 bit</u>s rather than 32 bits.
  - It has <u>less precision</u> and a smaller range of values.
  - FP16 is about **twice as fast as** FP32.

  ![image-20241009180016174](/images/2024-10-10-TIL24_Day88_DL/image-20241009180016174.png)

  - Weights in FP16 require half as much VRAM.
  - If we can train in lower precision (fewer bits), we would have more GPU memory available, and
    arithmetic operations would be faster as FP16 uses half the amount of RAM as FP32.
  - Additionally, we could potentially use a bigger model and a bigger batch size, yielding an even greater speedup. 

- Remember that we must also have a VRAM dedicated to 2x your model size.

  - – 1x is your model
  - – 1x is your optimizer: Momentum, Adam, AdamW, etc.



- **Mixed Precision Training**
  - We often encounter numerical problems when using FP16 everywhere. Although it is okay for the forward pass, it is not sufficient for computing weight updates.
  - Mixed precision training combines FP32 and FP16 (or BFLOAT16 or TFLOAT32).
  - It enables shorter training times and lowers the memory requirements of training with bigger models, batch sizes, and inputs. 



- Automatic Mixed Precision (AMP)
  - Using mixed precision arithmetic could enhance computational efficiency. Instead of relying solely on 32-bit floating point (FP32) operations, <u>AMP dynamically scales between 16-bit floating point (FP16) and FP32 arithmetic during training.</u> 
  - Using FP16 for less critical calculations, AMP can reduce memory and speed up computations, especially on modern hardware like **NVIDIA GPUs** with **Tensor Cores** optimized for FP16.



- BFLOAT16
  - bfloat16 (Brain Floating Point 16) is a <u>16-bit floating-point</u> representation used primarily to accelerate deep learning tasks. Google introduced it, and hardware manufacturers like Intel and **NVIDIA** adopted it.
  - Unlike the standard 16-bit floating-point format (FP16), **bfloat16 retains the same range as a 32-bit floating-point (FP32) format but with <u>reduced precision</u>.** This makes it particularly useful in machine learning tasks, where having a wide dynamic range is essential, but precision can be sacrificed for faster computations.



- Tensor Float 32
  - **TF32** offers a middle ground between FP32 and **FP16** formats by balancing performance with precision.
  - It only offers **18 bits (NOT 32 bits)** and the same dynamic range as FP32 and BFLOAT16, with the same precision as FP16, whereas BFLOAT16 has less precision than FP16.



![image-20241009181210116](/images/2024-10-10-TIL24_Day88_DL/image-20241009181210116.png)



- INT8

  - It refers to an 8-bit integer format used in machine learning and deep learning to represent numerical data.
  - **INT8 quantization** is the process of converting the weights and activations of a neural network from higher-precision formats (like **FP32**) into **8-bit integers**.
  - Two key methods are used:
    - **Post-training quantization:** Quantizing a model after it has been trained with floating-point values.
    - **Quantization-aware training**: <u>Training the model while simulating quantized arithmetic, leading to better accuracy after quantization</u>
  - **Accuracy Loss:** Since INT8 quantization reduces the precision of model weights and activations, it can lead to a slight degradation in model performance. This is particularly noticeable in models that rely on very fine-grained numerical precision.<br><br>

  

  

#### Hardware Considerations

- Different GPUs supports different precisions. Not all GPUs support bfloat16 or TF32. Some GPUs support special operations to enable mixed-precision training to be faster. 

- **Tensor Cores**
  - Tensor cores can <u>accelerate mixed precision training, but not all GPUs have them.</u>
  - Accelerated cores **for multiplying two FP16/BFLOAT16/TF32** matrices together and adding a FP32/FP16/BFLOAT16/TF32 matrix, but <u>returned value can be in FP32.</u>
  - When using Tensor cores for a speed up, this can greatly reduce the GPU utilization by having BatchNorm in the code.<br><br>
