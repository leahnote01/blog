---
title: "Day48 ML Review - Data Preprocessing (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, dataPreprocessing, TIL_24]
toc: true 
---

# Handling Categorical Data - Converting, Ordinal Encoding, and One-Hot Encoding

<img src="/blog/images/2024-08-07-TIL24_Day48/F6EB4DB5-A21F-4352-AF12-57B2710381F1_1_102_o.jpeg"><br><br>



## Dealing with Categorical Data

When discussing categorical data, it's important to differentiate between ordinal and nominal features. Ordinal features are categorical values that can be sorted or ordered, such as t-shirt sizes. On the other hand, nominal features do not imply any order, like t-shirt colors. Let's make a data frame example as below. 

```python
import pandas as pd
df = pd.DataFrame([
 ['green', 'M', 10.1, 'class2'],
  ['red', 'L', 13.5, 'class1'],
  ['blue', 'XL', 15.3, 'class2']])
df.columns = ['color', 'size', 'price', 'class-label']
```

<br><br>

### 1. Mapping Ordinal Features

To make sure that the learning algorithm interprets the ordinal features correctly, we need to convert the categorical string values into integers. Unfortunately, there is no convenient function that can automatically derive the correct order of the labels on features, so we have to define the mapping manually.

```python
size_mapping = {'XL' : 3, 
               'L': 2,
               'M', 1}
df['size'] = df['size'].map(size_mapping)
```

<br>

In order to convert the integer values back to their original string representation later on, we can create a reverse-mapping dictionary. This dictionary, inv_size_mapping, can be defined as: 
```python
inv_size_mapping = {v: k for k, v in size_mapping.items()}
```
We can then use this dictionary with the pandas `map` method on the transformed feature column. This process is similar to the `size_mapping` dictionary that we previously used.

```python
df['size'].map(inv_size_mapping)
```

<br>

### 2. Encoding Class Labels

It's recommended to provide class labels as integer arrays to avoid technical issues. To encode the class labels, we can use a method similar to the one used for mapping ordinal features. 

It's important to note that class labels are not ordinal, so <u>it doesn't matter which integer we assign to a specific label</u>. We can simply enumerate the class labels, starting at `0`.

```python
import numpy as np
class_mapping = {label: idx for idx, label in enumerate(np.unique(df['classlabel']))}
```

<br>

Then, we can use a mapping dictionary to convert the class labels into integers.

```python
df['classlabel'] = df['classlabel'].map(class_mapping)
```

<br>

In order to map the converted class labels back to their original string representation, we can reverse the key-value pairs in the mapping dictionary in the following way. 

```python
inv_class_mapping = {v: k for k, v in class_mapping.items()}
df['classlabel'] = df['classlabel'].map(inv_class_mapping)
```

<br>

Alternatively, we can use the convenient `LabelEncoder` class directly implemented in scikit-learn to achieve this. It's important to note that the `fit_transform` method is just a shortcut for calling fit and transform separately.

```python
 from sklearn.preprocessing import LabelEncoder
 class_le = LabelEncoder()
 y = class_le.fit_transform(df['classlabel'].values)
```

<br>

We can use the `inverse_transform` method to transform the integer class labels back into their original string representation.

```python
 class_le.inverse_transform(y)
```

<br>

### 3. Performing One-Hot Encoding on Nominal Features

In the earlier section on Mapping ordinal features, we converted the ordinal size feature into integers using a simple dictionary-mapping approach. As scikit-learn's classification estimators treat class labels as categorical data without any implied order (nominal), we conveniently used the `LabelEncode`r to encode the string labels into integers. It might seem that we could employ a similar approach to transform the nominal color column of our dataset, as follows:

```python
 X = df[['color', 'size', 'price']].values
 color_le = LabelEncoder()
 X[:, 0] = color_le.fit_transform(X[:, 0])
```

<br>

After running the code, the first column of the NumPy array X now contains the new color values: `blue (0)`, `green (1`), and `red (2)`. Although the color values are not in any specific order, a learning algorithm will interpret green as larger than blue and red <u>as larger than green.</u> This assumption is incorrect, but the algorithm could still generate useful results, although they may not be optimal.

To handle this issue, we can use a method known as **one-hot encoding**. This involves creating a new binary feature for each unique value in the nominal feature column. For instance, if we have a color feature with values like blue, green, and red, we would create new binary features for each color. Then, for each example, we would use <u>binary values to indicate the presence of a particular color</u> (e.g., blue=1, green=0, red=0). OneHotEncoder in scikit-learn's preprocessing module can be used to carry out this transformation.

```python
 from sklearn.preprocessing import OneHotEncoder
 X = df[['color', 'size', 'price']].values
 color_ohe = OneHotEncoder()
 color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()
```



<br><br>

