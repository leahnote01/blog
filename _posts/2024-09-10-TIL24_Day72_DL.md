---
title: "Day72 Deep Learning Lecture Review - Lecture 3-4"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, linear algebra, basic machine learning, TIL_24]
toc: true 
---

# Brief Explanation of Basic Algebra and Machine Learning

![747C11EE-F4C9-4AF9-89DD-1A23C1B977ED_1_105_c](/images/2024-09-10-TIL24_Day72_DL/747C11EE-F4C9-4AF9-89DD-1A23C1B977ED_1_105_c.jpeg)<br><br>

#### Linear Algebra

- **Vector Norm**: a function that takes a vector as input and outputs a <u>single non-negative real number</u> to show how significant that number is.

- **Error** for Regression: Error measures to use when p<u>redicting real values instead of discrete ones used in classification</u>. **MSE** (Mean Squared Error) is the most common one.

- Error for Classification: Error and Accuracy (Predicting discrete variables) <br><br>



#### Basic Machine Learning

- **Underfitting**: the classifier isn’t robust enough to model the input-output relationship. 

- Overfitting: if the training performance is much more excellent than the testing performance
  - **Regularization** is reducing **overfitting**<br><br>

- Data Augmentation: to get more data

<br><br>

- **Multi-Layer Perceptron (MLP) Networks**: Weights, bias, and activation function<br><br>



**Common Activation Function:**

- **Linear** activation function: just an identity function / used for **regression**

- **Logistic** sigmoid activation function: forces outputs to be between 0 and 1/ used for **classification**

- **Hyperbolic** Tangent Activation: Forces output to be between -1 and 1

- Rectified Linear Activation function: output between 0 and <u>positive infinity.</u>

- Without activation functions, you do not get additional non-linearity in an MLP.<br><br>



**Loss Function**

- Need to measure how good a given w and b are
- We do this with a loss function.
- Measures how good a system is given its current parameter setting
- We want to minimize the loss function on the training data (training loss)
- <u>As activation functions define the unit's outputs</u>, the output layer activation function and loss functions are closely related.
- **Linear activation function** for the output is typically paired most often
  - **L2 loss, Huber loss**
- **Logistic Sigmoid function** for output is typically paired with 
  - **Binary Cross-Entropy loss (BCE loss)**
- **Softmax for mutually exclusive classification**
  - **Cross-Entropy loss**<Br><br>







- Gradient descent is a **procedure for minimizing a loss function to find the network’s parameters. n is the learning rate.** 

![image-20241010105254191](/images/2024-09-10-TIL24_Day72_DL/image-20241010105254191.png)

- Compute partial derivatives of the loss function with respect to network parameters to give us the slope, which tells us in <u>which direction to change each parameter to get closer to the minima.</u>

- Backward Propagation: use gradient descent to change the parameters w to reduce the loss.

- SGD: taking small steps down a hill to find the lowest point.

  <br><Br>

![image-20241010105336523](/images/2024-09-10-TIL24_Day72_DL/image-20241010105336523.png)

- Gradient descent makes small changes to parameters to improve loss.
- An entire gradient descent passing through the training dataset is called an epoch.<br><Br><br>

When should we update the Parameters?

- **SGD**: Compute update **individually** for each of the N training instances. 
  - **N updates in an epoch.**
- Full Gradient Descent: Update after seeing all the instances in the training set. 
  - **1 update of in an epoch**
- We use (mini) batched gradient descent for deep learning.
  - Update **N / M** times, where M is the batch size and M is the batch size.<Br><br>

**Adam**

- a more sophisticated version of SGD that adapts the step sizes for different parameters
- Adam automatically adjusts the learning rate for each parameter based on the history of gradients. This speeds up training. 
- Adam keeps two things:
  - The average gradient
  - The average of the squared gradients<br><br>



**Minibatch size and learning rate**

-When you **increase the batch size**, your optimal learning rate will change, and this depends on the optimization algorithm.

- For SGD: When batch size increases by k, multiply the learning rate by k. <br><Br>



Checkpoint Selection

- Often, we choose the model with the best validation performance (e.g., lowest validation loss)



#### **Need to know**

- Evaluation: Accuracy, Error, AUC, Confusion Matrix, Decision Boundaries, etc.
- MLPs: Loss functions, activation functions
- Activation functions: Logistic Sigmoid, ReLU, Linear
- Loss functions: L2, Cross-entropy
- Regularization: Weight decay, early stopping/checkpoint selection, dropout
- Backpropagation: Gradients, Epochs, Batches<br><Br><br>





#### MLops Using tests

- CI/CD Pipelines are used. 
  - Code is built 
  - Tests execute after building (CI)
  - Tests fail there is a problem. 
  - Deploy (CD)

![image-20241010105726595](/images/2024-09-10-TIL24_Day72_DL/image-20241010105726595.png)

- Containers
  - Docker / Kubernetes
    - Docker containers are built from the images, which is where your code lives
    - Docker Images are the “source code” for creating a container

<br><Br>

**Tests for ML Systems**

- Pre-train Tests: Tests that do not need the system to be trained.
  - Check the shape of your model output and ensure it aligns with the labels in your dataset
  - test with a very small dataset and make sure you can overfit it
  - Steps

​		- Make sure your optimizer can reduce the loss to near zero. 

​		- If it cannot something is wrong, probably with your optimizer

- Post-train Tests: Tests that need the system to be trained.
  - Invariance test: check for consistency in the outputs despite reasonable perturbations
  - Robustness test: measure the robustness of outputs
  - Directional Expectation tests
  - Minimum functionality tests

- Version everything: Datasets, Experiments, Models<br><br>



Workflow Orchestration

-  Automating machine learning workflows.
- **What is workflow orchestration**
- **-> Changing pre-processing, training, and evaluation systems together**



![image-20241010105801556](/images/2024-09-10-TIL24_Day72_DL/image-20241010105801556.png)

