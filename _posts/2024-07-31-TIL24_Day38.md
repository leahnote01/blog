---
title: "Day38 ML Review - Random Forest (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,logisticRegression,TIL_24]
toc: true 
---

# Implementation Step by Step

<img src="/blog/images/2024-07-31-TIL24_Day38/3D0538C8-22BE-4B10-8445-6A6100537D6E.jpeg">

<br><br>

Let's talk about the **random forest** algorithm, which is based on decision trees and is known for its scalability and ease of use. A random forest is a collection of decision trees (other methods will be discussed later). The concept behind a random forest is <u>to combine multiple (deep) decision trees</u>, each of which may have <u>high variance on its own, to create a more reliable model with better generalization performance and reduced risk of overfitting.</u>



### Following Steps

1. Draw a random **bootstrap** sample of size *n* by randomly selecting *n* examples from the training dataset with replacement.
2. Grow a **decision tree from the bootstrap sample**. At each node:
   - Randomly select *d* features without replacement.
   - Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.
3. **Repeat** the steps 1-2 *k* times.
4. Aggregate the prediction by each tree to assign the class label by **majority vote**.  We will discuss majority voting in more detail later.

<br>

We need to slightly modify step 2 when training the individual decision trees. Instead of evaluating all features to determine the best split at each node, **we will only consider a random subset of those.**

<Br>

Random forests offer less interpretability than decision trees, but we don't have to worry about hyperparameter values much. We usually don't need to prune the random forest since the ensemble model is robust to noise. The only parameter that matters in practice is the number of trees, denoted as $k$, that we choose for the random forest.

In most implementation

