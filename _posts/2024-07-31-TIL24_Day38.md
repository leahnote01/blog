---
title: "Day38 ML Review - Random Forest (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,decisionTree,randomForest,TIL_24]
toc: true 
---

# Implementation Step by Step

<img src="/blog/images/2024-07-31-TIL24_Day38/3D0538C8-22BE-4B10-8445-6A6100537D6E.jpeg">

<br><br>

Let's talk about the **random forest** algorithm, which is based on decision trees and is known for its scalability and ease of use. A random forest is a collection of decision trees (other methods will be discussed later). The concept behind a random forest is <u>to combine multiple (deep) decision trees</u>, each of which may have <u>high variance on its own, to create a more reliable model with better generalization performance and reduced risk of overfitting.</u>



### Following Steps

1. Draw a random **bootstrap** sample of size *n* by randomly selecting *n* examples from the training dataset with replacement.
2. Grow a **decision tree from the bootstrap sample**. At each node:
   - Randomly select *d* features without replacement.
   - Split the node using the feature that provides the best split according to the objective function, for instance, maximizing the information gain.
3. **Repeat** the steps 1-2 *k* times.
4. Aggregate the prediction by each tree to assign the class label by **majority vote**.  We will discuss majority voting in more detail later.

<br>

We need to modify step 2 when training the individual decision trees slightly. Instead of evaluating all features to determine the best split at each node, **we will only consider a random subset of those.**



<Br>

### Hyperparameters

Random forests offer less interpretability than decision trees, but we don't have to worry about hyperparameter values much. We usually don't need to prune the random forest since the ensemble model is robust to noise. The only parameter that matters in practice is the number of trees, denoted as $k$, that we choose for the random forest.

In most implementations, such as the `RandomForestClassifier` in scikit-learn, the size of the **bootstrap** sample is usually set to be **equal to the number of training examples** in the original dataset. This choice generally strikes <u>a good balance between bias and variance</u>. As for the number of **<u>features</u>**, denoted as $d$, at each split, we should pick <u>a value that is <b>smaller</b> than the total number of features in the training dataset.</u> A common default used in scikit-learn and other implementations is to set $d$ to be equal to the number of features in the training dataset. A reasonable default that is used in scikit-learn and other implementations is **$d=\sqrt{m}$, where $m$ is the number of features in the training dataset.** 

<br>

```python
from sklearn.ensemble import RandomForestClssifier
forest = RandomForestClassifier(criterion='gini', n_estimator=25, random_state=1, n_jobs=2)
forest.fit(X_train, y-train)
plot_decision_regions(X_combined, y_combined, classifier=forest, test_idx=range(105,150))
```

<br>In the code above, we trained a random forest with 25 decision trees using the `n_estimators` parameter. We used the Gini impurity measure to split the nodes. Even though we are building a small random forest from a small training dataset, we used the `n_jobs` parameter for demonstration purposes. This parameter allows us to parallelize the model training using multiple cores of our computer (in this case, 2 cores).

<br><Br>
