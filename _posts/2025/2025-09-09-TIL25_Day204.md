---
title: "Day204 - DL Review: Revisiting Optimizers & CNNs"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, DLReview, TIL_25]
toc: true 
---

# Optimizers in Neural Networks, Parameter Sharing in CNNs, and Data & Concept Drifts

![27C0AD67-6E5C-42DB-9390-EC80550D4A31_1_102_o](../../images/2025-09-09-TIL25_Day204/27C0AD67-6E5C-42DB-9390-EC80550D4A31_1_102_o.jpeg)

<br><br>

## Optimizers

### 1. What Optimizers Do

Optimizers determine how weights are updated during training, based on gradients computed from back-propagation. The choice of optimizer affects:

- **Convergence Speed** (how fast the model learns)
- **Stability** (whether training oscillates, diverges, or converges smoothly)
- **Generalization** (how well the model performs on unseen data)

### 2. Core Optimizer Families

- **Stochastic Gradient Descent (SGD)**
  - Mechanism: Updates weights with a fraction of data (mini-batch).
  - Pros: Simple, good generalization, widely used in practice (e.g., ResNet)
  - Cons: Can be slow to converge, sensitive to learning rate.
  - Enhancements: Momentum
- **Adaptive Methods**
  - **AdaGrad:** Increases learning rate efficiency for sparse features, but aggressively decays learning rates.
  - **RMSProp:** Uses an exponential moving average of squared gradients to prevent learning rate decay.
  - **Adam**: Combines momentum and adaptive learning rates, often the default choice in practice.
  - **AdamW:** Variant of Adam with proper weight decay regularization (preferred in modern NLP/CV models).

### 3. Trade-offs in Optimizer Choice

- **SGD (with momentum):** Often better for generalization, used in vision tasks.
- **Adam / AdamW:** Converges faster, stable on noisy or sparse gradients, dominant in NLP and transformers.
- **RMSProp:** Historically significant in RNNs.

### 4. From an MLOps Perspective

When deploying models, optimizers matter beyond just "training well":

1. **Reproductibility**: Adam is more stable across random seeds; SGD might vary more.
2. **Resource Efficiency**: <u>Adam converges faster (with fewer epochs)</u>, thereby reducing compute costs. 
3. **Scalability**: For distributed training, optimizers like AdamW handle <u>larger sizes more effectively.</u>
4. **Hyperparameter Sensitivity**:
   - SGD: Learning rate, momentum
   - Adam: Learning rate, beta values (for moments), weight decay.

<br><Br>

## Parameter Sharing

> Parameter sharing is the idea that instead of learning a **unique weight for every pixel connection** (as in a fully connected layer), CNNs learn **a set of weights (a filter/kernel)** that is applied **across the entire input image**.

- In a **fully connected network** for an image of size $256 \times 256$, <u>a single hidden neuron connected to all pixels would already need $65,536$ weights.</u> 
- In contrast, CNNs use a **small filter** (e.g., $3 \times 3$) with only **$9$ weights** shared across all spatial positions.

So, instead of having millions of unique parameters, CNNs reuse the same parameters to detect the same feature <u>(like an edge, corner, or texture) at different parts of the image.</u> 

<Br>

### Why Do We Need It?

1. **Parameter Efficiency**
   - Dramatically reduces the number of parameters → faster training and less risk of overfitting.
   - Example: A fully connected layer for a $256 \times 256$ images has $\sim 16M$ weights; ***a $3 \times 3$ convolutional layer with $64$ filters have only $64 \times 9 = 576$ weights.***
2. **Translation Invariance** 
   - A filter detecting an "<u>edge</u>" in the top-left corner will also detect the same edge <u>in the bottom-right.</u>
   - This allows CNNs to generalize across spatial locations.
3. **Location & Hierarchy**
   - CNNs focus on **local features** first (edges, textures), and <u>stacking layers allows them to build <b>hierarchical features</b> (object parts → complete objects)</u>

<br>

### How It Works Mathematically

Let's say we have a filter of $K$ of size $3 \times 3$. 

For each location $(i, j)$ in the image $X$, the convolution computes:<br>

<center>
  $Y[i,j]= \sum^2_{m=0} \sum^2_{n=0} K[m,n] \cdot X[i+m,j+n]$<Br><Br>
</center>



<u>Here, <b>the same filter $K$ is reused (shared) across all $(i, j)$</b>. This is parameter sharing.</u>

<Br>

### Example: Without vs. With Sharing

- **Without sharing (Fully Connected)**: Each neuron learns its own weights → millions of parameters.
- **With sharing (CNN)**: The same filter slides (convolves) over the input → <u>a thousand parameters at most</u>.

This difference is why CNNs are practical for computer vision tasks.

<br>

### MLOps & Deployment Relevance

Parameter sharing isn't just a theory - <u>it impacts <b>real-world deployment</b></u>:

1. **Small model size**: easier to deploy on edge devices (phones, IoT).
2. **Less Computation**: faster inference, lower cost in production.
3. **Hardware accelaeration**: GPUs/TPUs are optimized for convolutions, making CNNs very efficient at scale.

<Br><Br>











