---
title: "Day169 - MLOps Review: Model Development and Offline Evaluation (3)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, TIL_25]
toc: true 
---

# Designing Machine Learning Systems: Auto ML (Hyperparameter Tuning & NAS), Model Offline Evaluation (1) (Establishing Baselines & Overall Accuracy)	

![579BA773-5294-4312-9F38-42FA3FEF3CDA_1_105_c](../../images/2025-06-29-TIL25_Day169/579BA773-5294-4312-9F38-42FA3FEF3CDA_1_105_c.jpeg)

<br>

### Auto ML: Automating the ML Lifecycle

> AutoML (Automated Machine Learning) aims **to reduce the human effort needed to design and train machine learning models.** It attempts to <u>automate tasks like model selection, feature engineering, hyperparameter tuning, and even neural architecture design.</u>



#### Soft AutoML: Hyperparameter Tuning

This is the **most common & popular** form of AutoML and involves automatically searching for the best values of hyperparameters, such as:

- Learning rate
- Batch size
- Dropout rate
- Number of layers
- Number of hidden units
- Optimizer types and their internal parameters (e.g., $\beta_1$, $\beta_2$ in Adam)



With different hyperparameters, <u>the same model can exhibit significantly different performance on the same dataset.</u> Melis et al. (2018) demonstrated that *weaker, well-tuned models can outperform stronger, more sophisticated ones.* 

Their popular tuning methods are as follows.s

- **Grid Search**: Tries all combinations in a predefined grid. Exhaustive but expensive.
- **Random Search**: Randomly samples from the space. Often more efficient than grid search.
- **Bayesian Optimization**: Models the performance function and iteratively chooses promising hyperparameters using a probabilistic model.

Always remember that

- Always use a **validation set** for tuning‚Äî<u>never the test set, to avoid overfitting.</u>
- <u>Some hyperparameters have greater <b>sensitivity</b> (e.g., learning rate) and should be more finely searched.</u>

<br>

#### Hard AutoML: Neural Architecture Search (NAS) and Learned Optimizers

Some teams take hyperparameter tuning to the next level: *What if we treat other components of a model, or even the entire model, as hyperparameters?*  

<u>The size of a convolution layer or whether to have a skip layer can be considered a hyperparameter.</u> Instead of manually adding pooling after convolution or ReLU after linear, you provide your algorithm with these building blocks and let it determine **how to combine them**. This research area, known as **neural architecture search (NAS)**, seeks to identify the optimal model architecture.



##### Neural Architecture Search (NAS)

Instead of manually choosing model structures (such as the number of CNN layers or where to place pooling layers), <u>NAS automates the design of the architecture.</u>

- **Search Space**: Defines possible components (e.g., conv, pooling, activation) and how they can be connected.
- **Search Strategy**: <u>Reinforcement learning, evolutionary algorithms, or random search.</u>
- **Performance Estimation**: Utilizes <u>proxies</u> (e.g., training a smaller model or a few epochs) to efficiently estimate performance.

Google‚Äôs **EfficientNet** is a successful NAS result‚Äî10√ó better compute efficiency with top-tier performance.

In typical ML training, you have a model and a learning procedure, an algorithm that finds parameters to minimize an objective function for data. **Gradient descent** is the most common method for neural networks, utilizing an optimizer to update weights based on the gradients. Popular optimizers include Adam, Momentum, and SGD. <u>Although optimizers can be part of NAS to find the best one,</u> **it isn't easy because <u>they are sensitive to hyperparameter settings, which often don‚Äôt work well across architectures.</u>**

This prompts a research question: ***What if neural networks replace functions that define update rules***? This network would determine <u>how much to update the model's weights, creating learned optimizers instead of hand-designed ones.</u> 

<br>

##### Learned Optimizers

Rather than using hand-designed optimizers like SGD or Adam, l**earned optimizers are themselves neural networks that know how to update weights.**

- These optimizers are **meta-learned** by being trained across many tasks.
- <u>Once trained, they generalize to unseen datasets and architectures.</u>
- They can also <u>train new, better learned optimizers,</u> resulting in recursive self-improvement.

<br>

#### Four Phases of ML Model Development

This framework helps guide the adoption of ML depending on maturity, available resources, and problem complexity:

##### Phase 1: Before ML (Baseline Heuristics)

Start with simple rules, especially if this is your first time solving a problem with ML.

- Examples: Recommend the most popular items, use median values, or sort posts chronologically.
- Heuristics can surprisingly achieve 50% or more of your desired performance.

**Tip**: Avoid overcomplicating things too early. A sound, rule-based system may suffice for many practical tasks.



##### Phase 2: Simplest ML Models

Once a baseline is insufficient:

- Start with interpretable models, such as **logistic regression**, **decision trees**, or **gradient boosting**.
- These give insight into data and feature importance.
- They‚Äôre easy to debug and quick to deploy.

**Goal**: Build an end-to-end system with this simple model (including data ingestion, training, evaluation, and deployment).



##### Phase 3: Optimize Simple Models

If you‚Äôve validated that ML works and built a pipeline:

- Apply feature engineering
- Add more data
- Use regularization
- Run a hyperparameter search.
- Try ensembling (e.g., averaging predictions of multiple models)

This phase can deliver significant performance improvements with minimal extra complexity.



##### Phase 4: Complex Models

When simple models plateau, move to:

- **Deep learning architectures** (CNNs, RNNs, Transformers)
- **Pretrained models** (e.g., BERT, ResNet)
- **Multi-task learning or multi-modal models**

At this stage:

- Consider cost, latency, and inference environment.
- Analyze model drift (decay in performance over time)
- Plan infrastructure for **frequent retraining and monitoring**



<br>

## Model Offline Evaluation

> **Offline evaluation** is the process of assessing an ML model‚Äôs performance before deploying it to a live production environment. It relies on **labeled datasets** and simulates real-world conditions as closely as possible using **held-out validation or test sets**.

The core challenge: *How can we trust a model‚Äôs performance when it‚Äôs never seen real-world (live) data yet?*



### 1. Establishing Baselines

Before evaluating a model, you must define a **baseline**‚Äîa reference point against which to compare. Metrics alone (e.g., ‚ÄúOur model gets 90% accuracy‚Äù) are meaningless unless you understand what you‚Äôre comparing them to.

#### Common Baseline Types:

| **Baseline Type**            | **Description**                                              | **Use Case**                                       |
| ---------------------------- | ------------------------------------------------------------ | -------------------------------------------------- |
| **Random baseline**          | Predict at random based on label distribution (e.g., 90% negative, 10% positive) | Detects if model is just memorizing majority class |
| **Zero Rule**                | Always predicts most frequent class                          | Fast sanity check                                  |
| **Heuristic-based**          | Uses simple rules (e.g., always recommend the latest item)   | Useful in recommendation, ranking, or forecasting  |
| **Human-level baseline**     | Performance of a human expert                                | Useful in fields like medicine or hiring           |
| **Existing system baseline** | Rule-based or older ML system                                | Compare cost, accuracy, and maintenance benefits   |

üí° *Example: A model predicting user churn should beat a ‚Äúrecommend last used app‚Äù baseline to justify complexity.*

<br>

#### 2. Beyond Overall Accuracy

High accuracy might hide deep model flaws, especially when:

- Data is **imbalanced** (e.g., 90% of cases are ‚ÄúNegative‚Äù)
- Specific **user groups** are underperforming
- The system needs to be **fair, robust, and trustworthy**

Hence, we use **advanced evaluation techniques**:

<Br><br>
