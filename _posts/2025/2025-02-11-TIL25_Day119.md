---
title: "Day119 - STAT Review: Regression and Prediction (1)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../../
tag: [StatReview, TIL_25]
toc: true 
---

# (Editing) Practical Statistics for Data Scientists: Simple Linear Regression, Least Squares, and Multiple Linear Regression

![296AC83E-3054-4848-97EF-47792996601B_1_105_c](../../images/2025-02-11-TIL25_Day119/296AC83E-3054-4848-97EF-47792996601B_1_105_c.jpeg)

> The primary objective in statistics is to determine whether variable $X$ is related to variable $Y$, and if it is, to understand the nature of this relationship and its potential to predict $Y$.  Statistics refers to data science grounded in the objective of **forecasting an outcome (the target variable) using predictor variables.** 

This type of model training on known outcomes for future unknown data is called supervised learning. Additionally, statistics are firmly utilized in *anomaly detection* since regression diagnostics can identify unusual records when used for data analysis.<br><br>



### Simple Linear Regression

> Simple linear regression offers a model that describes <u>the relationship</u> <u>between the magnitude of one variable and another</u>. For example, as X increases, Y also increases (or as X goes up, so does Y). 

**Correlation** serves as another way to measure the relationship between two variables. <br><br>

#### Key Terms For Simple Linear Regression

- Response
  - The variable we aim to predict
  - = **dependent variable, $Y$ variable, target, outcome**
- Independent Variable
  - The variable used to predict the outcome. 
  - = **$X$ variable, feature, attribute, predictor**
- Record
  - The vector of predictor and outcome values for a specific individual or case.
  - = **row**, case, instance, example
- Intercept
  - The intercept of the regression line represents the predicted value when $X=0$.
  - = $b_0, \beta_{0}$
- **Regression Coefficient**
  - The **slope** of the regression line.
  - = slope, $b_1$, $\beta_{1}$, parameter estimates, weights
- **Fitted Values**
  - The **difference** <u>between the observed values and the fitted values.</u>
  - = errors
- Least Squares
  - The method of fitting a regression <u>by minimizing the sum of squared residuals.</u>
  - = ordinary least squares, OLS<br><br>



#### The Regression Equation

Simple linear regression estimates how much Y will change when X changes by a certain amount. With the correlation coefficient, the variable X and Y are interchangeable. With regression, we aim to predict the Y variable from X using a linear relationship as follows. 

<center>
  $Y = b_0 + b_1 X$<br><br>
</center>

where,

- $b_0$ : intercept (constant) and $b_1$: slope
- In R, both are appeared as *coefficients*



Let's say we have a dataset displaying the number of years a worker was exposed to cotton dust (`Exposure`), a measure of lung capacity(`PEFR` or `peak expiratory flow rate`). Then let's draw a "best" line to predict the response `PEFR` as a function of the predictor variable `Explosure`,

- In R, we utilize `lm` function to show a linear regression.

  ```R
  model <- lm(PEFR ~ Exposure, data=lung)
  ---
  Call:
  lm(formula = PEFR ~ Exposure, data = lung)
  
  Coefficients:
  (Intercept)     Exposure
      424.583       -4.185
  ```
  
  The intercept $b_0$ is $424.583$, and can be interpreted as the predicted `PEFR` for a worker with zero years exposure. The regression coefficient $b1$ can be interpreted as follows: for each additional year that a worker is exposed to cotton dust, the worker's `PEFR` measurement is reduced by $-4.185$. 



- In Python, `LinearRegression` from `Scikit-learn` package is used.

  ```python
  predictors = ['Exposure']
  outcome = 'PEFR'
  
  model = LinearRegression()
  model.fit(lung[predictors], lung[outcome])
  
  print(f'Intercept: {model.intercept_:.3f}')
  print(f'Coefficient Exposure: {model.coef_[0]:.3f}')
  ```

  <Br>

  The result will be shown as follows. 

<center>
  <img src="../../images/2025-02-11-TIL25_Day119/image-20250221223809200.png" width="70%"><br><br>
</center>





#### Fitted Values and Residualss

In real-world, the data doesn't fall exactly on the line, so the regression equation should include term $e_1$.

<center>
  $Y_i = b_0 + b_1 X_i + e_i$ <br><Br>
</center>



The fitted values (*predicted values*) are typically denoted by $\hat{Y}$. So, it will be updated as

<center>
  $\hat{Y_i} = \hat{b_0} + \hat{b_1} X_i$ <Br><bR>
</center>

The notation $\hat{b_0}$ and $\hat{b_1}$ indicates the coefficients are estimated values. 

We compute the residual with the equation below by subtracting the predicted values from the original data.

<center>
  $\hat{e_i} = Y_i - \hat{Y_i}$ <br><br>
</center>

- In R

  ```R
  fitted <- predict(model)
  resid <- residuals(model)
  ```

- In Python, we use `LinearRegression` model, `predict` function as follows.

  ```python
  fitted = model.predict(lung[predictors])
  residuals = lung[outcome] - fitted
  ```

  <br>



#### Least Squares

How is the model fit the data? To evaluate the model's performance, we use "$RSS$" whici is the sum of squared residual values. Our goal of this test is to minimize it.

<center>
  $\text{RSS} = \sum_{i=1} ^n \big( Y_i - \hat{Y_i}^2 \big) \\ =\sum_{i=1} ^n \big( Y_i - \hat{b_0} -\hat{b_1}X_i \big)^2$ <br><Br>
</center>



The estimates $\hat{b_0}$ and $\hat{b_1}$  are the values that minimize RSS.



The method of minimizing the sum of squard residuals is termed ***"Least Sqaures Regression"*** or ***"Ordinary Least Squares" (OLS)*** regression. 

With the advent of big data, computational speed is still an important factor. Least squares, like the mean, are sensitive to outliers, although this tends to be a significant problem only in small or moderate-sized data sets. <br><br>



#### Prediction Versus Explanation (Profiling)

Regression historically focuses on **identifying linear relationships** between predictor variables and outcomes, aiming to understand and express these relationships through the data. The estimated **slope** of the regression equation is essential. Additionally, economists want to understand broader variable relationships by analyzing the connection between two variables â€” consumer spending and GDP growth- to uncover. 

With big data's advance, regression is often used for predictive modeling of new data rather than explaining existing data. The main focus is on fitted values Y. For instances, in marketing, regression forecasts revenue changes based on ad compaign size and universities use regression to predict students' GPA from SAT scores. 

