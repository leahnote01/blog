---
title: "Day117 - STAT Review: Statistical Experiments and Significance Testing (4)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../../
tag: [StatReview, TIL_25]
toc: true 
---

# (Editing) Practical Statistics for Data Scientists: ANOVA(One & Two-Way), F-statistic, and Chi-Square Test

![5BDF34C9-064E-4C43-B6F9-265EB87335FE_1_105_c](../../images/2025-02-07-TIL25_Day117/5BDF34C9-064E-4C43-B6F9-265EB87335FE_1_105_c.jpeg)<br><br>

### ANOVA

> Imagine that, rather than conducting an A/B test, we assessed **several groups**—A, B, C, and D—each containing numeric data. The statistical method used to determine whether there is a significant difference between these groups is known as **analysis of variance**, or ANOVA.

#### Key Terms for ANOVA

- **Pairwise Comparison**
  - A hypothesis test (e.g., of means) between two groups among multiple groups. 
- Omnibus Test
  - A single hypothesis test of the overall variance among multiple group means. 
- Decomposition of Variance
  - Separating components of an individual 
- **F-Statistic**
  - **A standardized statistic** that measures <u>how much the differences between group means exceed expectations</u> based on a chance model. 
- **SS**
  - "Sum of **Squares"**, which" refers to d<u>eviations from an average value</u>.  



Let's assume we have a table displayin<u>g the number of seconds each visitor spent on four pages.</u> The four pages are rotated so each web visitor receives one randomly. Each page has five visitors in the table, and each column represents an independent data set. 

| --            | Page 1 | Page 2 | Page 3 | Page 4 |
| ------------- | ------ | ------ | ------ | ------ |
|               | 164    | 178    | 175    | 155    |
|               | 172    | 191    | 193    | 166    |
|               | 177    | 182    | 171    | 164    |
|               | 156    | 185    | 163    | 170    |
|               | 195    | 177    | 176    | 162    |
| Average       | 172    | 185    | 176    | 162    |
| Grand Average |        |        |        | 173.75 |

Comparing two groups was simple; with four means, there are six comparisons between them.

- Page 1 compared to page 2
- Page 1 compared to page 3
- Page 1 compared to page 4
- Page 2 compared to page 3
- Page 2 compared to page 4
- Page 3 compared to page 4

Instead of pairwise comparisons, we <u>can conduct an overall test</u> to determine whether all pages share the same underlying stickiness. Differences in stickiness arise from **a random allocation of page times among them**.

The procedure used to test this is **ANOVA**. It can be demonstrated using the following resampling method for the A/B/C/D web page stickiness test: 

1. <u>Combine</u> all data into a box.  

2. Shuffle and draw four <u>resamples of five values each.</u>  

3. <u>Record the mean</u> of each group.  

4. <u>Calculate the variance</u> among the means.  

5. Repeat steps 2–4 many times (e.g., 1,000).

The ***p-value*** indicates the proportion of the time the resampled variance exceeded the observed variance. This permutation test is more complicated than the one we discussed in earlier posts. (Check out here: [Day114 - STAT Review: ...Permutation Test](https://leahnote01.github.io/blog/til_25/TIL25_Day114/))

- In R, we can use the `aovp` function in the `lmPerm` package.

  ```R
  > library(lmPerm)
  > summary(aovp(Time ~ Page, data=four_sessions))
  ---
  [1] "Settings:  unique SS "
  Component 1 :
              Df R Sum Sq R Mean Sq Iter Pr(Prob)
  Page         3    831.4    277.13 3104  0.09278 .
  Residuals   16   1618.4    101.15
  ---
  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
  ```

  The p-value `Pr(Prob)` is $0.09278$. This means that, with the same underlying stickiness, **$9.3 \%$ of the time**, <u>the response rate among the four pages could differ significantly from what was observed.</u> 

  The column `Iter` indicates <u>the number of iterations completed in the permutation test.</u> The remaining columns relate to a standard ANOVA table. 

- In Python, we can compute the permutation test as follows.

  ```python
  observed_variance = four_sessions.groupby('Page').mean().var()[0]
  print('Observed Means:', four_sessions.groupby('Page').mean().values.ravel())
  print('Variance:', observed_variance)
  
  def perm_test(df):
    df = df.copy()
    df['Time'] = np.random.permutation(df['Time'].values)
    return df.groupby('Page').mean().var()[0]
  
  perm_variance = [perm_test(four_sessions) for _ in range(3000)]
  print('Pr(Prob)', np.mean([var > observed_variance for var in perm_variance]))
  ```

  

  <br><br>

### F-Statistic

> Similar to how the *t-test* serves <u>as an alternative to a permutation test for comparing the means of two groups</u>, a statistical test for ANOVA utilizes the <b><I>F-statistic</I></b>.

The F-Statistic measures **the variance ratio between group means** (treatment effect) **to residual error variance.** A higher ratio indicates a more significant result. 

- In R, we utilize `aov` function to generate ANOVA table.

  ```R
  > summary(aov(Time ~ Page, data=four_sessions))
  ---
              Df Sum Sq Mean Sq F value Pr(>F)
  Page         3  831.4   277.1    2.74 0.0776 .
  Residuals   16 1618.4   101.2
  ---
  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
  ```

- In Python, we use `statsmodels` package.

  ```python
  model = smf.ols('Time ~ Page', data=four_sessions).fit()
  
  aov_table = sm.stats.anova_lm(model)
  aov_table
  ```



`DF` = degrees of freedom, `Sum Sq` = sum of squares, `Mean Sq` = mean squared deviations, `F value` = F-statistic. For the grand average, the sum of squares is the squared departure from $0$, multiplied by $20$ (the number of observations). The degrees of freedom for the grand average is defined as $1$.

**Treatment** means have $3$ degrees of freedom (once three values are set and the grand average is determined, one treatment mean remains constant). The **sum of squares** for treatment means is the <u>total of squared deviations from the grand average.</u> 

**Residuals** have $16$ degrees of freedom ($20$ observations, with $16$ varying after the grand mean and treatments). **SS** is the sum of squared <u>differences between individual observations and treatment means.</u> **Mean Squares (MS**) <u>is the sum of squares divided by degrees of freedom.</u> 

The **F-statistic** is the ratio <u>MS(Treatment)/MS(Error)</u>. It compares this ratio to the F-distribution <u>to assess if treatment mean differences exceed random chance variation.</u> <br><br>
