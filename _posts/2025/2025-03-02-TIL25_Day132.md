---
title: "Day132 - STAT Review: Classification (2)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../../
tag: [StatReview, TIL_25]
toc: true 
---

# (Editing) Practical Statistics for Data Scientists: Discriminant Analysis (Covariance, Discriminant Function, and Application)

![88F1B083-2CF7-4E95-9E5C-7F49249CF1D2_1_105_c](../../images/2025-03-02-TIL25_Day132/88F1B083-2CF7-4E95-9E5C-7F49249CF1D2_1_105_c.jpeg)<br><br>

## Discriminant Analysis

Discriminant Analysis is one of the earliest statistical classification techniques, introduced by **R. A. Fisher in 1936**. It is used to distinguish between different **classes** in a dataset based on their features.

**When to Use Discriminant Analysis?**

- When the **predictor variables are continuous** (or categorical).
- When we want to classify observations into distinct groups.
- When we assume that the classes **follow a normal distribution** and have equal variance.<br><br>

#### Key Terms for Discriminant Analysis

- **Covariance**
  - It quantifies <u>how much one variable changes</u> concerning another, indicating <u>the direction and magnitude</u> of their relationship.
- **Discriminant Function**
  - The function **that maximizes class separation** when applied to the <u>predictor variables.</u>
- Discriminant Weights
  - The scores obtained from using the discriminant function **estimate the probabilities of belonging to one class or another.**  

Although discriminant analysis includes several techniques, the most frequently utilized at that time was *linear discriminant analysis (LDA)*. However, LDA has become less prevalent with the emergence of more advanced methods, such as tree models and logistic regression. However, we may still come across LDA in some applications, and it is connected to other more commonly used methods (like **principal components analysis**). <br><Br>



#### Covariance Matrix

Covariance measures **how two variables vary together**. If two variables <u>increase and decrease togethe</u>r, they have **positive covariance**; if <u>one increases while the other decreases</u>, they have **negative covariance**.

For two variables $X$ and $Y$, the covariance is calculated as:

<center>
  $\text{Cov}(X,Y) = \frac{\sum_{i-1}^n(X_i -\bar{X})(Y_i-\bar{Y})}{n-1}$<Br><br>
</center>

Where:

- $X_i, Y_i$ = individual values of $X$ and $Y$.
- $\bar{X}, \bar{Y}$ = means of $X$ and $Y$.
- $n$ = number of records.

The **covariance matrix** is a matrix that contains variances on the diagonal and covariances off the diagonal as follows.

<center>
  $\Sigma =
\begin{bmatrix}
\text{Var}(X_1) & \text{Cov}(X_1, X_2) & \dots & \text{Cov}(X_1, X_n) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2) & \dots & \text{Cov}(X_2, X_n) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_n, X_1) & \text{Cov}(X_n, X_2) & \dots & \text{Var}(X_n)
\end{bmatrix}$ <br><br>
</center>

The covariance matrix is essential in **Linear Discriminant Analysis (LDA)** because it helps measure how different classes are spread in feature space. <Br><br>
