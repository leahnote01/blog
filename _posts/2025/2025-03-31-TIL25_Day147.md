---
title: "Day147 - STAT Review: Statistical Machine Learning (9)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [StatReview, TIL_25]
toc: true 
---

# Practical Statistics for Data Scientists: Boosting (2) (Regularization: Avoiding Overfitting)

![A8E6C737-24BC-4F71-9568-134CB096E10A_1_105_c](../../images/2025-03-31-TIL25_Day147/A8E6C737-24BC-4F71-9568-134CB096E10A_1_105_c.jpeg) 

<br>

## Regularization

> Applying `XGBoost` without careful consideration can result in unstable models due to overfitting the training data. 

The issue of overfitting has two main aspects:

- The model's **accuracy** on new data not in the training set will be degraded.
- The **predictions** from the model are highly **variable**, leading to **unstable** results. 

All modeling methods can risk overfitting. For instance, **including excessive variables in a regression equation may lead to misleading predictions.** Fortunately, in most statistical approaches, <u>careful choice of predictor variables can help prevent overfitting</u>. Additionally, **random forests** typically yield a good model even without parameter adjustments. However, <u>overfitting in XGBoost can occur relatively quickly if we continue adding trees without managing the complexity.</u>

<br>

#### How Can We Spot Overfitting?

