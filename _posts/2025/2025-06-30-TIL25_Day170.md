---
title: "Day170 - MLOps Review: Model Development and Offline Evaluation (4)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, TIL_25]
toc: true 
---

# Designing Machine Learning Systems: Model Offline Evaluation (Methods: Perturbation Tests, Invariance Tests, etc.)

![F45DE1D9-35DC-40B1-9BDE-89E9EB72F606_1_105_c](../../images/2025-06-30-TIL25_Day170/F45DE1D9-35DC-40B1-9BDE-89E9EB72F606_1_105_c.jpeg)

<br>

### Evaluation Methods

> In academic settings, when evaluating ML models, people tend to fixate on their performance metrics. However, in production, we also want our models to be robust, fair, calibrated, and overall make sense. We’ll introduce some evaluation methods that help with measuring these characteristics of a model.



#### 1. Perturbation Tests (Robustness)

Ideally, your model's inputs <u>should match those in production</u>; however, **this is often not feasible due to the cost or challenges associated with data collection**, resulting in training data that differs from real-world data. *The best training performance doesn't guarantee the best real-world performance.*

- **Goal:** Understand how your model behaves <u>under noisy, real-world conditions.</u>
- **Method:** Add noise or alterations to your test data and evaluate the model.

> Example: For a cough-detection model, **simulate noise from background music or poor mic quality.** If performance drops drastically, your model is brittle.

Used in:

- Audio and speech models
- Image models (e.g., adversarial pixel perturbations)
- NLP (word shuffling, typos, etc.)

<br>

#### 2. Invariance Tests (Fairness)

When researchers used income and credit scores from rejected applications, **removing race-identifying features**, the applications were accepted. <u><I>Some input changes shouldn’t affect outcomes, such as race, name, or gender in pay</I></u>. If they do, it indicates bias, risking the model's usability regardless of performance.

- **Goal:** Ensure the model's output doesn’t change when it *shouldn’t*.

> If changing the applicant's name or gender changes a resume screening result, the model is biased.

- **Approach:**
  - Hold all features constant except sensitive ones (e.g., race, gender)
  - **Check if outputs change—if yes, that’s a red flag.**

<br>

#### 3. Directional Expectation Tests (Sanity Checks)

If <I><u>the outputs change in a manner opposite to expectations</u></I>, the model may not be learning correctly, requiring further investigation before deployment.

- **Goal:** Ensure the model behaves as expected with *logical changes* in input.

> If increasing a house’s square footage causes the predicted price to drop, your model might be mislearning.

- Use this to:
  - Catch non-intuitive model logic.
  - Reveal data leakage or label corruption.
  - Validate interpretability.

<br>

#### 4. Model Calibration (Probability Quality)

Imagine someone predicts *a 70% chance of an event.* This means that over many predictions, the outcome matches 70% of the time. If a model predicts team A will beat team B with 70% probability, **but team A only wins 60% of the 1,000 matches, it's uncalibrated.** A calibrated model would predict 60%.

- <u>**Goal:** Align predicted probabilities with real-world outcomes.</u>

> If your model predicts a 70% chance that a user will click an ad, it should occur 70% of the time over many samples.

- **Uncalibrated models can:**
  - Mislead users
  - Harm decision-making (e.g., underestimating rare events)

- **How to check:**
  - <u>Use reliability diagrams or calibration plots</u> (e.g., from `sklearn.calibration_curve`)
  - <u>Perfectly calibrated models lie on the diagonal line (X = Y)</u>

<center>
  <img src="../../images/2025-06-30-TIL25_Day170/image-20250704182842745.png" width="65%"><br><br>
</center>



- **How to fix:**
  - Platt Scaling
  - Isotonic Regression
  - `CalibratedClassifierCV` in Scikit-learn

<br>

#### 5. Confidence Thresholding (Decision Trust)

Not all predictions are equally reliable.

> For example, a predictive policing system should flag only cases where the model is confident.

Key questions:

- At what confidence threshold do you show or hide predictions?
- What do you do with low-confidence cases (e.g., flag for human review)?

**Tip:** Confidence scores can be used to route cases (e.g., automate high-confidence, escalate low-confidence).

<br>

#### 6. Slice-Based Evaluation (Fairness + Critical Use Cases)

**Goal:** Check performance on different subsets of your data.

Why?

- Models might work for the majority but fail for minorities.
- Some slices (e.g., paid users, mobile users) are more critical.

**Examples:**

| **Slice**               | **Reason for Evaluation**          |
| ----------------------- | ---------------------------------- |
| Mobile vs Desktop users | Different UI/UX behavior           |
| Age groups              | Healthcare predictions             |
| Geography               | Language or culture variance       |
| Income                  | Credit scoring or pricing fairness |



#### Common Problems Slice Analysis Reveals:

- Hidden bias
- Simpson’s paradox (model performs worse on *both* slices but better overall due to distribution skew)
- Usability issues (e.g., bad button layout for mobile users)

**How to discover slices:**

- Domain knowledge (e.g., browser type, platform)
- Error analysis (cluster misclassified samples)
- Tools: Slice Finder (Chung et al.)

<br>

### Best Practices for Offline Evaluation

1. **Always define a baseline** before building complex models
2. Use **multiple metrics**: including accuracy, F1 score, precision, recall, AUC, calibration, and confidence.
3. **Stress test models** with perturbation and invariance checks
4. Track **per-slice performance**, not just global metrics
5. Calibrate probabilities and use **confidence-based decision-making**
6. Avoid test data leakage — *never tune hyperparameters on test set*



<Br><Br>
