---
title: "Day161 - MLOps Review: Training Data (2)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, TIL_25]
toc: true 
---

# (Editing) Designing Machine Learning Systems: Labelling (Hand Labels, Natural Labels, & Handling the Lack of Labels) and Class Imbalance 

![B3D64639-0899-41C8-8384-1C8FACAE0DCD_1_105_c](../../images/2025-05-23-TIL25_Day161/B3D64639-0899-41C8-8384-1C8FACAE0DCD_1_105_c.jpeg)

<br>

## Labelling

Labeling plays a pivotal role in supervised machine learning systems. Although there is much excitement around unsupervised and self-supervised methods, <u><i>the reality in production ML today still heavily leans on supervised models, which require quality labeled data</i></u>. A model's accuracy and generalization are often bottlenecked not by architecture or hyperparameters but by how well the training data is labeled.



### The Evolution of Labeling

Labeling has evolved from a secondary task to a core component of ML workflows. As Andrej Karpathy humorously put it when asked how long Tesla would need a labeling team: ***"How long do we need an engineering team for?"*** This statement reflects the shift in mindset that data labeling is as vital as model development.

<br>

#### Hand Labeling: The Gold Standard with Practical Challenges

Hand labeling, though common, presents several practical difficulties:

- **Costly**: Hiring experts like radiologists for specialized domains is expensive.
- **Privacy Concerns**: Sensitive data, such as medical records, can't be easily outsourced.
- **Slow Iteration**: Transcribing an hour of phonetic speech can take 400 hours.

These limitations hamper the speed of model iteration. Consider a sentiment analysis system that initially distinguishes only between POSITIVE and NEGATIVE sentiment. Suppose the product team later requires the model to recognize ANGRY sentiment as a new class. In that case, existing data may need to be relabeled or new examples collected, which significantly slows adaptation.

<br>

#### Label Multiplicity and Annotator Disagreement

With multiple annotators, inconsistencies arise due to varying expertise and interpretations. For example, three annotators might mark different spans as entities in an entity recognition task. To address this, it's critical to:

- Define a **clear annotation policy**.
- Train annotators with **consistent guidelines**.

Unresolved disagreement raises the question: <u><i>if experts can't agree, what does "ground truth" really mean?</i></u>

<br>

#### Data Lineage: Tracing Label Quality

<u><i>Mixing labeled data from various sources can degrade model performance.</i></u> Tracking **data lineage**—where each label came from, how it was generated, and by whom—is essential for debugging. For instance, *if newly added labels reduce model accuracy, data lineage helps isolate and correct the issue.*

<br>

#### Natural Labels: Leveraging Built-In Feedback

Some tasks provide labels naturally as part of system feedback. These include:

- **ETA estimation in navigation**: The actual duration is recorded automatically.
- **Stock prediction**: Ground truth is revealed by future prices.
- **Recommendation systems**: User clicks and skips serve as implicit feedback.

These naturally occurring labels are called <u>**behavioral labels** and are increasingly common in industrial applications.</u>

The **feedback loop length** matters here:

- **Short loops** (clicks on ads) enable rapid iteration.
- **Long loops** (fraud detection) delay response, potentially increasing risk.

Understanding feedback types and their reliability is crucial in system design.

<br>

### Handling Label Scarcity

Due to the difficulty of acquiring labeled data, several alternative strategies exist:

#### 1. Weak Supervision

Use heuristics or labeling functions (LFs) to label data programmatically. Examples:

- Keyword matches
- Regex patterns
- Outputs from existing models

Tools like **Snorkel** combine and denoise multiple noisy labels. Programmatic labeling offers:

- **Cost savings**
- **Faster scaling**
- **Greater privacy control**

Case studies show weak supervision can match hand-labeling performance with far fewer resources.

#### 2. Semi-Supervised Learning

Leverages a small labeled dataset to train a model, which is then used to label unlabeled data. Techniques include:

- **Self-training**: Use confident predictions to generate new labels.
- **Similarity-based assumptions**: Similar instances share labels.
- **Perturbation-based methods**: Slight input variations should not change the label.

Semi-supervision can match supervised performance effectively, especially with modern representation learning.

#### 3. Transfer Learning

Instead of training from scratch, use a **pretrained model** (e.g., BERT, GPT) and fine-tune it on your specific task.

- Reduces the amount of labeled data required
- Accelerates development

Transfer learning has democratized ML, making it accessible to teams without vast labeling budgets.

#### 4. Active Learning

Allows the model to **choose the most informative samples** to label, reducing the label volume needed:

- **Uncertainty sampling**: Choose the samples the model is least confident about.
- **Query-by-committee**: Use ensemble disagreement to pick ambiguous samples.

This efficient approach in dynamic or streaming environments enables continual model improvement.

Labeling is a data preparation step and a strategic process determining model success. From hand-labeling to weak supervision, natural feedback, and advanced sampling strategies, a modern ML system must intelligently balance cost, privacy, speed, and accuracy in its approach to labeling.

<br><Br>

