---
title: "Day141 - STAT Review: Statistical Machine Learning (3)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../../
tag: [StatReview, TIL_25]
toc: true 
---

# (Editing) Practical Statistics for Data Scientists: K-Nearest Neighbors (3) (KNN as a Feature Engine) & Tree Models (1)

![B3873656-BE2F-47E8-834D-C1A399D07448_1_105_c](../../images/2025-03-20-TIL25_Day141/B3873656-BE2F-47E8-834D-C1A399D07448_1_105_c.jpeg)

<br>

### KNN as a Feature Engine

KNN is **not always the best-performing final model**, especially compared to advanced models like random forests or gradient boosting. But—it’s **beneficial as a feature engineering tool**.

The idea comes from using KNN to **generate new features** that describe local behavior in the data and feed those into a **more robust model** later.

In practical model fitting, however, KNN can contribute “local knowledge” in a staged process alongside other classification techniques:

1. KNN is run on the data, and a **classification** (or quasi-probability of a class) is derived for each record.
2. **That result is added as a new feature to the record**, and another classification method is then run on the data. Thus, the original predictor variables are used twice.

You may wonder <u>if using some predictors twice creates multicollinearity.</u> It does not, as **the second-stage model incorporates highly local information** from a few nearby records, <u>providing additional, non-redundant information.</u> 

<br>

#### How It Works – Step-by-Step:

1. **Run KNN** on your dataset (either classification or regression).
2. For each record, calculate:
   - Its **predicted class**, or
   - The **probability** (e.g., likelihood of default)
3. **Add that output as a new feature** in the dataset.
4. Now, train a more robust model (e.g., logistic regression, decision tree, or XGBoost) on the entire dataset, **including the new KNN-based feature**.

<br>

For example, consider the King County housing data we reviewed earlier. When pricing a home for sale, a realtor determines the price based on recently sold comparable homes, commonly called "**comps**."

In essence, realtors <u>utilize a manual approach similar to KNN</u>; they assess the sale prices of comparable homes to estimate a property's selling price. By using KNN on recent sales data, <u>we can create a new feature for a statistical model that mimics real estate professionals.</u>

**The predicted value** is the sales price, and the existing predictor variables may <u>include location, total square footage, type of structure, lot size, and the number of bedrooms and bathrooms</u>. **The new predictor variable (feature) we introduce through KNN is the KNN predictor for each record, similar to the realtors’ comps.** Since we are predicting a **numerical** value, we utilize the **average** of the K-Nearest Neighbors instead of a majority vote, a method known as *KNN regression.*

Similarly, we can create features representing <u>different aspects of the loan process for the data</u>. For example, the following *R* code would build a feature representing a borrower’s creditworthiness.

- In R

  ```R
  borrow_df <- model.matrix(~ -1 + dti + revol_bal + revol_util + open_acc +
                            delinq_2yrs_zero + pub_rec_zero, data=loan_data)
  borrow_knn <- knn(borrow_df, test=borrow_df, cl=loan_data[, 'outcome'],
                    prob=TRUE, k=20)
  prob <- attr(borrow_knn, "prob")
  borrow_feature <- ifelse(borrow_knn == 'default', prob, 1 - prob)
  summary(borrow_feature)
  ---
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
    0.000   0.400   0.500   0.501   0.600   0.950
  ```

- In *Python*, we use `predict_proba` method from `scikit-learn` of the trained model to get the probabilities.

  ```python
  predictors = ['dti', 'revol_bal', 'revol_util', 'open_acc',
                'delinq_2yrs_zero', 'pub_rec_zero']
  outcome = 'outcome'
  
  X = loan_data[predictors]
  y = loan_data[outcome]
  
  knn = KNeighborsClassifier(n_neighbors=20)
  knn.fit(X, y)
  
  loan_data['borrower_score'] = knn.predict_proba(X)[:, 1]
  loan_data['borrower_score'].describe()
  ```

  The result is a feature that predicts the likelihood of a borrower defaulting based on their credit history.

<br>

## Tree Models

> **Tree models**, also known as **Classification** and **Regression Trees (CART)**, **decision trees**, or simply trees, are an effective and popular method for machine learning classification (and regression). 

- **Classification**: Predicting categories (e.g., will a customer default? Yes/No
- **Regression**: Predicting continuous values (e.g., what price will this house sell for?)

These models, along with their more powerful derivatives like random forests and boosted trees (see “Bagging and the Random Forest” and “Boosting”), form the foundation of the most widely used and robust predictive modeling tools in data science for both regression and classification.

<br>

#### Key Terms for Trees

