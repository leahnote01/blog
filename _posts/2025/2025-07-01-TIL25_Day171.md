---
title: "Day171 - MLOps Review: Model Deployment And Prediction Service (1)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, TIL_25]
toc: true 
---

# Designing Machine Learning Systems: Model Deployment (Definition, Common Myths) and Batch Prediction Versus Online Prediction

![34BEB9D9-BD9F-4535-8487-8ADA68657E1F](../../images/2025-07-01-TIL25_Day171/34BEB9D9-BD9F-4535-8487-8ADA68657E1F.jpeg)

<br>

## Model deployment

### What Is Deployment in ML?

So far, we've discussed how to collect and clean data, extract useful features, and train and evaluate ML models. This is the **‚Äúmodel logic‚Äù** ‚Äî the pipeline that takes raw input and produces a trained model. It‚Äôs the part that data scientists and ML engineers typically own.

But once the model is trained, the next question is:

> **"How do I make this model usable by real users or systems?"**

That‚Äôs what **deployment** is about ‚Äî <u>moving the model from development to a <b>production environment</b>,</u> where it can respond to real requests from applications or users.

<br>

#### What Does "Deploying" Really Mean?

- Deployment involves activating your model. '**Running**' indicates that the model is active and working; '**Accessible**' means that other systems can send requests to receive predictions.


A typical production deployment means:

- Wrapping the model's `predict()` function into an **API endpoint** (e.g., with Flask or FastAPI)
- Packaging code + dependencies into a **container** (e.g., Docker)
- Pushing the container to a **cloud service** (e.g., AWS, GCP, Azure)
- Exposing the endpoint so your app/frontend can send requests and get predictions

When moving beyond hobby projects to real-world deployment, several significant challenges arise:

- **Latency constraints:** Responses may be required within milliseconds.
- **Scalability:** Support for thousands or even millions of users is necessary.
- **Monitoring:** Alerts and logs are crucial when the model or infrastructure encounters issues.
- **Reliability:** Maintaining a 99.9% uptime standard is the goal.
- **Model updates:** Fixes, enhancements, or retraining must be implemented seamlessly.

These factors make production ML challenging and require **robust DevOps/MLOps** practices.

<br>

#### **Exporting Models for Deployment**

Before deployment, you need to **serialize** the trained model into a format that is portable.

##### Two parts of a model:

1. <u><b>Model Definition</b> ‚Äì The architecture (e.g., number of layers, units)</u>
2. <u><b>Learned Parameters</b> ‚Äì The weights learned during training</u>

##### Examples:

- **TensorFlow 2**: `model.save()` ‚Üí `SavedModel` format
- **PyTorch**: `torch.onnx.export()` ‚Üí `ONNX` format

Exporting ensures <u>the model can be loaded and used in <b>another environment</b> (e.g., a cloud service, mobile app, or API).</u>

<br>

#### **Why Deployment Often Fails in Teams**

In some orgs:

- **The ML team both builds and deploys** *<u>(a common practice in startups).</u>*
- In others:
  - The ML team builds ‚Üí The deployment team handles deployment.

However, **splitting these roles** can lead to <u>increased communication overhead, slower feedback loops, and greater difficulty in debugging model issues</u> in production. A better practice is *<u>to have cross-functional teams with shared ownership and precise tooling.</u>*

<br>

#### Common Myths About Deployment

##### Myth 1: "You Only Deploy One or Two Models"

Wrong. In real-world companies, you often deploy **hundreds or thousands** of models.

##### Example: Uber

- Models for: demand, supply, ETA, pricing, fraud, churn, etc.
- Each **country or region** may require a separate version of the model due to local differences.

> A real app like Uber could easily have **200+ models** in production.

##### Netflix Example

The author references a chart that shows dozens of ML use cases, including personalization, search ranking, and streaming quality optimization.



##### Myth 2: "Model Performance Doesn‚Äôt Decay"

False. ML models decay over time due to <u>software rot</u>, as libraries or systems evolve, breaking compatibility, and due to <u>data distribution shift</u>, where real-world data changes, such as user behavior and market trends. So a model that performed well last month might now be **outdated or harmful**.



##### Myth 3: "We Don‚Äôt Need to Update Often"

Wrong again. <u>ML models require regular updates</u> because they tend to decay (see Myth 2), competitors advance, and business objectives evolve. 

Real-World Example:

- Weibo: **Retrains models** every 10 minutes
- ByteDance, Alibaba**: Similarly fast update cycles**
- Netflix, Etsy, AWS: **Deployed changes** dozens to thousands of times per day
- Quote: ‚Äú*<u>How often can we update our models?</u>*‚Äù is a better question than ‚ÄúHow often should we?‚Äù



##### Myth 4: "Only Big Tech Needs to Worry About Scale"

Wrong again. According to the <u>Stack Overflow Developer Survey:</u>

- More than **50%** of developers work at companies *<u>with 100 or more employees.</u>*
- At this size, ML systems often serve **large user bases.**

So if you want to work in ML in industry, you should **expect to deal with scale**, not avoid it.

<br>

### Batch Prediction Versus Online Prediction

**Batch Prediction** and **Online Prediction** refer to **how and when** a model generates predictions for users or applications:

#### üü© Batch Prediction (Asynchronous)

- **How it works:** Predictions are **precomputed** for a batch of inputs at regular intervals (e.g., every hour, every night) and stored for later retrieval.
- **Example:** Netflix precomputes recommendations for all users every 4 hours.
- **Advantages:**
  - Efficient: Can leverage distributed systems like Spark to compute predictions in parallel.
  - Low latency at serving time: Retrieve stored results.
- **Disadvantages:**
  - Not real-time: Can't adapt to user behavior immediately.
  - Wasteful if only a small percentage of users log in (e.g., Grubhub‚Äôs 2% daily active users).
  - You must **predict in advance** which inputs to generate predictions for.

#### üü¶ Online Prediction (Synchronous)

- **How it works:** Predictions are **generated on demand** as soon as a request arrives.
- **Example:** You type a sentence into Google Translate and get an instant translation.
- **Advantages:**
  - Real-time: Responds to the user‚Äôs current behavior and context.
  - Only compute predictions for actual users (no waste).
- **Disadvantages:**
  - Requires low-latency infrastructure and fast models.
  - Computationally more expensive if not optimized.

<br>

#### Streaming Features vs. Batch Features

- **Batch Features:** Derived from historical data (e.g., average prep time of a restaurant over the past month).
- **Streaming Features:** Derived from real-time data (e.g., how many delivery drivers are currently active).
-  **Online prediction** can use either:
  1. Only batch features (e.g., embedding vectors)
  2. Both batch and streaming features are often referred to as **streaming prediction**.





#### Why Use Batch Prediction?

Batch is ideal when:

- Real-time responsiveness is not critical.
- You have a massive number of users or items (e.g., millions of restaurant recommendations).
- You want to leverage distributed batch processing (MapReduce, Spark).
- Use Case:

```text
Generate likelihood scores for all customers on buying a new product.
Email the top 5% based on scores.
```





<br>

#### Why Use Online Prediction?

Online is necessary when:

- You need **instant feedback or decision-making** (e.g., fraud detection, real-time translation).
- Predictions must reflect the most **recent user behavior** (e.g., browsing comedy movies after browsing horror movies).
- Inputs are **unpredictable** (e.g., translating arbitrary English sentences).
- Use Case:

```text
Face recognition to unlock your phone ‚Üí decision in milliseconds.
```



<Br>

#### From Batch to Online: Transition & Hybrid

Sometimes you start with **batch prediction** and migrate to **online** as the infrastructure matures. Many companies now use a **hybrid** approach:

- **Batch** for frequently accessed predictions (popular items/users).
- **Online** for rare, personalized, or unpredictable inputs.





#### Infrastructure Challenge: Mismatched Pipelines

Batch and online often require **separate pipelines**:

- Batch for training (e.g., DataFrames, Spark jobs).
- Streaming for inference (e.g., sliding windows, stream processors).

- Problem: Divergence between pipelines = **bugs and inconsistent features**.
- **Solution**:
  - Use **stream processors** like Apache Flink to unify batch + stream pipelines.
  - Use **feature stores** to ensure feature consistency during both training and serving (covered in Chapter 10).

<br><br>
