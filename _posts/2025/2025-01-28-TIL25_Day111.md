---
title: "Day111 - STAT Review: Data & Sampling Distributions (1)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../../
tag: [dlReview, TIL_25]
toc: true 
---

# Practical Statistics of Data Scientists: Sampling, Bias, and Sampling Distribution(Central Limit Theorem)

![A6EDFC05-568C-4C4A-890B-B2976BFD4DB0](../../images/2025-01-26-TIL25_Day2/A6EDFC05-568C-4C4A-890B-B2976BFD4DB0.jpeg)<bR><Br>

*Book Source: Practical Statistics for Data Scientists, 2nd Edition, by Peter Bruce, Andrew Bruce, Peter Gedeck / Released May 2020 / Publisher(s): O'Reilly Media, Inc.*



> A common misunderstanding is that the era of big data renders sampling unnecessary. In reality, **the vast amount of data**—often inconsistent in quality and relevance—**underscores the critical importance of sampling.** This approach is essential for effectively managing diverse datasets and reducing bias.

Traditional statistics focused on analyzing entire populations based on theories that rely on strong assumptions about those populations. Conversely, modern statistics has transitioned to **sample analysis**, which does not necessitate these assumptions. <br><BR>

### Random Sampling and Sample Bias

#### Key Terms for Random Sampling

- Sample
  - A subset from a more extensive data set.
- Population
  - The more extensive data set or idea of a data set.
- N(n)
  - The size of the population
- Random Sampling
  - Drawing elements into a sample at random.
- Stratified Sampling
  - Dividing the population into strata and randomly sampling from each stratum.
- Stratum (pl., strata)
  - A homogeneous subgroup of a population <u>with common characteristics</u>. 
- Simple Random Sample
  - The sample results from random sampling without stratifying the population.
- Bias 
  - Systematic error.
- Sample Bias
  - A sample that misrepresents the population.

Self-selection samples might not always reflect the actual situation accurately, but <u>they can help compare similar places as the same biases affect both.</u><br><br>

#### Bias

> Statistical bias refers to **errors** in measurement or sampling arising from the systems used or the measurement and sampling processes themselves.

It is crucial to differentiate between errors arising from random chance and those stemming from bias.

Bias manifests <u>in various forms</u>, sometimes being <u>visible</u> while at other times remaining <u>hidden</u>. When a result indicates bias, such as through comparison with a benchmark or actual values, it's often a sign that a statistical or machine learning model has been improperly specified or that a key variable has been omitted.<br><br>



#### Random Selection

Random sampling poses difficulties because of diverse situations and conditions. Therefore, <u>clearly defining an accessible population is crucial.</u> Let's say we need to conduct a pilot customer survey.

1. **Define who a customer is** (e.g., those who make purchases once a month, twice a month, etc.).
2. **Specify a sampling procedure** (e.g., randomly select 100 customers or visitors from 10 am to 10 pm).

The population is divided into strata in *stratified sampling*, and random samples are drawn from each stratum.<br><br>



#### Size vs. Quality

- Surprisingly, there are instances when smaller is better because the outliers may contain helpful information.
- It's better to have a larger dataset when dealing with large, sparse data. 
  - We must also evaluate <u>the costs and effectiveness of managing big data.</u>
  - **Data quality is typically much more important** than data quantity, and random sampling can help lower bias and promote quality enhancements that would otherwise be excessively challenging or expensive.<br><br>



#### Sample Mean vs. Population Mean

- $\bar{x}$ ("x-bar") represents the mean of a sample from a population, while μ denotes the mean of a population. <br><br>





### Selection Bias

> Selection bias occurs when data is chosen selectively, resulting in misleading or temporary conclusions.

#### Key Terms for Selection Bias

-  Selection Bias
  - Bias occurred due to the method of selecting observations. 
- **Data Snooping**
  - Extensive hunting through data in search of something interesting.
- **Vast Search Effect**
  - Bias or unreliability can stem <u>from repeated data modeling or using numerous predictor variables.</u>
  - As <u>repeated analysis of extensive data sets</u> is crucial in data science, <u>selection bias is essential.</u> 
  - You will likely uncover interesting findings if you consistently test different models and ask various questions using a large dataset. However, <u><i>are your discoveries genuinely significant or just a random outlier?</i></u><br><br>



### Regression To the Mean

> Regression to the mean describes a phenomenon involving consecutive measurements of a specific variable: **extreme observations are likely to be succeeded by more central ones.**

- This outcome arises from a specific type of selection bias. Skills and luck likely play a role when we choose the rookie with the best performance. In the following season, while the skill remains, luck often does not, leading to a decline in performance—essentially, a **regression**. 
- Regression to the mean, which means "to go back,” differs from the statistical modeling method of linear regression, where a linear relationship is estimated between predictor variables and an outcome variable.<br><br>





### Sampling Distribution of a Statistic

#### Key Terms for Sampling Distribution

- Sample Statistic
  - A measurement derived from a sample of data collected from a larger population.
- Data Distribution
  - The frequency distribution of individual values within a data set.
- Sampling Distribution
  - The frequency distribution of a sample statistic across numerous samples or resamples.
- **Central Limit Theorem**
  - The sampling distribution **assumes a normal shape as the sample size increases.**
- **Standard Error**
  - A sample statistic's variability (standard deviation) across several samples should not be confused with standard deviation, which pertains to the variability of individual data values.

A sample is often taken to measure or model something. Since our estimate or model <u>is based on this sample</u>, it can have **errors** and might change if we took another sample. We need to understand how much it can vary, called sampling **variability**. If we had lots of data, we could take more samples to see how a sample statistic behaves. Usually, we use as much data as possible to make our estimate or model.

<br><Br>

#### Central Limit Theorem

The central limit theorem states that means from multiple samples resemble <b><i>a bell-shaped normal curve</i></b>, even if the source population isn’t normally distributed, provided the sample size is large enough and the deviation from normality is minimal. <u>This theorem allows normal approximation formulas like the t-distribution to calculate sampling distributions in inference, including confidence intervals and hypothesis tests</u>. <br><Br>



#### Standard Error

The standard error **measures variability in the sampling distribution for a statistic.** It can be estimated using the sample standard deviation s and size $n$. As sample size increases, standard error decreases.

<center>
  $\text{Standard Error} = SE = \frac{s}{\sqrt{n}}$<Br><Br>
</center>

You *actually don’t have to depend* on the central limit theorem to grasp standard error. Here’s a method for measuring it:

1. Gather several fresh samples from the population.

2. For each sample, compute the statistic (like the mean).

3. **Next, determine the standard deviation of the statistics obtained in the previous step; this will serve as your standard error estimate.**

In reality, collecting new samples to estimate standard error can be impractical (and statistically inefficient). Thankfully, you don’t need to obtain entirely new samples; **bootstrap resamples are a viable alternative.** 

<br><br>
