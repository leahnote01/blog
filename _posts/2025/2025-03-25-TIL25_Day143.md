---
title: "Day143 - STAT Review: Statistical Machine Learning (5)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [StatReview, TIL_25]
toc: true 
---

# Practical Statistics for Data Scientists: Tree Models (3) (Dealing With Overfitting Problems in R and Python & Predicting a Continuous Value)

![59662805-981D-444B-B0A8-3B9BED8393F9_1_105_c](../../images/2025-03-25-TIL25_Day143/59662805-981D-444B-B0A8-3B9BED8393F9_1_105_c.jpeg)

<Br>

### Stopping the Tree from Growing

When we let a decision tree grow **without restriction**, <u>it can create a split for almost every variation in the training data.</u> This leads to:

- **Pure leaves** (100% accuracy on training data)
- But terrible performance on **new, unseen data**

This is called **overfitting** — the tree learns not just the signal but the **noise,** too.

We need a method to determine <u>when to stop growing a tree at a stage</u> that will generalize to new data. There are various approaches to stopping splits in *R* and *Python*:

1. **Minimum Partition Size:**
   - Do not split a partition if the **subpartition** or **terminal leaf** is *too small*. In `rpart`(*R*), this is controlled by `minsplit` (default `20`) and `minbucket` (default `7`). In *Python*’s `DecisionTreeClassifier`, use `min_samples_split` (default `2`) and `min_samples_leaf` (default `1`).
2. **Impurity-Based Stopping:**
   - Avoid splitting a partition **if it doesn't significantly reduce impurity.** In `rpart`, this is managed by the complexity parameter `cp`, <u>measuring tree complexity</u>: **higher** `cp` values indicate **greater** **complexity**. Practically, `cp` <u>limits tree growth</u> **by penalizing additional complexity (splits)**. `DecisionTreeClassifier` in *Python* includes `min_impurity_decrease` <u>to control splits based on weighted impurity decrease.</u> **Smaller** values result in **more complex** trees.

These methods use arbitrary rules and are helpful for exploration, but <u>determining optimal values for maximizing predictive accuracy with new data is challenging.</u> We must **combine cross-validation** with systematically changing the model parameters</u> or <u>modifying the tree through pruning.</u> 

In summary,

- **Minimum Partition Size**: Preventing the tree from making splits that produce **tiny groups**
  -  In R (`rpart`):
    - `minsplit`: Minimum number of observations in a node to consider splitting (default = 20)
    - `minbucket`: Minimum number of observations allowed in a **leaf** (default = 7)
  - In Python (`DecisionTreeClassifier`):
    - `min_samples_split`: Minimum number of samples required to split a node
    - `min_samples_leaf`: Minimum samples allowed in a leaf node

- **Impurity-Based Stopping**: Only split if it **significantly reduces impurity**.
  - In R:
  - `cp` (complexity parameter): Penalizes overly complex trees
    - Low `cp` → big tree, may overfit
    - High `cp` → small tree, may underfit
  - In Python:
    - `min_impurity_decrease`: Split only if impurity drops by a minimum value
    - `ccp_alpha` (cost-complexity pruning): Controls post-training pruning

<br>

### Controlling Tree Complexity in R

With the complexity parameter, `cp`, **we can estimate the optimal tree size for new data.** *If `cp` is too small, the tree will overfit, capturing noise rather than signal. Conversely, if `cp` is too large, the tree will be too small and exhibit limited predictive power.* The `rpart` default is 0.01, **which may be too large for larger datasets.** In the previous example, `cp` was set to 0.005 as the default led to a tree with only one split. **During exploratory analysis, trying a few values is sufficient.**

Finding the optimal `cp` shows the <u>bias-variance trade-off</u>. The common method to estimate `cp` is cross-validation. 

1. Split data into training and validation sets.
2. Grow the tree with training data, **pruning it step by step** while recording `cp` at each step. 
3. Identify the `cp` corresponding **to minimum error on validation data.** 
4. **Re-split data** and repeat the growing, pruning, and cp recording process. 
5. **Average** the `cp`s reflecting minimum error for each tree. 
6. Finally, use the original or future data to grow a tree, stopping at the optimum `cp` value.
