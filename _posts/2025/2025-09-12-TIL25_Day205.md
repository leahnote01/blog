---
stitle: "Day205 - Leetcode: Python 175 & SQL Join & DL Review "
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, DLReview, TIL_25]
toc: true 
---

# Python 175: TwoSums / SQL: Join Revisiting / DL Review: Embedding Layers

![27C0AD67-6E5C-42DB-9390-EC80550D4A31_1_105_c](../../images/2025-09-12-TIL25_Day205/27C0AD67-6E5C-42DB-9390-EC80550D4A31_1_105_c.jpeg)

<br><br>

## ðŸŸ© Python Review

As it had been long time since I reviewed Python, I decided to go through the code line by line. I wrote out each line my self, check out the output, and then updated it by comparision with the solution step by step. This hands-on approach 

#### Two Sum

- Given an array of integers `nums` and an integer `target`, return *indices of the two numbers such that they add up to `target`*
- You may assume that each input would have **exactly one solution**, and you may not use the *same* element twice.
- You can return the answer in any order.

```python
### Solution with my comments

class Solution(object):
    def twoSum(self, nums: List[int], target: int) -> List[int]:
        for i in range(len(nums)):
            for j in range (i + 1, len(nums)):
                if nums[j] == target - nums[i]:
                    return [i, j]
        # Retrun an empty list if no solution is found
        return []

        """
        --> Goes to the type definitions
        :type nums: List[int]
        :type target: int
        :rtype: List[int]
        """
        

"""
- Put constraints
- Randomly select numbers in "nums" and "target"


--- Before input J ==> Not yet entered
-10^9 <= nums[i] <= 10^9
2 <= len(nums) <= 10^4

--- 3rd try
return target = nums[i] + nums[j] ?


"""
```

- Return type: (`nums: List[int]`, `target: int`) -> `List[int]`-

- `for j in range(i + 1, len(nums)):`Nested loop that starts from `i+1` to avoid reusing the same element twice.

  

## ðŸŸ¨ SQL Review

## ðŸŸ¦ DL Review: Embedding Layers

> An **embedding** maps high-dimensional categorical inputs (e.g., words, user IDs, products) <u>into dense, continuous vectors in a lower-dimensional space.</u> These vectors <u>capture semantic similarity and relationships.</u>

**Why it matters**

- **Efficiency**: Reduces dimensionality from millions (one-hot vectors) to a few hundred.
- **Representation Power**: Embedding captures semantic meaning (e.g., word2vec: "king - man" + "woman $\sim$ Queen").
- **Versatility**: Used in NLP, recommendation systems, graph embeddings, etc.

> "Embedding layer learn dense representations of categorical variables, reducing dimensionality and capturing semantic similarities. They're fundamental in NLP and recommendation systems because they transform discrete entities into **meaningful continuous spaces.** "

**MLOps Angle**

* Embeddings can drift in production if distribution shifts, such as new slang in tweets or new items in a recommender system. Monitoring embedding spaces through clustering or cosine similarity changes is essential for maintaining robust pipelines. 

<Br><Br>
