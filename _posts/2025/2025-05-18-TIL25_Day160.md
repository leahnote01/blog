---
title: "Day160 - MLOps Review: Training Data (1)"
layout: single
classes: wide
categories: TIL_25
read_time: True
typora-root-url: ../
tag: [MLOpsReview, TIL_25]
toc: true 
---

# Designing Machine Learning Systems: Sampling (Nonprobability, Simple Random, Stratified, Weighted, Reservoir, and Importance Sampling)

![BBD538C5-840E-463A-8774-E59B886F76CF_1_105_c](../../images/2025-05-18-TIL25_Day160/BBD538C5-840E-463A-8774-E59B886F76CF_1_105_c.jpeg)

<br>

## Training Data

*How data quality, sampling, and representation shape model performance*

In the previous chapter, we explored how data moves between systems. Now, we shift focus to something **arguably even more fundamental in ML**—**the training data** itself.

Despite the buzz around state-of-the-art models and algorithms, most experienced practitioners agree: **models are only as good as the data they're trained on**. Yet, many ML courses emphasize modeling and treat data preparation as an afterthought. <u>This chapter confronts that imbalance and provides tools and insights for managing training data effectively.</u>



### Why Training Data Matters

Training data isn't just the data you use to fit a model—it's everything that shapes how your model behaves in the real world. It includes:

- Data for training, validation, and testing (often split into different subsets).
- Labeled or unlabeled examples, structured or unstructured inputs.
- All data collected or sampled before a model sees the world.

> In short, training data is the raw material of machine learning. <u>If it’s biased, noisy, or insufficient, no model—no matter how advanced—can compensate.</u>

#### A Word of Caution: Data Biases

Training data is riddled with **biases**, many of which are <b>invisible at first glance.</b>  These include **sampling bias**, <u>which refers to collecting data that isn't representative</u>; **historical bias**, <u>which involves using data embedded with human or systemic prejudice</u>; and **labeling bias**, <u>arising from inconsistent or subjective labeling by annotators.</u> While it is essential to **use data**, one should **never trust it blindly**, *as biases can creep in during collection, annotation, storage, and even model evaluation.*

<br>

#### Sampling: The Foundation of Training Data

> Sampling refers to how we select data to include in training and is an essential process that occurs throughout a machine learning (ML) project. It is employed <u>to build the training set from raw data, create validation and test splits, and monitor models during production.</u>

#### Why do we sample?

<u>We often don’t have access to all real-world data,</u> and even when we do, processing all of it is **infeasible**. Sampling enables **quicker experimentation** and **faster iteration**.

<br>

### 1. **Nonprobability Sampling**

In nonprobability sampling, <u>data is not selected based on formal probability.</u> This type of sampling is often **convenient**, but it can *<u>lack objectivity, raising concerns about the reliability of the results obtained from such methods.</u>* Therefore, while nonprobability sampling may be easier to implement, <u>researchers should be mindful of its limitations and potential biases.</u>

#### Examples:

- **Convenience Sampling**: Use what’s easy to access (e.g., Wikipedia for NLP).
- **Snowball Sampling**: Start with a few samples, and expand from there (e.g., following Twitter accounts).
- **Judgment Sampling**: Experts handpick examples.
- **Quota Sampling**: Fixed counts for specific groups, without randomness.

> ⚠️ These methods are quick, but they often introduce bias and fail to represent real-world distributions. Still, they're commonly used in practice, especially in early-stage projects or when high-quality labeled data is scarce.

<br>

### 2. **Random (Probability-Based) Sampling**

These methods <u>guarantee more representative data selections</u> by utilizing statistical rigor.

- **Simple Random Sampling**
  - Every data point in the population <u>has an equal probability of being selected.</u> This method is praised for its straightforwardness and ease of implementation, making it a popular choice in various research fields, including social sciences, healthcare, and market research. <u>Its simplicity allows researchers to implement.</u>
  - **Limitation**: **Rare categories can sometimes be overlooked or missed entirely, leading to potential biases in the analysis.** This oversight might *skew results, especially in studies focused on niche populations or specific subgroups.* To address this limitation, they might consider <u>combining simple random sampling with stratified or cluster sampling methods,</u> which can enhance the diversity of the sample and provide more accurate insights. 
- **Stratified Sampling**
  - Stratified sampling involves dividing data into distinct groups, known as **strata**, and sampling from each group separately. This technique ensures that <u>each group is represented in the sample</u>, which is particularly important when certain groups, even relatively small ones, may be underrepresented in the population. 
  - This method is **commonly utilized in classification** tasks where certain classes, particularly minority ones, may be overlooked without careful sampling strategies**. It helps improve the robustness and reliability of the insights gained from the data.** 
  - **Limitation**: It is challenging to stratify the data appropriately <u>when dealing with overlapping classes or multilabel classification scenarios, where multiple courses may pertain to a single sample.</u>
- **Weighted Sampling**
  - Assigning weights to samples can **significantly influence selection probability in a dataset.** This approach is <u>beneficial when specific subgroups, such as recent users or high-value customers</u>, are more significant. 
  - It becomes essential when the data distribution diverges from real-world expectations, <u>such as during seasonal trends or demographic shifts</u>. 
  - This concept relates **to sample weighting during training**, <u>which subsequently impacts not only the loss function and decision boundaries in a machine learning model but also the overall model performance and interpretability.</u>
- **Reservoir Sampling (for streaming data)**
  - It is needed when:
    - You don't know the total size of the dataset.
    - You can't fit all the data in memory.
    - You want to maintain a representative sample over time, even as new data continuously arrives.
    - <u>The data comes from an unpredictable source where you can't access the entire dataset simultaneously (e.g., Twitter).</u>
    - You require that each element <u>has an equal chance of being included in the final sample, preventing bias in your results.</u>
  - Reservoir Sampling keeps **a fixed-size sample with equal probabilit**y from a potentially infinite stream (e.g., Twitter).
- How it works :
  1. Fill the reservoir with the first *k* elements.
  2. For each new element *n*, <u>pick a random number between 1 and *n*.</u>
  3. If that number is ≤ *k*, <u>replace an existing item in the reservoir.</u>
- Reservoir sampling ensures uniform sampling from a data stream and is helpful in **real-time or resource-constrained environments**.

<br>

### 3. Importance Sampling

Sometimes, you want to **simulate sampling from one distribution**, but only have access to another. That’s where **importance sampling** comes in.

> You sample from a more convenient or efficient distribution $Q(x)$, but **correct the bias** by **reweighting samples** using the ratio $\frac{P(x)}{Q(x)}$.

> You trade sampling ease for statistical correction.

- When is it used?
  1. **Reinforcement Learning**: Estimating new policy performance using past experiences from an older policy.
  1. **Rare Event Modeling**: This technique is crucial for accurately estimating the probabilities

  3. **Simulations**: Often employed when collecting real-world data is impractical due to high costs or slow

- Additional Uses:

  1. **Robotics**: Reinforcement learning is used to train robots to navigate intricate environments adeptly, enhancing their ability to perform tasks autonomously.
  2. **Finance**: It is vital in evaluating risks and constructing investment strategies based on historical data and predicted future trends.
  3. **Healthcare**: This technique is utilized to forecast patient outcomes by analyzing past medical records and treatment results, aiding in better patient care and decision-making

- Key Condition:

  - **Q(x) > 0 wherever P(x) ≠ 0**: This mathematical condition ensures that the <u>estimated state-action values (Q-values) remain positive wherever the corresponding probabilities (P-values) are non-zero.</u> This is essential to prevent overlooking significant regions in the data or possible outcomes, thereby allowing for more accurate and comprehensive modeling.

<br><br>
