---
title: "Day92 Deep Learning Lecture Review - Fine-Tuning Models"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, softmax, activation function, TIL_24]
toc: true 
---

# Pretrained model embeddings (ResNet+SBERT vs. CLIP) for Visual Question Answering (VQA)

<img src="/blog/images/2024-10-17-TIL24_Day92_DL/49689F42-7B59-46FD-902E-CBF66599B817_1_105_c.jpeg"><br><br>

Recently, I conducted several experiments with LoRA (Low-Rank Adaptation), an innovative approach to efficiently fine-tune large pre-trained language models for sentiment analysis and compare it to traditional fine-tuning methods. Here, I want to share the insights I discovered and the key lessons I learned through this experience.<br><br>

## Understanding LoRA

> **LoRA** fine-tunes large language models in a computationally demanding manner, requiring adjustments to all model parameters. However, LoRA cleverly addresses this issue by modifying only the **low-rank components of a weight matrix <u>instead of every parameter</u>. I fine-tuned the `roberta-base` model for sentiment classification using the TweetEval dataset in my experiment. First, I applied standard fine-tuning, followed by LoRA, to evaluate the efficiency and performance of both approaches.

Fine-tuning large language models is often costly in terms of computation <u>due to the need to update all model parameters.</u> I discovered that LoRA offers a more efficient solution by only **modifying low-rank weight matrix components** while keeping the majority of the original model unchanged. This approach significantly lowers the count of trainable parameters, thereby reducing computational expenses. In particular, LoRA enhances weight matrices by incorporating low-rank updates, enabling effective training without compromising performance significantly. 

In my experiments, I analyzed the impact of LoRA on both computational efficiency and model accuracy. I also explored where LoRA could be integrated into Transformer architectures. I learned that LoRA is commonly applied to the query and value projection matrices in the self-attention mechanism, which allows it to efficiently adapt the model while maintaining the integrity of the original pre-trained weights. This approach was insightful because it showcased leveraging low-rank adaptations for efficient fine-tuning.<br><Br>

### Comparing Standard Fine-Tuning and LoRA

To understand the impact of LoRA in a practical setting, I fine-tuned the `roberta-base` model on the TweetEval Sentiment dataset using both standard fine-tuning and LoRA. For the standard fine-tuning, I updated all model parameters, resulting in approximately 124.6 million trainable parameters. The training took over 2700 seconds, and the test accuracy was 0.6955. This gave me a baseline for comparison. 

![image-20241025171302768](/images/2024-10-17-TIL24_Day92_DL/image-20241025171302768.png)



The code snippet for standard fine-tuning using Hugging Face's Transformers library:

``` python
from transformers import robertaForSequenceClassification, Trainer, TrainingArguments
from datasets import load_dataset

# Load dataset and model
dataset = load_dataset("tweet_eval", name="sentiment")
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=3)

# Training arguments
training_args = TrainingArguments(
		output_dir="./results",
    evaluation_strategy="epoch",
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    logging_dir="./logs",
    logging_steps=10,
    save_total_limit=2
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

# Train model
trainer.train()
```

Next, I implemented LoRA using the Hugging Face PEFT library. By applying LoRA, I reduced the trainable parameters as below.

![image-20241025171659035](/images/2024-10-17-TIL24_Day92_DL/image-20241025171659035.png)

The code snippet for fine-tuning with LoRA using PEFT:

```python
from peft import LoraConfig, get_peft_model, TaskType

# Configure LoRA using PEFT
lora_config = LoraConfig(
    r=8,  # Rank
    inference_mode=False,  # Training mode
    task_type=TaskType.SEQ_CLS  # Sequence classification
)

# Apply LoRA to the model
model = get_peft_model(model, lora_config)

# Training arguments (same as above)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

# Train model with LoRA
trainer.train()
```



I gained a critical insight from this comparison: LoRA can be <u>particularly beneficial in resource-constrained environments with limited memory and computational power</u>. It offers an effective way to fine-tune models without requiring the full computational load of standard fine-tuning.<br><Br>



### Exploring the Influence of Model Size

To investigate LoRA's capabilities more deeply, I conducted additional experiments with **TinyBERT**, a more compact model. Without LoRA, TinyBERT had approximately 14.35 million trainable parameters, but this number dropped to just 40k when LoRA was implemented. I will follow the detailed results below.

The code snippet for fine-tuning TinyBERT without LoRA:

```python
from transformers import BertForSequenceClassification

# Load TinyBERT model
model = BertForSequenceClassification.from_pretrained("huawei-noah/TinyBERT_General_4L_312D", num_labels=3)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"]
)

# Train model
trainer.train()
```

And the code snippet for fine-tuning TinyBERT with LoRA:

```python
# Configure LoRA for TinyBERT
lora_confif = LoraConfig(
		r=8,
		inference_mode=False,
		task_type=TaskType.SEQ_CLS
)

# Apply LoRA to TinyBERT model
model = get_peft_model(model, lora_config)

trainer = Trainer(
		model=model,
		args=training_args,
		train_dataset=dataset["train"],
		eval_dataset=dataset["validation"]
)

# Train model with LoRA
trainer.train()
```

![image-20241027010705780](/images/2024-10-17-TIL24_Day92_DL/image-20241027010705780.png)

The finding revealed that while LoRA works well for larger models—where cutting down trainable parameters makes a big difference—the performance trade-off can be more pronounced in smaller models. TinyBERT, already tuned for fewer parameters, appeared to be more affected by the reductions from LoRA, leading to a significant decline in performance. This insight was crucial, indicating that the advantages of LoRA might significantly vary based on the model's size and complexity during fine-tuning.<br><br>

#### **Key Insights and Reflections**

1. **Balancing Efficiency and Performance**: A critical insight was balancing efficiency and performance. LoRA significantly lowers computational needs for fine-tuning, which is helpful in resource-limited settings. However, this efficiency slightly decreases accuracy. Understanding this trade-off is vital when choosing a fine-tuning approach.
2. **Scalability Across Model Sizes**: LoRA's scalability across model sizes was a key takeaway. While effective for larger models like roberta-base, its impact on TinyBERT was complex, showing a significant accuracy drop. This suggests that LoRA may not suit models optimized for efficiency, highlighting the need to consider model architecture and size for parameter-efficient techniques in future projects.
3. **Practical Implications for Real-World Applications**: The experiments clarified LoRA's practical implications. In settings with limited computational resources and training time, LoRA is a viable alternative to traditional fine-tuning, making it suitable for deploying large models where efficiency is crucial. However, in cases where accuracy is critical, such as medical diagnostics, the minor performance drop may be unacceptable, making standard fine-tuning a better choice.<br><br><br>



## Using Pretrained-Model Embedding

I further examined the potential of pre-trained models to generate embeddings for downstream tasks, particularly Visual Question Answering (VQA). I employed two configurations to extract embeddings: **ResNet** combined with **SBERT** (sentence-BERT) and **CLIP** (Contrastive Language-Image Pretraining). This process was distilled into a classification problem utilizing the DAQUAR dataset. 

- **ResNet-50** is a deep convolutional neural network (CNN) pre-trained on the ImageNet dataset, which contains millions of labeled images across thousands of classes. Using a pre-trained model like ResNet-50, we benefit from transfer learning, where the model's ability to recognize and extract high-level image features can be leveraged for your specific task without retraining the entire model.
- **SBERT (Sentence-BERT)** is a variant of BERT (Bidirectional Encoder Representations from Transformers) designed to create sentence embeddings. SBERT is much faster than BERT when generating sentence embeddings, as it can produce dense, fixed-size representations of sentences or phrases that capture their semantic meaning.
- For tasks like Visual Question Answering, it is essential to have embeddings that understand the relationship between words in a sentence (e.g., a question). The particular version we used (`all-MiniLM-L6-v2`) is **a lightweight transformer** model optimized for performance with minimal loss in accuracy.



I used ResNet-50 to generate image embeddings in the initial setup. The sentence transformer `all-MiniLM-L6-v2` <u>produced sentence embeddings</u>. These image and text embeddings were merged and fed into a linear layer for classification. 

The model consists of a linear layer utilizing ReLU activation for dimensionality reduction, followed by concatenating the processed embeddings. This combined representation is then input into a linear classifier. I trained the model and assessed its performance on the test set.

<br>

Here are the code snippets for this part:

**1. Image Embedding Extraction using ResNet50:**

```python
from torchvision import models, transforms
import torch
from PIL import Image

# Load pretrained ResNet-50 wihtout the classification head
resnet = models.resnet50(pretraied=True)
resnet = torch.nn.Sequential(*list(resnet.childeren())[:-1])

# Image processing
transform = transforms.Compose{[
  transforms.Resize((224, 224)),
  transforms.ToTensor(),
  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=0.229, 0.224, 0.225)
]}

# Extract image embedding
image = Image.open("path/to/image.jpg")
image = transform(image).unsqueeze(0)
with torch.no_grad():
    image_embedding = resnet(image).flatten()
```

- The `mean` and `std` values provided (`mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]`) are commonly used normalization values for image data. These values represent the channel-wise mean and standard deviation for the RGB channels (Red, Green, Blue) of images from the ImageNet dataset, which is used extensively to train many popular deep learning models, including ResNet.
- **Embedding dimensionality (2048):** The 2048-dimension embedding comes <u>from the output of the last convolutional layer</u> (before the fully connected layer, which is used for classification in the original model). This represents a compressed but detailed summary of the high-level features of the image. Each of the 2048 values corresponds to a particular feature learned by the model (e.g., edges, textures, or patterns).8.<br><br><br>



**2. Text Embeddings Extraction Using SBERT**

```python
from sentence_transformers import SentenceTransformer

sbert = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
text = "This is a sample question."
text_embedding = sbert.encode(text)
```

- **Embedding dimensionality (384):** The output embedding dimension of 384 from SBERT is a compact representation of the sentence’s meaning. The <u>smaller dimensionality</u> (compared to other BERT variants, which typically have 768 dimensions) makes it more computationally efficient while still retaining enough semantic detail for downstream tasks like classification.<br><br><br>

**3. Combining Image and Text Embeddings**

We create a feature vector that blends visual and textual information by summing the embeddings. This combined embedding has a dimension of **2048 + 384**, totaling **2432**. However, this substantial embedding may include excessive information, which can lead to high computational costs for subsequent tasks, thus necessitating dimensionality reduction.

- **Dimensionality Reduction to 512**: To decrease the size of the merged embeddings, I implemented a linear layer to project the 2432-dimensional vector <u>down to a more manageable 512 dimensions</u>. This approach mitigates computational complexity and helps prevent overfitting, particularly when training a classification model. The linear layer employs **ReLU activation** to efficiently reduce dimensionality while retaining the most vital features from both the image and text.

After training the model with the combined embeddings, I found that the test accuracy reached 0.3595 after 20 epochs. It was fascinating to see how merging visual and textual features created a richer representation of the input, which led to reasonably good results.<br><br><br>

**4. Embedding Extraction Using CLIP**

I utilized the CLIP model in the second setup, incorporating a visual encoder (ViT-B/32) and a textual encoder. Both yielded embeddings of dimension 512. I noted that CLIP's joint training of visual and textual encoders delivered slightly improved performance compared to the ResNet + SBERT setup, achieving a final test accuracy of 0.3767. Below is the detailed code for extracting embeddings and training with CLIP.<br>

- **CLIP** (Contrastive Language–Image Pretraining) is designed to align visual and textual representations in the same space. It jointly trains a visual encoder and a textual encoder on a massive dataset of image-text pairs, making it well-suited for tasks like VQA, where both image and text information are critical.

![image-20241027133803173](/images/2024-10-17-TIL24_Day92_DL/image-20241027133803173.png)





```python
from transformers import CLIPProcessor, CLIPModel

clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# Extract image embedding
image = Image.open("path_of_the_image")
inputs = processor(images=image, return_tensors="pt")
with torch.no_grad():
  image_features = clip_model.get_image_features(**inputs)
  
# Extract text embedding
text = "This is a sample question."
inputs = processor(text=text, return_tenxors="pt")
with torch.no_grad():
  text_features = clip_model.get_text_features(**inputs)
```

The visual and textual embeddings produced by CLIP were both of dimension 512, resulting in a concatenated embedding of dimension. This more balanced representation provided better alignment between visual and textual information, likely contributing to higher accuracy.

- **CLIP?** CLIP models (in my experiment, `ViT-B/32`) are trained to match images and text representations, which makes them <u>inherently better at understanding the relationship between the two modalities.</u> By using both encoders simultaneously, CLIP generates embeddings that are more likely to be aligned meaningfully, improving performance for multimodal tasks like VQA.
- **Embedding dimensionality (512):** The visual and textual embeddings produced by CLIP are of dimension 512. 



**5. Comparison of Setups**

- **ResNet+SBERT**: This setup involves extracting features independently from both modalities (image and text) and concatenating them. Since these models were not trained together, the embeddings might not be perfectly aligned, which could lead to slightly lower performance. Nevertheless, combining a powerful CNN for image embeddings(ResNet-50) and a strong sentence embedding model (SBERT) still provides reasonably good results.
- **CLIP**: CLIP's joint training of visual and textual encoders ensures that the image and text embeddings are more naturally aligned, resulting in better performance on tasks like VQA, where the relationship between the image and the question is crucial. The fact that both embeddings have the same dimensionality (512) also makes it easier to combine them and train a downstream classifier without additional dimensionality reduction. 



In conclusion, CLIP outperforms the ResNEt + SBERT setup because it was designed to create more harmonious and aligned embeddings for text and images, directly benefiting tasks like VQA. Additionally, its balanced dimensionality(512) for both image and text embeddings makes the classification task more manageable than the larger combined dimensionality in the ResNet + SBERT setup. 

<br><br>
