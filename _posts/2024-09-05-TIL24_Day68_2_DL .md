---
title: "Day68 Deep Learning Review - CNN (2) & Transformer"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, TIL_24]
toc: true 
---

# -(editing)

<img src="/blog/images/2024-09-05-TIL24_Day68_DL/9C0DB8D1-E5F6-40F6-9C08-6C572349A4D4_1_105_c.jpeg">

<br><br>

In deep learning, a **transformer** is a model architecture primarily used in natural language processing (NLP). However, its applications have expanded into other domains like computer vision and time-series analysis. The transformer model was introduced in the paper "Attention is All You Need" by Vaswani et al. in 2017. It marked a significant departure from earlier models that relied heavily on recurrent neural networks (RNNs) or convolutional neural networks (CNNs) for processing sequences.

<br><br>

### Core Components and Mechanisms

**1. Attention Mechanism:**

   - The transformer uses a mechanism known as "self-attention" or "multi-head attention," which allows it to weigh the significance of different words in a sentence, regardless of their positional distance from each other. This capability helps the model to understand the context and the relationships between words more effectively.

**2. Multi-Head Attention:**
   - In this setup, the attention mechanism is run several times in parallel. The independent attention outputs (heads) are then combined and processed, which enables the model to capture different aspects of semantic relationships within the data at different positions simultaneously.

**3. Positional Encoding:**
   - Since the transformer does not inherently process sequential data in order (like RNNs), it uses positional encodings to inject information about the order of the sequence into the model. This allows the model to utilize the sequence order, which is crucial for understanding structured data like language.

**4. Encoder and Decoder:**
   - The typical transformer model comprises an encoder to process the input data and a decoder to produce the output. This architecture is particularly effective in tasks like machine translation.
   - Each encoder and decoder consists of a stack of identical layers that include multi-head attention and fully connected networks.

<br><br>

### Advantages over Previous Models

- **Parallelization:** Unlike RNNs that must process data sequentially, transformers process all sequence elements simultaneously. This allows for more efficient training on modern hardware (GPUs/TPUs).
- **Long-Range Dependencies:** Transformers can better handle long-range dependencies in data thanks to the self-attention mechanism, which directly computes interactions between all input pairs regardless of their positions in the sequence.
- **Flexibility and Scalability:** The architecture is highly flexible and scalable, making it suitable for various tasks beyond NLP, such as image recognition and playing strategic games.

<br><br>

### Applications

Transformers have become the backbone of modern NLP applications, leading to state-of-the-art models like BERT (Bidirectional Encoder Representations from Transformers), GPT (Generative Pretrained Transformer), and others that handle tasks like text classification, question answering, text generation, and more.

The model's success in NLP has also inspired adaptations in other fields, such as vision transformers (ViTs) for image classification tasks, demonstrating the broad impact and versatility of the transformer architecture in deep learning.
