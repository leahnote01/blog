---
title: "Day53 ML Review - Dimensionality Reduction (4)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, dimensionalityReduction, TIL_24]
toc: true 
---

# Implementing a Kernal Principal Component Analysis in Python

<img src="/blog/images/2024-08-15-TIL24_Day53/C2BA010A-676A-4D44-AC2E-B1E7ADEBE8BA_1_105_c.jpeg"><br><br>

We are going to implement an RBF KPCA in Python for example.

```python
from scipy.spatial.distance import pdist, squareform
from scipy import exp
from scipy.linalg import eigh
import numpy as np
def rbf_kernel_pca(X, gamma, n_components):
  """
  RBF kernel PCA implementation:
  X: {NumPy ndarray}, shape = [n_examples, n_features]
  gamma (float): Tuning parameter of the RBF kernel
  n_components (int): Number of principal components to return
  
  returns projected  dataset 
  X_pc: {NumPy ndarray}, shape = [n_examples, k_features]
  """
  
  # Calculate pairwise squared Euclidean distances
  # in the MxN dimensional dataset.
  sq_dists = pdist(X. 'sqeuclidean')
  
  # Convert pairwise distances into a square matrix
  mat_sq_dists = squareform(sq_dists)
  
  # Compute the symmetric kernel matrix.
  K = exp(-gamma * mat_sq_dists)
  
  # Center the kernel matrix.
  N = K.shape[0]
  one_n = np.ones((N,N)) / N
  K = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)
  
  # Obtaining eigenpairs from the centered kernel matrix
  # scipy.linalg.eigh returns them in ascending order
  eigvals, eigvecs = eigh(K)
  eigvals, eigvecs = eigenvals[::-1], eigvecs[:, ::-1]
  
  # Collect the top k eigenvectors (projected examples)
  X_pc = np.column_stack([eigvecs[:, i] for i in range(n_components)
  
  return X_pc  
```

<br>

### Example 1 - separating half-moon shapes

Let's use our `rbf_kernel_pca` on various nonlinear example datasets. First, we'll generate a 2D dataset with 100 sample points that depict two half-moon shapes.

```python
from sklearn.datasets import make_moons
X, y = make_moons(n_samples = 100, random_state=123)
```

<center>
  <img src="/blog/images/2024-08-15-TIL24_Day53/image-20240821184653849.png">
</center>



In the following diagram, you can see how the dataset transforms when using Standard PCA.

<center>
  <img src="/blog/images/2024-08-15-TIL24_Day53/image-20240821182954217.png" width="70%"><br>
</center>

The original half-moon shapes appear slightly sheared and flipped across the vertical center. This transformation would not assist a linear classifier in distinguishing between circles and triangles. Similarly, if we project the dataset onto a one-dimensional feature axis, the circles and triangles corresponding to the two half-moon shapes are not linearly separable, as shown in the right subplot.



Let's evaluate the performance of the `rbf_kernel_pca` method we implemented by following the steps below.

 ```python
 X_kcpa = rbf_kernel_pca(X, gamma=15, n_components=2)
 ```



The two categories (circles and triangles) are <u>distinctly separated linearly</u>, so we have an appropriate training dataset for linear classification.

<center>
  <img src="/blog/images/2024-08-15-TIL24_Day53/image-20240821185047922.png">
</center>





<br><br>

