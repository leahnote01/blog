---
title: "Day06 ML Review - Principle Component Analysis (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statReview, mlReview, TIL_24, PCA]
toc: true
---

# PCA Applications on Machine Learning & Further Explanations

<img src="/blog/images/2024-05-21-TIL24_Day6/JPEG image-78DE4E8ECAE9-1.jpeg" alt="day06in">

<br>

**All explanation is mainly from: (https://ddongwon.tistory.com/114)**

Let’s assume we have a dataset of 5 points representing weight and height. Here’s how Principal Component Analysis (PCA) can be applied: 

<br>

1) **Centering the Data**

First, calculate the **average (mean) values for each dimension**: weight (x-axis) and height (y-axis). Then, shift all data points to align these mean values with the origin. This involves subtracting the mean of each variable from their respective values, <u>resulting in a new dataset centered around the origin.</u> This step is crucial, as it normalizes the data by removing any inherent bias towards a particular scale or starting point.

![image-20240608104942727](/images/2024-05-21-TIL24_Day6/image-20240608104942727.png)

<Br>

The graph shows the original data points and their averages on each axis. After centering, the intersection of these average values becomes the graph's origin.

<br>

2) **Finding the Principal Component**

Once the data is centered, the next goal is <u>to draw a line through the origin (0,0)</u> and <u>find the direction along which the **data's variance is maximized</u>.** This is done by rotating a hypothetical line around the origin and calculating the sum of the squares of the perpendicular distances (projected points) from the data points to the line. 

![image-20240608111829448](/images/2024-05-21-TIL24_Day6/image-20240608111829448.png)

- Mathematical Objectives:
  In Principal Component Analysis (PCA), <u>the goal is to maximize the sum of squares of the distances, as these distances indicate the variance of the data when it's projected onto a line</u>. The direction where this sum of squared distances is maximized represents <u>the first principal component, PC1.</u>

<br>

3) **Identifying the First Principal Component (PC1)**

The line that maximizes the sum of squares becomes the first principal component. The vector that defines this line is the <u>"eigenvector" associated with the largest eigenvalue</u> since it accounts for the most variance among all possible line orientations.



![image-20240608113016317](/images/2024-05-21-TIL24_Day6/image-20240608113016317.png)

* **Loading Score**
  The Loading scores are the coordinates of the eigenvector (also referred to as the components of the eigenvector) and indicate the contribution of each original variable (weight and height in this case) to the principal component. In this example, a loading score ratio of 0.97 for x (weight) and 0.242 for y (height) indicates how much weight each variable has in the principal component. 

<br>

4) **Identifying the Second Principal Component (PC2)**

Once PC1 is established, the next step is to find a direction that is orthogonal (perpendicular) to PC1 to identify the second principal component (PC2). This step is crucial for ensuring the new features are uncorrelated, a key aspect of PCA. <br>

<u>In 2D Scenarios:</u>
*Only one line is orthogonal* to PC1 for two-dimensional data. This line is automatically the second principal component. 

<u>In Higher Dimensions:</u>
The set of directions orthogonal to PC1 forms a hyperplane for data with more than two dimensions. The next principal component (PC2) is the direction within this hyperplane that maximizes the variance of the data projected onto it, similar to how PC1 was determined. 

![image-20240608120706940](/images/2024-05-21-TIL24_Day6/image-20240608120706940.png)







<img src="/blog/images/2024-05-21-TIL24_Day6/300269DC-C52D-4DF7-8220-F804837186C3.jpeg" alt="day06out">
