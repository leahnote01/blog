---
title: "Day06 ML Review - Principle Component Analysis (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statReview, mlReview, TIL_24, PCA]
toc: true
---

# PCA Applications on Machine Learning & Further Explanations

<img src="/blog/images/2024-05-21-TIL24_Day6/JPEG image-78DE4E8ECAE9-1.jpeg" alt="day06in">

<br>

**All explanation is mainly from: (https://ddongwon.tistory.com/114)**

Let’s assume we have a dataset of 5 points representing weight and height. Here’s how Principal Component Analysis (PCA) can be applied: 



1) **Centering the Data**

First, calculate the **average (mean) values for each dimension**: weight (x-axis) and height (y-axis). Then, shift all data points to align these mean values with the origin. This involves subtracting the mean of each variable from their respective values, <u>resulting in a new dataset centered around the origin.</u> This step is crucial, as it normalizes the data by removing any inherent bias towards a particular scale or starting point.

![image-20240608104942727](/images/2024-05-21-TIL24_Day6/image-20240608104942727.png)

<Br>

The graph shows the original data points and their averages on each axis. After centering, the intersection of these average values becomes the graph's origin.



2. **Finding the Principal Component**

Once the data is centered, the next goal is to draw a line through the origin (0,0) and find the direction along which the variance of the data is maximized. 















<img src="/blog/images/2024-05-21-TIL24_Day6/300269DC-C52D-4DF7-8220-F804837186C3.jpeg" alt="day06out">
