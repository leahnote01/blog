---
title: "Day96 Deep Learning Lecture Review - Lecture 15"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, transfer learning, NLP models, attention, TIL_24]
toc: true 
---

# Model Comparison and Bias Mitigation; McNemar’s Test, 

![55B765BC-34BF-466C-A3B8-AAE1938C65DA_1_105_c](/images/2024-11-01-TIL24_Day96_DL/55B765BC-34BF-466C-A3B8-AAE1938C65DA_1_105_c.jpeg)



## Model Comparison

The lecture highlights that merely evaluating models based on overall accuracy or error metrics tends to overlook important details, especially for deep learning systems used in various contexts.



- How Do We Tell if a Model is Better?
  - Machine learning papers often report the result of a single run, but this approach is unreliable. Deep learning models exhibit variance due to random initializations.
  - Different random seeds can lead to other models, affecting their overall performance.
- Point Estimates Aren't Sufficient.
  - <u>Metrics like mean accuracy provide only a limited view.</u>
  - Carefully well-designed experiments are needed.
  - Must identify relevant sub-groups.
  - Analysis methods for comparing models:
    - Confidence Intervals
    - Hypothesis testing

<br>

- Confidence Interval

  - Confidence Intervals are recommended to add <u>bounds to these estimate</u>s. If confidence **intervals between models do not overlap, there is a statistically significant difference.**

  - A smaller confidence value indicates a more precise estimate.

    

    

    ![image-20241127123507682](/images/2024-11-01-TIL24_Day96_DL/image-20241127123507682.png)

  ![image-20241127123710045](/images/2024-11-01-TIL24_Day96_DL/image-20241127123710045.png)

  - If the confidence intervals **do not overlap**, they are significantly different, and the one with the <u>best accuracy could be chosen.</u> 
  - If they overlap, the results are not necessarily conclusive.

  - What if the distribution is <u>non-Gaussian</u>?

    - We could use <u>non-parametric methods</u> instead, e.g., a bootstrap estimator.

    - Confidence intervals do not take paired data into account, but often, we evaluate two models on exactly the same data, so a **paired test** makes sense.<br>

      <br>



- **McNemar’s Test**

  - Often, we want to compare two models using <u>paired data,</u> i.e., the same dataset.
    - We want to compare the behavior of two models in each example.
      - Image 1: Model 1 vs Model 2
      - Image 2: Model 1 vs Model 2 ... 
  - McNemar’s Test is a statistical test for **paired nominal data.**
  - It is most commonly used for **binary** classifiers, but it can also be used for multi-class.
    - McNemar’s Test tells you that <u>the one with the lower error is probably better</u> if two classifiers are not equal.s
  - We need to construct a 2x2 contingency table for each model.

  ![image-20241127130144280](/images/2024-11-01-TIL24_Day96_DL/image-20241127130144280.png)

  - Count how many times the models were:
    - Both models were correct.
    - Model 1 was correct, but Model 2 was incorrect.
    - Model 2 was correct, but Model 1 was incorrect.
    - Both models were wrong.
  - Set significance level of alpha, typically set to 0.05 (95%)
  -  $n = B+C$ , and  $p = 2 \sum^n_{i=b} \binom n i 0.5^i (1-0.5)^{n-i} $ <br><br>



- Example : Two Scenarios
  - For Scenario A:
    - We get a p-value of 0.006, which is less than an alpha of 0.05, so we can reject the null hypothesis that both perform equally well.
    - So in Scenario A, model 2 is better.
  - For Scenario B:
    - We get a p-value of 0.155, which is greater than an alpha of 0.005, so we cannot reject are null hypothesis and assume there is no significant difference.
    - So in Scenario B, models perform equally well.
- Various statistical tests are available. *Relying solely on whether the population statistic (mean accuracy) is greater can mislead model selection.*
- Many AI researchers rely solely on point estimates to compare models, and neglecting statistical analysis and thoughtful design constitutes poor scientific practice.

<br><br>

## Bias in Machine Learning

- In the real world, the datasets often matter a lot more than the AI algorithms.
- 



<br><br>







