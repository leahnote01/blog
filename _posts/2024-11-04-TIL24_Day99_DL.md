---
title: "Day99 Deep Learning Lecture Review - Lecture 16 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Uncertainty in Deep Learning, Distribution Shifts, Model Calibration, and Out-Of-Distribution (OOD) Detection

<img src="/blog/images/2024-11-04-TIL24_Day99_DL/19512157-FA95-4D8E-80C2-79A90503984F_1_105_c.jpeg"><br><Br>

### **1. Types of Uncertainty**

- **Aleatoric** Uncertainty (stochastic uncertainty)
  - Caused by random noise inherent in data or experiments.
  - Present in observations and needs to improve with more data.
  - Where model predictions **have high error.** 

- **Epistemic** Uncertainty
  - Due to the need for more knowledge or training data.
  - This can decrease as the model encounters more diverse data.
  - Where we **lack of data to feed through the model.**
- Both forms of uncertainty are present in most deep-learning models. 
- Users will use a system on data that may differ from the training data.
  - We have epistemic uncertainty in areas where we lack data


- We want models to be:
  - <u>Robust to distribution shift.</u>
  - <u>Aware of when it happens using uncertainty estimation methods.</u>



- Types of Uncertainty in Deep Learning
  - Robustness to distribution shift
  - Classifier threshold selection
  - Model calibration
  - Detecting out-of-distribution inputs
  - Conformal prediction - an uncertainty-aware framework<br><br>



### 2. Robustness to Distribution Shift

- Making Models Robust to Distribution Shift

  - There are many different methods, but most need to work better.
  - Successful approaches in vision:
    - (Extreme) data augmentation
    - **Conditional generative models** take **input** from a source distribution and **modify** it to resemble the target distribution.
    - We need a lot of data for this.

- Training & Evaluation Strategy

  - We often have abundant data from distribution A, but **very little from distributions B, C, D,** and others.
  - How can we determine if modifications in the model or training process (augmentations) <u>enhanced the system's robustness to distribution shifts?</u>
  - General Recommendations
    - Do not train on A mixed with minority data B. This will improve the performance on B, but we won't know for the others!
    - **Reserve minority data for performance evaluation and picking an operating threshold for the model.**
    - **Operating threshold**: The prediction score exceeds the threshold to establish the category.
  - Example
    - A significant distribution shift will occur if all training data comes from a single hospital.

- **Picking a Threshold**

  - <u>Most cases require more than AUC and probability scores</u> for production systems.

    - We need to choose **an operating threshold.**
    - We want to choose a threshold that is robust <u>across all distributions.</u>

  - AUC is the area under the curve, so we don't have to choose a threshold.

    - But this is not a universal threshold.<Br><Br>

    <i>Image Source: [Medium- ROC Curve and AUC: Evaluating Model Performance by ilyurek K](https://medium.com/@ilyurek/roc-curve-and-auc-evaluating-model-performance-c2178008b02)</i>

    <center>
      <img src="/blog/images/2024-11-04-TIL24_Day99_DL/image-20241218135536711.png" width="75%"><br><br>
    </center>

    

  - Sweep a threshold across all evaluation sets from different distributions **to compute an evaluation metric,** such as accuracy or F1-score.

  - Choose the threshold that <u>achieves desirable performance across all distributions.</u>

- **Operating Threshold**

  - We need a universal threshold that works best <u>for all distributions.</u>
    - May only draw curve if minimum criteria are met,
      - e.g., sufficiently high recall and low false positive rate for product needs.
    - Average across all curves and choose threshold corresponding to the maximum point.<br><Br><br>



### 3. Model Calibration

> If a model is calibrated, then t<u>he estimated class probabilities are consistent</u> with what should naturally occur. So, if a logistic regression classifier iss calibrated then a score of 0.7 would mean <u><i>it is the positive class 70% of the time.</i></u> For binary classification, we can use a model calibration plot to inspect.



- Most Networks are poorly calibrated
  - Good Calibration: If **a sample has a sftmax score of 0.8** then it says it has an 80% chance of being the target class
  - In practice, these "probabilities" are not well calibrated when we train our networks
    - Interpreting the output of a softmax as a set of probabilities is not a good idea unless you know the model is calibrated.

<center>
  <img src="/blog/images/2024-11-04-TIL24_Day99_DL/image-20241219191041062.png" width="70%"><br><Br>
</center>



<br>

#### 1. Platt Scaling

- Key Essentials:
  - **Model calibration for binary prediction**
  - Done in a <u>post-hoc manne</u>r. 
  - <u>Train a binary classifier first that outputs a score</u>, e.g., a non-calibrated probability.
  - Platt Scaling requires <u>a calibration set.</u>
- 





â€‹	















