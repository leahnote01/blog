---
title: "Day99 Deep Learning Lecture Review - Lecture 16 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Uncertainty in Deep Learning, Distribution Shifts, Model Calibration, and Out-Of-Distribution (OOD) Detection

<img src="/blog/images/2024-11-04-TIL24_Day99_DL/19512157-FA95-4D8E-80C2-79A90503984F_1_105_c.jpeg"><br><Br>

### **1. Types of Uncertainty**

- **Aleatoric** Uncertainty (stochastic uncertainty)
  - Caused by random noise inherent in data or experiments.
  - Present in observations and needs to improve with more data.
  - Where model predictions **have high error.** 

- **Epistemic** Uncertainty
  - Due to the need for more knowledge or training data.
  - This can decrease as the model encounters more diverse data.
  - Where we **lack of data to feed through the model.**
- Both forms of uncertainty are present in most deep-learning models. 
- Users will use a system on data that may differ from the training data.
  - We have epistemic uncertainty in areas where we lack data


- We want models to be:
  - <u>Robust to distribution shift.</u>
  - <u>Aware of when it happens using uncertainty estimation methods.</u>



- Types of Uncertainty in Deep Learning
  - Robustness to distribution shift
  - Classifier threshold selection
  - Model calibration
  - Detecting out-of-distribution inputs
  - Conformal prediction - an uncertainty-aware framework<br><br>



### 2. Robustness to Distribution Shift

- Making Models Robust to Distribution Shift

  - There are many different methods, but most need to work better.
  - Successful approaches in vision:
    - (Extreme) data augmentation
    - **Conditional generative models** take **input** from a source distribution and **modify** it to resemble the target distribution.
    - We need a lot of data for this.

- Training & Evaluation Strategy

  - We often have abundant data from distribution A, but **very little from distributions B, C, D,** and others.
  - How can we determine if modifications in the model or training process (augmentations) <u>enhanced the system's robustness to distribution shifts?</u>
  - General Recommendations
    - Do not train on A mixed with minority data B. This will improve the performance on B, but we won't know for the others!
    - **Reserve minority data for performance evaluation and picking an operating threshold for the model.**
    - **Operating threshold**: The prediction score exceeds the threshold to establish the category.
  - Example
    - A significant distribution shift will occur if all training data comes from a single hospital.

- **Picking a Threshold**

  - <u>Most cases require more than AUC and probability scores</u> for production systems.

    - We need to choose **an operating threshold.**
    - We want to choose a threshold that is robust <u>across all distributions.</u>

  - AUC is the area under the curve, so we don't have to choose a threshold.

    - But this is not a universal threshold.<Br><Br>

    <i>Image Source: [Medium- ROC Curve and AUC: Evaluating Model Performance by ilyurek K](https://medium.com/@ilyurek/roc-curve-and-auc-evaluating-model-performance-c2178008b02)</i>

    <center>
      <img src="/blog/images/2024-11-04-TIL24_Day99_DL/image-20241218135536711.png" width="75%"><br><br>
    </center>

    

  - Sweep a threshold across all evaluation sets from different distributions **to compute an evaluation metric,** such as accuracy or F1-score.

  - Choose the threshold that <u>achieves desirable performance across all distributions.</u>

- **Operating Threshold**

  - We need a universal threshold that works best <u>for all distributions.</u>
    - May only draw curve if minimum criteria are met,
      - e.g., sufficiently high recall and low false positive rate for product needs.
    - Average across all curves and choose threshold corresponding to the maximum point.<br><Br><br>



### 3. Model Calibration

> If a model is calibrated, then t<u>he estimated class probabilities are consistent</u> with what should naturally occur. So, if a logistic regression classifier iss calibrated then a score of 0.7 would mean <u><i>it is the positive class 70% of the time.</i></u> For binary classification, we can use a model calibration plot to inspect.



- Most Networks are poorly calibrated
  - Good Calibration: If **a sample has a sftmax score of 0.8** then it says it has an 80% chance of being the target class
  - In practice, these "probabilities" are not well calibrated when we train our networks
    - Interpreting the output of a softmax as a set of probabilities is not a good idea unless you know the model is calibrated.

<center>
  <img src="/blog/images/2024-11-04-TIL24_Day99_DL/image-20241219191041062.png" width="70%"><br><Br>
</center>



<br>

#### 1. Platt Scaling

> In machine learning, Platt scaling or Platt calibration <u>is a way of transforming the outputs of a classification model into a probability distribution over classes</u>. 

- Key Essentials:

  - **Model calibration for binary prediction**
  - Done in a <u>post-hoc manne</u>r. 
  - <u>Train a binary classifier first that outputs a score</u>, e.g., a non-calibrated probability.
  - Platt Scaling requires <u>a calibration set.</u>

  <center>
    <img src="/blog/images/2024-11-04-TIL24_Day99_DL/image-20241219192915586.png" width="80%"><br><br>
  </center>

  

<I>Explanation Source: [Wikipedia: Platt Scaling](https://en.wikipedia.org/wiki/Platt_scaling)</I>

- Platt scaling is an algorithm to solve the aforementioned problem. It produces probability estimates. 

  <center>
    $P(y=1 \vert x) = \frac{1}{1+\text{exp}(A(f(x))+B)}$ <Br><Br>
    $t_+ = \frac{N_+ + 1}{N_{-}+2}$ for positive samples ($y=1$), and <br>
  $t_- = \frac{1}{N_{-}+2}$ for negative samples ($y=-1$)
    </center>

    

  - Using trained model, <u>run it on the calibration set</u> not used for training to get the $f(x)$ values. 
  - Learn two scalars ($A$ and $B$) by <u>fitting a univariate logistic regression model</u> to these scores, where targets are probabilities (rather than 1 or 0).
  - The parameters $A$ and $B$ are estimated using a **maximum likelihood** method that optimizes on the same training set as that for the original classifier $f$. 
  - To avoid overfitting to this set, a heldout **calibration set** or **cross-validation** can be used. 
  - This transformation follows by applying Bayes' rule to a model of out-of-sample data that has a uniform prior over the labels.
  - The **constants 1 and 2**, on the numerator and denominator respectively, are derived from the application of Laplace smoothing.

- Platt scaling has been shown **to be effective for SVMs** as well as other types of classification models, including boosted models and even naive Bayes classifiers, which produce distorted probability distributions

- It is particulary effective for <u>max-margin methods</u> such as **SVMs** and **boosted trees**, which show sigmoidal distortions in their predicted probabilities, but has less of an effect with well-calibrated models such as <u>logistic regression, multilayer perceptrons, and random forests.</u> 



<center>
  
</center>







â€‹	















