---
title: "Day70 Deep Learning Lecture Review - Lecture 5 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, transformer, foundationModels, TIL_24]
toc: true 
---

# Transformers and Foundation Models: GELU, Layer Norm, Key Concepts & Workflow

<img src="/blog/images/2024-09-07-TIL24_Day70_DL/82AAB9F0-91BF-4C9E-9792-610E1A8F76DD_1_105_c.jpeg"><br><br>

> Foundation models are large-scale, pre-trained models used as the basis for a wide range of tasks in machine learning, especially in <u>Natural Language Processing (NLP), Computer Vision (CV), and other areas.</u> They are trained on diverse, extensive datasets using unsupervised or self-supervised learning techniques. These models can be fine-tuned for specific tasks using smaller datasets. They are <u>huge, often containing billions of parameters, and can be applied to many different functions</u> without being re-trained completely.

Foundation models are <u>pre-trained</u> in a <u>self-supervised</u> manner, which means they learn representations without requiring labeled data. Following pre-training, they can be fine-tuned on a smaller, task-specific dataset. They mark a shift toward **building generalized AI systems** that can solve diverse problems without task-specific retraining. However, their development and deployment come with challenges, such as resource-intensive training, ethical considerations, and biases inherent in the training data.

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912180607808.png" width="90%"><br><br>
</center>




<br>

## GELU Activation Function

> **The GELU (Gaussian Error Linear Unit)** activation function is a smooth, non-linear function primarily used in neural networks. It is often used in models like BERT (Bidirectional Encoder Representations from Transformers) and other transformer-based architectures because of its <u>superior performance</u> compared to traditional activation functions such as ReLU (Rectified Linear Unit).

- Used in most transformer models, and now in many CNN and MLP architectures.
- Doesn't have a dying ReLU problem. <u>Smoother</u> Activation near zero, Probabilistic Behavior, Differentiable in all ranges, and <u>allows (small) gradient in the negative range.</u>
  - Unlike ReLU, which is piecewise linear and has discontinuities, GELU is a smooth and differentiable function, which is beneficial for optimization in deep learning models.
  - GELU’s smoother behavior allows gradients to flow more easily during backpropagation, especially <u>for values near zero.</u> 
  - This reduces the chances of the model facing **"dead neurons" that can happen in ReLU (where negative values are zeroed out entirely).**<br>

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/1BF01C0A-3B38-4853-8C27-64CF9C44A6BB_1_201_a.jpeg" width="70%"><br><I><Font size="3pt">
  Image from: Kanan, C., "End-to-End Deep Learning," CSC477.01.FALL2024ASE Lecture Slides, University of Rochester, 2024.
  </Font></I><br><br><Br>
</center>




<br>

## Normalization Functions

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912182855444.png" width="90%"><br><br>
</center>



- Layer Norm
  -  Normalizes across all features in a given layer. It applies normalization across the **channels (C)** and **merged spatial dimensions (H, W)**, for each individual sample in the mini-batch.
  - It works well <u>for non-sequential data</u> and has applications <u>in transformer architectures,</u> where mini-batch normalization isn't as effective.
- Batch Normalization
  - Normalizes over the **mini-batch samples (N)** and the **spatial dimensions (H, W)**, *but each channel (C) is normalized independently.*
  - This normalization is <u>applied per channel for a mini-batch.</u> It is commonly used in CNNs <u>to reduce internal covariate shift,</u> speeding up training and allowing for higher learning rates.
- Group Normalization
  - <u>Splits channels into groups</u> and normalizes across each group and spatial dimensions.

- Instance Normalization: Normalizes for each sample and channel separately.
- Convolutional Weight: Shows how convolution kernels work across input and output channels.

<br>

- Layer Norm vs. Batch Norm
  - Layer Norm: Subtract <u>mean of each input vector</u> and divide by the **vector’s** standard deviation
  - Batch Norm: Subtract <u>channel mean</u> computed using all samples in batch, and then divide by <u>channel standard deviation computed using all samples in batch.</u>
  - Layer norm is invariant to batch size.
  - Layer norm tends to make training slower

<br>

### Batch Norm is a Frquent Problem.

- Batch normalization (Batch Norm) has been a popular technique in deep learning, primarily used to speed up training and stabilize neural network models. However, it also comes with some limitations and challenges

  1. Dependency on **Batch Size**: Batch Norm's performance is highly dependent on the batch size used during training. If the batch size is too small, the statistical estimates of the mean and variance may be inaccurate, leading to noisy gradients. It calculates the mean and variance of features <u>across the mini-batch</u>, and smaller batch sizes mean **less accurate estimates of these statistics, potentially hurting model performance.**

  2. Training vs. **Inference Discrepancy**: During training, batch normalization uses the statistics of the current mini-batch, but during inference (or testing), it uses <u>running estimates (moving averages) of the batch statistics</u>. These estimates might not perfectly match the real statistics at inference time. The difference between training and inference statistics can lead to a performance drop at inference time, particularly when the model is sensitive to the exact distribution of features.

  3. Not Effective **for Small Batches or Online Learning**: Batch normalization <u>struggles in settings where small batch sizes are required</u> (e.g., very large models, memory-constrained environments) or in online learning (streaming data) scenarios. When using small batch sizes, the statistics are often noisy and unstable, which reduces the effectiveness of normalization. In online learning, <u>since the data comes in a stream and mini-batch statistics are not available, applying Batch Norm becomes impractical.</u>

  4. Batch Norm Doesn't Work Well with **Recurrent Neural Networks (RNNs)**: Batch normalization is less effective in Recurrent Neural Networks (RNNs) and other sequential models. In RNNs, <u>the internal state is passed over many time steps, and normalizing these sequences using batch statistics can interfere with the temporal dependencies in the data.</u> Other normalization techniques, such as **Layer Normalization**, are typically preferred in RNNs and transformers because they work better in sequential models.

<br>

### Side Note: Layer Norm

>  **Layer Normalization (LayerNorm)** is a normalization technique used in neural networks, particularly effective <u>for models where the input is sequential,</u> such as Recurrent Neural Networks (RNNs) and Transformers. Unlike Batch Normalization (BatchNorm), which normalizes across the batch dimension, LayerNorm normalizes <u>across the features within each layer for each individual input, making it independent of the batch size.</u>

For each input instance (or sample), LayerNorm normalizes the activations across all features (neurons) in a laye<u>r by calculating the mean and variance for the entire layer’s output</u>, rather than across a mini-batch. This ensures that the activations have a **consistent distribution for each input**, which **helps stabilize and speed up the training process.**



<br><br>



## Transformers

> The **Transformer** is a deep learning model architecture introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017. It revolutionized the field of Natural Language Processing (NLP) and is now the backbone of many state-of-the-art models, *including BERT, GPT, and T5.* The Transformer is unique because it is based entirely on attention mechanisms, **without relying on the recurrence** (as in Recurrent Neural Networks, RNNs) or convolutions (as in Convolutional Neural Networks, CNNs). This allows it to handle longer-range dependencies and parallelize training more efficiently.



The conventional seq2seq model was composed of an encoder-decoder structure. <u>Here, the encoder compresses the input sequence into a single vector representation, and the decoder generates the output sequence based on this vector representation.</u> However, this structure has the downside of **losing some information** from the input sequence while compressing it into a single vector, and attention mechanisms were used to address this. But what if we create an encoder and decoder using only attention rather than using it as a correction for RNN?



### Key Components of the Transformer:

1. **Self-Attention Mechanism**
   - The Transformer's most critical innovation part is its self-attention mechanism, which <u>enables the model to attend all words in a sequence simultaneously</u>, rather than processing them sequnetially as in RNNs.
   - Self-attention computes <u>a weighted represetnation of the entire input sequence</u>, where the <u>importance of each word in the sequence is calculated relative to all other words.</u> This helps the model understand the **context** of each word by considering its relationships with every other word in the sequence.  <br><br>
2. **Positional Encoding**
   - Unlike RNNs, which have an inherent sense of order due to their sequential nature, the Transformer processes <u>input in parallel</u>. This means the model lacks a natural undersatnding of word order or positional information. 
   - To encode this postional information, the Transformer adds **postional encodings** to the input embeddings, allowing the model to capture the order of words in the sequence. <br><br>
3. **Encoder-Decoder Architecture**
   - The original Transformer architecture consists of two main components.
     - **Encoder**: <u>A stack of identical layers</u> where each layer <u>applies self-attention followed by a feed-foward network.</u> The encoder's job is to take in the input sequence and learn rich representations of it.
     - **Decoder**: Also a stack of identical layers, but in addition to self-attention, it uses cross-attention to attend to the encoder's output. <bR><Br>

   
   ​     <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240912192215557.png" width="90%"><br>
     <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240912192345319.png" width="90%"><br><br>
   ​     </center>
   
   
   
4. **Multi-Head Attention**:

   - The Transformer uses multi-head attention, which is a mechanism that <u>applies several self-attention operations in parallel.</u> Each attention "head" focuses on different parts of the input sequence. The results are then concatenated and projected to get the final output.
   - This allows the model to learn different aspects of relationships between words (e.g., syntactic and semantic relations) from multiple perspectives.<br><Br>

5. **Feed-Forward Network (FFN)**

   - After applying self-attention, each encoder and decoder block has a feed-forward network, <u>which is applied to each position independently.</u> The FFN typically concists of two linear layers with a ReLU activation in between allowing for non-linear transformations of the attention outputs.<br><br>

6. **Layer Normalization and Residual Connections**

   - Each sub-layer (like self-attention and feed-forward networks) in the Transformer is followed by <u>layer normalization and a residual connection.</u> This helps with training stability and allows better gradient flow, which speeds up convergence.

<br><br>

### Workflow of the Transformer

The Transformer consists of two algorithms- an encoder and a decoder. 


<center>
       <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240912192241945.png" width="90%"><br><br>
</center>

The Transformer does not use RNNs, but it retains the encoder-decoder structure that, like traditional seq2seq models, receives an input sequence in the encoder and outputs an output sequence in the decoder. In the previous seq2seq structure, each encoder and decoder consisted of a single RNN with t time steps; however, in the transformer, <u>the encoder and decoder units are composed of N components.</u> The paper proposes the Transformer used a total of 6 for both the encoder and decoder.

<center>
    <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240912192120193.png" width="90%"><br><br>
</center>


<font size="3pt"><I>Explanation from: <a href="https://wikidocs.net/31379">Wiki Docs in Korean</a></I></font>

The image below shows the transformer structure <u>where the decoder generates output results based on information received from the encoder.</u> The decoder operates similarly to the traditional seq2seq structure, taking the start symbol \<SOS\> as input and continuing its computations until the end symbol is produced \<EOS\>. This demonstrates that although RNNs are not used, the encoder-decoder structure is still maintained. 

Let's start by gradually exploring the internal structure of the transformer. Before diving into the encoder and decoder structure, we'll first comprehend the transformer's input. <u>The encoder and decoder</u> of the transformer do not simply receive the embedding vectors of each word. Instead, they <u>take adjusted values from the embedding vectors,</u> which we will explore by expanding on the input section. 

1. **Input Embedding and Positional Encoding**

   Before understanding the internals of a transformer, let's first take a look at the inputs of a transformer. 

   - The reason RNNs were helpful in natural language processing was due <u>to the characteristic of RNNs</u> that allows them to sequentially process input words based on their positions, which enables them to retain positional information. 
   - However, since transformers do not receive word inputs sequentially, it is necessary to communicate positional information differently. **To obtain positional information, transformers add positional encodings to the embedding vectors of each word and use them as input to the model, referred to as positional encoding.**

   - <u>Each word in the input sequence is converted into an embedding (vector representation)</u>. Positional encodings are added to these embeddings to provide information about the position of each word in the sequence.<br><br>

2. **Encoder**

   - <u>The input embeddings pass through the stack of encoder layers,</u> which apply self-attention and <u>feed-forward networks to transform the input into contextualized representations.</u>

3. **Decoder**

   - In tasks like machine translation, the decoder receives the target sequence (e.g., the translation so far). It uses self-attention and cross-attention (To attend to the encoder's output) <u>to generate the next word in the output sequence.</u> <br><br>

   <center>
     <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240913191710540.png" width="90%">
     <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240913191736741.png" width="90%">
   </center>
   
   
   
   The positional encoding values are added to the embedding vectors before they are used as input to the transformer, as shown in the illustrations above. The transformer employs two functions to assign the positional encoding values to the vectors.
   
   <center>
     $PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}})$ <br><Br>
     $PE_{(pos, 2i+1)} = cos(pos/10000^{2i/d_{model}})$ <br><br><br>
   </center>
   
   <u>The transformer adds the values of the sine and cosine functions to the embedding vectors,</u> thereby incorporating the order information of the words. Additionally, it is important to understand that the addition of the embedding vectors and positional encodings is actually performed <u>through the addition operation between the sentence matrix created by the embedding vectors and the positional encoding matrix.</u>
   
   
   
   <center>
     <img src="/blog/images/2024-09-07-TIL24_Day70_DL/image-20240913200614477.png">
   </center>
   
   
   
   The $pos$ indicates <u>the position of the embedding vector in the input sentence</u>, while $i$ signifies the <u>index of the dimension within the embedding vector</u>. 
   
   According to the formula above, if the index of each dimension within the embedding vector is even, the sine function value is used; if it is odd, the cosine function value is used. Note that when $(pos, 2i)$ is involved, the sine function is utilized, while for $(pos, 2i+1)$, the cosine function is applied.
   
   Additionally, in the formula above, $d_{model}$ refers to the output dimension of all layers in the transformer, which is a hyperparameter of the transformer.  The embedding vector also has the dimension of $d_{model}$; in the above diagram.
   
   **Using the positional encoding method as described, the order information is preserved. For example, when the positional encoding values are added to each embedding vector, even if they are the same word, the values of the embedding vectors entering the transformer as inputs will differ based on their position in the sentence. Consequently, the input to the transformer will consist of embedding vectors that consider order information.**<br><br>
   
   <B><span style="background-color: #CC8899; color: white">*We will further explore its structure in the next post.*</span></B>
   
   
   
4. Output
   - The final output of the decoder passes through a linear layer followed by <u>a softmax to produce the probability distribution</u> over the target vocabulary for the next word in the sequence. 

<Br><Br><Br>
