---
title: "Day70 Deep Learning Lecture Review - Lecture 5 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, transformer, foundationModels, TIL_24]
toc: true 
---

# Lecture 5: Transformers and Foundation Models

<img src="/blog/images/2024-09-07-TIL24_Day70_DL/82AAB9F0-91BF-4C9E-9792-610E1A8F76DD_1_105_c.jpeg"><br><br>

> Foundation models are large-scale, pre-trained models used as the basis for a wide range of tasks in machine learning, especially in <u>Natural Language Processing (NLP), Computer Vision (CV), and other areas.</u> They are trained on diverse, extensive datasets, usually using unsupervised or self-supervised learning techniques. These models can be fine-tuned for specific tasks using smaller datasets. They are <u>huge, often containing billions of parameters, and can be applied to many different functions</u> without being re-trained completely.

Foundation models are <u>pre-trained</u> in a <u>self-supervised</u> manner, which means they learn representations without requiring labeled data. Following pre-training, they can be fine-tuned on a smaller, task-specific dataset. They mark a shift toward **building generalized AI systems** that can solve diverse problems without task-specific retraining. However, their development and deployment come with challenges, such as resource-intensive training, ethical considerations, and biases inherent in the training data.

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912180607808.png" width="90%"><br><br>
</center>




<br>

## GELU Activation Function

> **The GELU (Gaussian Error Linear Unit)** activation function is a smooth, non-linear function primarily used in neural networks. It is often used in models like BERT (Bidirectional Encoder Representations from Transformers) and other transformer-based architectures because of its <u>superior performance</u> compared to traditional activation functions such as ReLU (Rectified Linear Unit).

- Used in most transformer models, and now in many CNN and MLP architectures.
- Doesn't have a dying ReLU problem. <u>Smoother</u> Activation near zero, Probabilistic Behavior, Differentiable in all ranges, and <u>allows (small) gradient in the negative range.</u>
  - Unlike ReLU, which is piecewise linear and has discontinuities, GELU is a smooth and differentiable function, which is beneficial for optimization in deep learning models.
  - GELU’s smoother behavior allows gradients to flow more easily during backpropagation, especially <u>for values near zero.</u> 
  - This reduces the chances of the model facing **"dead neurons" that can happen in ReLU (where negative values are zeroed out entirely).**<br>

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/1BF01C0A-3B38-4853-8C27-64CF9C44A6BB_1_201_a.jpeg" width="70%"><br><I><Font size="3pt">
  Image from: Kanan, C., "End-to-End Deep Learning," CSC477.01.FALL2024ASE Lecture Slides, University of Rochester, 2024.
  </Font></I><br><br><Br>
</center>




<br>

## Normalization Functions

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912182855444.png" width="90%"><br><br>
</center>



- Layer Norm
  -  Normalizes across all features in a given layer. It applies normalization across the **channels (C)** and **merged spatial dimensions (H, W)**, for each individual sample in the mini-batch.
  - It works well <u>for non-sequential data</u> and has applications <u>in transformer architectures,</u> where mini-batch normalization isn't as effective.
- Batch Normalization
  - Normalizes over the **mini-batch samples (N)** and the **spatial dimensions (H, W)**, *but each channel (C) is normalized independently.*
  - This normalization is <u>applied per channel for a mini-batch.</u> It is commonly used in CNNs <u>to reduce internal covariate shift,</u> speeding up training and allowing for higher learning rates.
- Group Normalization
  - <u>Splits channels into groups</u> and normalizes across each group and spatial dimensions.

- Instance Normalization: Normalizes for each sample and channel separately.
- Convolutional Weight: Shows how convolution kernels work across input and output channels.

<br>

- Layer Norm vs. Batch Norm
  - Layer Norm: Subtract <u>mean of each input vector</u> and divide by the **vector’s** standard deviation
  - Batch Norm: Subtract <u>channel mean</u> computed using all samples in batch, and then divide by <u>channel standard deviation computed using all samples in batch.</u>
  - Layer norm is invariant to batch size.
  - Layer norm tends to make training slower

<br>

### Batch Norm is a Frquent Problem.

- Batch normalization (Batch Norm) has been a popular technique in deep learning, primarily used to speed up training and stabilize neural network models. However, it also comes with some limitations and challenges

  1. Dependency on **Batch Size**: Batch Norm's performance is highly dependent on the batch size used during training. If the batch size is too small, the statistical estimates of the mean and variance may be inaccurate, leading to noisy gradients. It calculates the mean and variance of features <u>across the mini-batch</u>, and smaller batch sizes mean **less accurate estimates of these statistics, potentially hurting model performance.**

  2. Training vs. **Inference Discrepancy**: During training, batch normalization uses the statistics of the current mini-batch, but during inference (or testing), it uses <u>running estimates (moving averages) of the batch statistics</u>. These estimates might not perfectly match the real statistics at inference time. The difference between training and inference statistics can lead to a performance drop at inference time, particularly when the model is sensitive to the exact distribution of features.

  3. Not Effective **for Small Batches or Online Learning**: Batch normalization <u>struggles in settings where small batch sizes are required</u> (e.g., very large models, memory-constrained environments) or in online learning (streaming data) scenarios. When using small batch sizes, the statistics are often noisy and unstable, which reduces the effectiveness of normalization. In online learning, <u>since the data comes in a stream and mini-batch statistics are not available, applying Batch Norm becomes impractical.</u>

  4. Batch Norm Doesn't Work Well with **Recurrent Neural Networks (RNNs)**: Batch normalization is less effective in Recurrent Neural Networks (RNNs) and other sequential models. In RNNs, <u>the internal state is passed over many time steps, and normalizing these sequences using batch statistics can interfere with the temporal dependencies in the data.</u> Other normalization techniques, such as **Layer Normalization**, are typically preferred in RNNs and transformers because they work better in sequential models.

<br>

### Side Note: Layer Norm

>  **Layer Normalization (LayerNorm)** is a normalization technique used in neural networks, particularly effective <u>for models where the input is sequential,</u> such as Recurrent Neural Networks (RNNs) and Transformers. Unlike Batch Normalization (BatchNorm), which normalizes across the batch dimension, LayerNorm normalizes <u>across the features within each layer for each individual input, making it independent of the batch size.</u>

For each input instance (or sample), LayerNorm normalizes the activations across all features (neurons) in a laye<u>r by calculating the mean and variance for the entire layer’s output</u>, rather than across a mini-batch. This ensures that the activations have a **consistent distribution for each input**, which **helps stabilize and speed up the training process.**



<br><br>



## Transformers

> The **Transformer** is a deep learning model architecture introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017. It revolutionized the field of Natural Language Processing (NLP) and is now the backbone of many state-of-the-art models, *including BERT, GPT, and T5.* The Transformer is unique because it is based entirely on attention mechanisms, **without relying on the recurrence** (as in Recurrent Neural Networks, RNNs) or convolutions (as in Convolutional Neural Networks, CNNs). This allows it to handle longer-range dependencies and parallelize training more efficiently.

### Key Components of the Transformer:

1. **Self-Attention Mechanism**:

   - The Transformer’s most critical innovation is its **self-attention** mechanism, which enables the model to attend to all words in a sequence simultaneously, rather than processing them sequentially as in RNNs.
   - **Self-attention** computes a weighted representation of the entire input sequence, where the importance of each word in the sequence is calculated relative to all other words. This helps the model understand the context of each word by considering its relationships with every other word in the sequence. <br>

2. **Positional Encoding**:

   - Unlike RNNs, which have an inherent sense of order due to their sequential nature, the Transformer processes input in parallel. This means the model lacks a natural understanding of word order or positional information.
   - To encode this positional information, the Transformer adds **positional encodings** to the input embeddings, allowing the model to capture the order of words in the sequence.<br>

3. **Encoder-Decoder Architecture**:

   - The original Transformer architecture consists of two main components:
     - **Encoder**: A stack of identical layers where each layer applies self-attention followed by a feed-forward neural network. The encoder's job is to take in the input sequence and learn rich representations of it.
     - **Decoder**: Also a stack of identical layers, but in addition to self-attention, it uses **cross-attention** to attend to the encoder’s output. The decoder is primarily used in sequence-to-sequence tasks (e.g., machine translation), where it generates the output sequence step by step, based on the input sequence's representation from the encoder.
     

   ![image-20240912192241945](/images/2024-09-07-TIL24_Day70_DL/image-20240912192241945.png)

   ![image-20240912192120193](/images/2024-09-07-TIL24_Day70_DL/image-20240912192120193.png)

   ![image-20240912192215557](/images/2024-09-07-TIL24_Day70_DL/image-20240912192215557.png)

   ![image-20240912192345319](/images/2024-09-07-TIL24_Day70_DL/image-20240912192345319.png)

4. **Multi-Head Attention**:

   - The Transformer uses **multi-head attention**, which is a mechanism that applies several self-attention operations in parallel. Each attention "head" focuses on different parts of the input sequence. The results are then concatenated and projected to get the final output.
   - This allows the model to learn different aspects of relationships between words (e.g., syntactic and semantic relations) from multiple perspectives.

5. **Feed-Forward Network (FFN)**:
   - After applying self-attention, each encoder and decoder block has a **feed-forward network**, which is applied to each position independently. The FFN typically consists of two linear layers with a ReLU activation in between, allowing for non-linear transformations of the attention outputs.

6. **Layer Normalization and Residual Connections**:
   - Each sub-layer (like self-attention and feed-forward networks) in the Transformer is followed by **layer normalization** and a **residual connection**. This helps with training stability and allows better gradient flow, which speeds up convergence.<br><br><br>

### Workflow of the Transformer

1. **Input Embedding and Positional Encoding**:
   - Each word in the input sequence is converted into an embedding (vector representation). Positional encodings are added to these embeddings to provide information about the position of each word in the sequence.
2. **Encoder**:
   - The input embeddings pass through the stack of encoder layers, which apply self-attention and feed-forward networks to transform the input into contextualized representations.
3. **Decoder**:
   - In tasks like machine translation, the decoder receives the target sequence (e.g., the translation so far) and uses self-attention and cross-attention (to attend to the encoder’s output) to generate the next word in the output sequence.
4. **Output**:
   - The final output of the decoder passes through a linear layer followed by a softmax to produce the probability distribution over the target vocabulary for the next word in the sequence.

<br>

### Self-Attention: Key, Query, and Value

- The self-attention mechanism involves three components: 

  <b>Key (K), Query (Q) , and Value (V) </b> vectors.

  - Each word in the sequence is mapped to a query, key, and value vector.
  - The self-attention mechanism computes a similarity score between each query and key (dot product), which is then used to weight the value vectors.
  - The output is a weighted sum of the value vectors, where words more relevant to the current word (based on the key-query similarity) are given more attention.

<br>

### Benefits of the Transformer:

1. **Parallelization**:
   - Unlike RNNs, which process input sequentially, the Transformer can process the entire input sequence in parallel, which significantly speeds up training and inference, especially on large datasets.
2. **Long-Range Dependencies**:
   - Self-attention allows the Transformer to capture long-range dependencies in the data. RNNs can struggle with this due to vanishing gradients, but the Transformer can easily attend to words that are far apart in a sequence.
3. **Scalability**:
   - Transformers scale effectively to very large datasets and models with billions of parameters. This scalability has led to models like GPT-3 and BERT, which achieve state-of-the-art results on numerous NLP tasks.
4. **Versatility**:
   - The Transformer architecture is highly versatile and has been adapted for a wide range of tasks beyond NLP, such as image processing (Vision Transformers) and speech recognition.

<br>

### Limitations of the Transformer:

1. **Computationally Expensive**:
   - The self-attention mechanism has a quadratic complexity in relation to the input sequence length. This means that for long sequences, the computation required can become very expensive.
2. **Lack of Inductive Bias**:
   - Unlike RNNs or CNNs, which have an inherent understanding of temporal order (RNNs) or spatial locality (CNNs), the Transformer relies on positional encodings to provide this information. This lack of inherent inductive bias can make the Transformer less effective when training on smaller datasets.

<Br>

### Popular Transformer-Based Models:

1. **BERT (Bidirectional Encoder Representations from Transformers)**:
   - BERT is a pre-trained Transformer model that uses the encoder portion of the Transformer architecture and focuses on bidirectional learning from text. It is pre-trained on masked language modeling tasks and has achieved state-of-the-art results in a wide range of NLP tasks.
2. **GPT (Generative Pre-trained Transformer)**:
   - GPT uses the decoder portion of the Transformer to generate text. Models like GPT-3 are extremely powerful at text generation and have been used in many applications, including conversation agents and text completion.
3. **T5 (Text-To-Text Transfer Transformer)**:
   - T5 treats every NLP task as a text-to-text problem, where both the input and output are represented as text strings. It uses the full Transformer architecture for a wide range of NLP tasks like translation, summarization, and more.
4. **Vision Transformer (ViT)**:
   - The Transformer architecture has also been applied to image data through Vision Transformers, which treat image patches as a sequence of tokens and apply self-attention to model global dependencies in the image.

<Br><Br><Br>
