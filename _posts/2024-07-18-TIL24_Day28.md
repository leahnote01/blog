---
title: "Day26 ML Review - Training Perceptron (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,TIL_24]
toc: true 
---

<img src="/blog/images/2024-07-18-TIL24_Day28/C75BB982-CD7B-4DCE-A5A0-2DB986BB5F1D_1_105_c.jpeg">

<br><br>

*(Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2, 3rd Edition)*

## Step by step of Training Perceptron (3)

<br>

```python
y_pred = ppn.predict(X_test_std)
print(‘Misclassified examples: %d’ % (y_test!=y_pred).sum()
```



After running the code, we observed that the perceptron misclassifies one out of 45 flower examples, resulting in a misclassification error of approximately 0.022 on the test dataset. 



Additionally, it's worth noting that scikit-learn offers a wide range of performance metrics through the `metrics` module. For instance, we can calculate the classification accuracy of the perceptron on the test dataset using these metrics.



```python
from sklearn.metrics import accuracy_score
print(‘Accuracy: %.3f’ % accuracy_score(y_test, y_pred))
```



Here, `y_test` represents the true class labels, and `y_pred` represents the previously predicted class labels. In scikit-learn, each classifier has a `score` method, which calculates the prediction accuracy by combining the `predict` call with `accuracy_score`, as demonstrated below. 



```python
print(‘Accuracy: %.3f’ % ppn.score(X_test_std, y_test))
```



Please remember the following:

- Classification error versus accuracy

Instead of focusing on the misclassification error, many machine learning practitioners prefer to report the classification accuracy of a model. This is calculated as follows:

<center>
  $error = 1-0.978 = 0.022$
</center>

Whether we utilize the classification error or accuracy is simply a matter of personal preference.



- Overfitting

It's important to note that we will be assessing the performance of our models based on the test dataset. Additionally, other valuable techniques, such as learning curves, can be used to identify and prevent overfitting. Overfitting occurs when a model effectively captures the patterns in the training data but struggles to generalize well to unseen data.



<br><br>
