---
title: "Day104 Deep Learning Lecture Review - Lecture 19 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Data-Centric AI: Label Noise, Error Analysis for Model Improvement, Splitting Datasets

<img src="/blog/images/2024-12-12-TIL24_Day104_DL/JPEG image-84EFE4B47418-1.jpeg"><br><br>

> Model-centric AI focuses on algorithmic elements like training techniques and regularization, relying on a dataset that is often static. However, **real-world datasets are dynamic**. In contrast, **data-centric AI emphasizes the systematic enhancement of data to create superior AI systems**, prioritizing the data itself over the specific models used.

### Label Noise

- Many Datasets Have Label Noise
  - Many datasets (e.g., ImageNet, CIFAR-100) contain mislabeled samples. For example, CIFAR-100 has 6% label errors.
  - Some label errors can be corrected, while others arise from <u>multi-label inputs, out-of-distribution data</u>, etc. 

- **Impact**:
  - Small amounts of label noise often do not significantly impact learning.
  - However, label noise in test sets can lead to misleading performance metrics.

- **Test Set Noise**: Models like ResNet-18 can outperform larger models like ResNet-50 in the presence of noisy labels, highlighting potential overfitting to noise.

- Correcting Errors
  - Identifying and correcting label errors can improve performance and make evaluation more accurate.
  - Remember to version the datasets. Correcting errors creates a new version of the dataset.

- Techniques to Address Label Noise

  1. **Confident Learning:**

     - Identifies likely label errors using **predicted probabilities** and **observed noisy labels**.
     - Does not require any **guaranteed uncorrupted labels**. 
     - <u>Non-iterative and applicable to multi-class datasets.</u>
     - Example: Detecting label errors in ImageNet in 3 minutes.<br><br>

     Source: [Confident Learning: Estimating Uncertainty in Dataset Labels - Curtis G. Northcutt, Lu Jiang, Isaac L. Chuang](https://arxiv.org/abs/1911.00068)

     ![image-20250122184726461](/images/2024-12-12-TIL24_Day104_DL/image-20250122184726461.png)

     ![image-20250122184739888](/images/2024-12-12-TIL24_Day104_DL/image-20250122184739888.png)

     ![image-20250122185026554](/images/2024-12-12-TIL24_Day104_DL/image-20250122185026554.png)

     - Using the **Q** Matrix  
       - Multiply **Q** by the number of examples.  
       - If there are 100 examples in this case, then there would be <u>10 images labeled as dog that are predicted to actually be foxes.</u>  
       - Identify the 10 images labeled as dog <u>that have the highest probability of belonging to the fox class.</u>  
       - Repeat this process for all non-diagonal entries in the matrix.

     - Confident Learning and **Cross-Validation**
       - The process outlined applies solely to the test dataset.
       - To identify label errors in the **training** dataset, utilize **cross-validation**.<br><br>

  2. **Using Embeddings**





0





