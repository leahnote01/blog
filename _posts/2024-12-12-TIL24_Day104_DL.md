---
title: "Day104 Deep Learning Lecture Review - Lecture 19 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Data-Centric AI: Label Noise, Error Analysis for Model Improvement, Splitting Datasets

<img src="/blog/images/2024-12-12-TIL24_Day104_DL/JPEG image-84EFE4B47418-1.jpeg"><br><br>

> Model-centric AI focuses on algorithmic elements like training techniques and regularization, relying on a dataset that is often static. However, **real-world datasets are dynamic**. In contrast, **data-centric AI emphasizes the systematic enhancement of data to create superior AI systems**, prioritizing the data itself over the specific models used.

### Label Noise

- Many Datasets Have Label Noise
  - Many datasets (e.g., ImageNet, CIFAR-100) contain mislabeled samples. For example, CIFAR-100 has 6% label errors.
  - Some label errors can be corrected, while others arise from <u>multi-label inputs, out-of-distribution data</u>, etc. 

- **Impact**:
  - Small amounts of label noise often do not significantly impact learning.
  - However, label noise in test sets can lead to misleading performance metrics.

- **Test Set Noise**: Models like ResNet-18 can outperform larger models like ResNet-50 in the presence of noisy labels, highlighting potential overfitting to noise.

- Correcting Errors
  - Identifying and correcting label errors can improve performance and make evaluation more accurate.
  - Remember to version the datasets. Correcting errors creates a new version of the dataset.

- Techniques to Address Label Noise

  1. **Confident Learning:**

     - Identifies likely label errors using **predicted probabilities** and **observed noisy labels**.

     - Does not require any **guaranteed uncorrupted labels**. 

     - <u>Non-iterative and applicable to multi-class datasets.</u>

     - Example: Detecting label errors in ImageNet in 3 minutes.

     - 

       

       Source: [Confident Learning: Estimating Uncertainty in Dataset Labels - Curtis G. Northcutt, Lu Jiang, Isaac L. Chuang](https://arxiv.org/abs/1911.00068)

![image-20250122184726461](/images/2024-12-12-TIL24_Day104_DL/image-20250122184726461.png)

![image-20250122184739888](/images/2024-12-12-TIL24_Day104_DL/image-20250122184739888.png)

![image-20250122185026554](/images/2024-12-12-TIL24_Day104_DL/image-20250122185026554.png)



0





