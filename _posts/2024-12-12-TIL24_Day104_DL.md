---
title: "Day104 Deep Learning Lecture Review - Lecture 19 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Data-Centric AI: Label Noise, Selection Bias, Data Leakage, Error Analysis for Model Improvement, Splitting Datasets

<img src="/blog/images/2024-12-12-TIL24_Day104_DL/JPEG image-84EFE4B47418-1.jpeg"><br><br>

> Model-centric AI focuses on algorithmic elements like training techniques and regularization, relying on a dataset that is often static. However, **real-world datasets are dynamic**. In contrast, **data-centric AI emphasizes the systematic enhancement of data to create superior AI systems**, prioritizing the data itself over the specific models used.

### Label Noise

- Many Datasets Have Label Noise
  - Many datasets (e.g., ImageNet, CIFAR-100) contain mislabeled samples. For example, CIFAR-100 has 6% label errors.
  - Some label errors can be corrected, while others arise from <u>multi-label inputs, out-of-distribution data</u>, etc. 

- **Impact**:
  - Small amounts of label noise often do not significantly impact learning.
  - However, label noise in test sets can lead to misleading performance metrics.

- **Test Set Noise**: Models like ResNet-18 can outperform larger models like ResNet-50 in the presence of noisy labels, highlighting potential overfitting to noise.

- Correcting Errors

  - Identifying and correcting label errors can improve performance and make evaluation more accurate.
  - Remember to version the datasets. Correcting errors creates a new version of the dataset.

  

#### Techniques to Address Label Noise

1. **Confident Learning:**

   - Identifies likely label errors using **predicted probabilities** and **observed noisy labels**.
   - Does not require any **guaranteed uncorrupted labels**. 
   - <u>Non-iterative and applicable to multi-class datasets.</u>
   - Example: Detecting label errors in ImageNet in 3 minutes.<br><br>

   Source: [Confident Learning: Estimating Uncertainty in Dataset Labels - Curtis G. Northcutt, Lu Jiang, Isaac L. Chuang](https://arxiv.org/abs/1911.00068)

   <center>
     <img src="/blog/images/2024-12-12-TIL24_Day104_DL/image-20250122184726461.png"><br>
     <img src="/blog/images/2024-12-12-TIL24_Day104_DL/image-20250122184739888.png"><br>
     <img src="/blog/images/2024-12-12-TIL24_Day104_DL/image-20250122185026554.png"><br><br>
   </center>

   

   - Using the **Q** Matrix  
     - Multiply **Q** by the number of examples.  
     - If there are 100 examples in this case, then there would be <u>10 images labeled as dogs that are predicted actually to be foxes.</u>  
     - Identify the 10 images labeled as dogs <u>with the highest probability of belonging to the fox class.</u>  
     - Repeat this process for all non-diagonal entries in the matrix.

   - Confident Learning and **Cross-Validation**
     - The process outlined applies solely to the test dataset.
     - To identify label errors in the **training** dataset, utilize **cross-validation**.<br><br>

2. **Using Embeddings**

   - Train a **strong self-supervised model** (e.g., DinoV2).
   - Cluster embeddings and <u>inspect outliers for potential errors.</u>
   - Human validation is often needed to confirm errors.
   - For class $k$, identify outliers that have a significant distance from the nearest cluster centroid for that class.
     - For outliers, determine if they are nearer to a centroid from a different class.
     - If they are closer to a different class, review these samples for potential label errors with the assistance of a human expert.<br><br>

3. **Human-validation of Label Errors**
   - **Why It's Needed**:
     - Methods like confident learning or embedding-based clustering are not 100% accurate.
     - Human review ensures ambiguous cases are resolved accurately.
   - **Best Practices**:
     - Use multiple annotators to check each flagged sample.
     - Resolve disagreements with a consensus review.

<Br><Br>

### Selection Bias

> Deep learning systems can perform exceptionally well on test data that aligns with the training distribution; **however, dataset bias remains a significant challenge.** Addressing this issue using more data might not be practical, as some events are inherently rare and challenging to extract.



### Data Leakage

> When <u>the test set is contaminated with training data</u>, leading to inflated performance metrics.

- Write A Lot of Tests to Mitigate Leakage
  - We should have a lot of tests to <u>mitigate leakage and ensure our splits are not contaminated.</u>
  - Try to ensure all data is unique before splitting.

- **Examples**:
  - Cancer detection systems should ensure that individual patient data is not included in the training or test sets. <span style="background-color: #499; color: white">**The test set should be as independent from the training set as possible.**</span>
    - Otherwise, we may inflate performance.
  - Slightly modified duplicates of training data are present in the test set.



- <u>Statistics Are Not Sufficient</u>
  - While accuracy and other summary statistics are helpful, they obscure many facets of performance and **do not indicate how to enhance the model.** 
  - We must think deeply about what we are trying to achieve and design appropriate metrics.

![image-20250125202539319](/images/2024-12-12-TIL24_Day104_DL/image-20250125202539319.png)

- **One Statistic?**

  - Due to label bias, **one statistic** can create the impression that a model is performing well when, in fact, it isn't.  

  - We must define " **chance** " and <b>ensure that the model outperforms merely predicting <u>the most likely category.</u> </b>

  - Reporting only average performance can under-represent severe failure cases <u>for rare examples and subpopulations.</u> <br>

    

- **Confusion Matrix**

  ![image-20250125204027774](/images/2024-12-12-TIL24_Day104_DL/image-20250125204027774.png)

  - Use a confusion matrix for classification and mutually exclusive classes. A confusion matrix provides a comprehensive overview of the performance of a classification model by showing true positives, false positives, true negatives, and false negatives.  
  - However, it does not suffice for subgroups. Alternative measures, such as F1 scores or precision-recall curves, should be considered for datasets that contain overlapping classes or subgroup dependencies. 
  - The mean of the normalized diagonal is **the mean per class accurac**y. 







