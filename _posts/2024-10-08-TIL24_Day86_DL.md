---
title: "Day86 Deep Learning Lecture Review - Lecture 8"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, LLM, NLP, TIL_24]
toc: true 
---

# Large Language Model - Generating Texts, 

![AE0FC9EC-0991-419A-97A4-9AE8740AAFDA](/images/2024-10-08-TIL24_Day86_DL/AE0FC9EC-0991-419A-97A4-9AE8740AAFDA.jpeg)

<br><Br>

#### Generating Text from Large Language Models (LLMs)

- Autoregressive LLMs
  - **Autoregressive Language Models (LLMs)** generate text by predicting the next token in a sequence based on the previous tokens <u>in a sequential, step-by-step manner.</u>
  - The model generates <u>tokens one at a time</u>. After predicting a token, <u>that token becomes part of the input for predicting the next token.</u> It continues until the end of generates [EOS] or reaches a pre-defined length. 
  - $
    P(w_1, w_2, \dots, w_T) = P(w_1) \cdot P(w_2 \mid w_1) \cdot P(w_3 \mid w_1, w_2) \cdot \dots \cdot P(w_T \mid w_1, w_2, \dots, w_{T-1})$ <Br>




- Greedy / Argmax Approach
  1. Token Prediction: At each step in the generation process, <u>the model outputs a probability distribution over the entire vocabulary for the next token, based on the tokens generated so far.</u>
  2. **Argmax** Selection: The model selects the token with the **<u>highest probability</u>** (i.e., the token that maximizes the probability ($P$) as the next token in the sequence. This is done by taking the argmax of the probability distribution- ($\text{argmax}_{w}P(w \vert w_1, w_2, \dots, w_t)$)
  3. Sequence Continuation: <u>The selected token is added to the sequence</u>, and the process repeats until an end-of-sequence token is generated or the maximum sequence length is reached. <br>



- Disadvantages of the Greedy/Argmax Approach
  - Lack of Diversity: Potentially leading to repetitive or overly simplistic sentences. *(Maybe partially a side-effect of training with cross-entropy / softmax.)*
  - Suboptimal Global Outcome: While the model is maximizing the probability of the next token at each step, it may not lead to the best overall sequence. The model can **get stuck in local optima**, where a high-probability word might lead to a suboptimal continuation of the sequence.
  - We have a heavy tailed output distribution.
    - Exponentially <u>higher loss for linearly lower probability</u> (due to the cross-entropy loss) 
      +
      Exponentially l<u>ower probabilities for linear lower scores</u> 
    - It makes unlikely events far too likely to avoid high loss values.



- Direct Sampling
  - Randomly sample the next token proportional to the probability produced by the softmax output.
- Top-k Sampling
  - Sample from the top-k highest probability tokens at each time step
  - Use a temperature to control how "peaky" the softmax is.
  - But it can be too low-information.
    - Lots of specific choices not considered.











