---
title: "Day60 ML Review - Cross Validation (5)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, TIL_24]
toc: true 
---

# ROC area Under The Curve (ROC AUC)

<img src="/blog/images/2024-08-23-TIL24_Day60/83B605C7-5ACE-47E4-9545-F7A09E7CBF4B_1_105_c.jpeg"><br><br>

<font size=3pt><I>Image and Explanation from <a href="url">link text</a></I></font>



## Key Concepts

> Receiver operating characteristic (ROC) graphs are useful tools for selecting classification models **based on their performance in terms of false positive rate (FPR) and true positive rate (TPR)**. These rates are calculated by adjusting the classifier's decision threshold. The diagonal line in an ROC graph represents random guessing, and any classification models falling below the diagonal line are considered worse than random guessing.

In an ideal scenario, a classifier would be positioned in the top-left corner of the graph, with a True Positive Rate (TPR) of 1 and a False Positive Rate (FPR) of 0, as below.

<center>
  <img src="/blog/images/2024-08-23-TIL24_Day60/image-20240829191426145.png" width="70%"><br><br>
</center>



Using the ROC curve, we can calculate the ROC area under the curve (ROC AUC) to evaluate a classification model's performance.

![image-20240829191522866](/images/2024-08-23-TIL24_Day60/image-20240829191522866.png)



## AUC and ROC for Choosing Model and Threshold

AUC is a useful measure for comparing the performance of two different models, as long as the dataset is roughly balanced. (See **Precision-recall curve**, above, for imbalanced datasets.) The model with greater area under the curve is generally the better one.



![image-20240829191647241](/images/2024-08-23-TIL24_Day60/image-20240829191647241.png)

<br><br>

