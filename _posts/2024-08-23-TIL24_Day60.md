---
title: "Day60 ML Review - Cross Validation (5)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [mlReview, classifier, crossValidation, TIL_24]
toc: true 
---

# ROC area Under The Curve (ROC AUC)

<img src="/blog/images/2024-08-23-TIL24_Day60/83B605C7-5ACE-47E4-9545-F7A09E7CBF4B_1_105_c.jpeg"><br><br>

<font size=3pt><I>Image and Explanation from <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc">Google Developer_ML Concepts Webpage</a> <br>(https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)</I></font>



## Key Concepts

> Receiver operating characteristic (ROC) graphs are useful tools for selecting classification models **based on their performance in terms of false positive rate (FPR) and true positive rate (TPR)**. These rates are calculated by adjusting the classifier's decision threshold. The diagonal line in an ROC graph represents random guessing, and any classification models falling below the diagonal line are considered worse than random guessing.

In an ideal scenario, a classifier would be positioned in the top-left corner of the graph, with a True Positive Rate (TPR) of 1 and a False Positive Rate (FPR) of 0, as below.

<center>
  <img src="/blog/images/2024-08-23-TIL24_Day60/image-20240829191426145.png" width="50%"><br><br>
</center>




Using the ROC curve, we can calculate the ROC area under the curve (ROC AUC) to evaluate a classification model's performance.

<center>
  <img src="/blog/images/2024-08-23-TIL24_Day60/image-20240829191522866.png" width="50%"><br><br><Br>
</center>





## AUC and ROC for Choosing the Model and Threshold

Area Under the Curve (AUC) is a helpful metric for <u>comparing the performance of two models</u>, provided that the dataset is balanced. When dealing with imbalanced datasets, using the precision-recall curve is better. The model **with a larger area under the curve is generally considered better.** 



<center>
  <img src="/blog/images/2024-08-23-TIL24_Day60/image-20240829191647241.png" width="70%"><br><br>
</center>

The points on a ROC curve closest to (0,1) indicate **a range of the best-performing thresholds** for the given model. As explained in the Thresholds, Confusion Matrix, and Choice of Metric and Tradeoffs sections, <u>the choice of threshold depends on which metric is most important for the specific use case</u>. Consider points A, B, and C in the following diagram, each representing a threshold.

<center>
  <img src="/blog/images/2024-08-23-TIL24_Day60/image-20240829200110630.png" width="50%"><br><br><br>
</center>

### Scikit-learn Application

In the upcoming code example, we will generate an ROC curve <u>to evaluate the performance of a classifier</u> that utilizes only two specific features from the Breast Cancer Wisconsin dataset to determine whether a tumor is benign or malignant. Despite using the same logistic regression pipeline as before, this time we will only employ two features. <I>(This decision is made to increase the difficulty of the classification task by withholding valuable information from the other features, resulting in a more challenging ROC curve.) </I> To further enhance the visual interest of the ROC curve, <u>we will reduce the number of folds</u> in the `StratifiedKFold` validator to <u>three</u>. Below is the code snippet:

```python
from sklearn.metrics import roc_curve, auc
from scipy import interp
pipe_lr = make_pipeline(StandardScaler(), PCA(n_components=2),
                        LogisticRegression(penalty='12',
                                           random_state=1, 
                                           solver='lbfgs', 
                                           C=100.0)
# use only two features
X_train2 = X_train[:, [4,14]]
cv = list(StratifiedKFold(n_splits=3, random_state=1).split(x_train, y_train))
                        
fig = plt.figure(figsize=(7, 5))
mean_tpr = 0.0
mean_fpt = np.linspace(0, 1, 100)
all_tpr = []
                        
for i, (train, test) in enumerator(cv):
                        probabs = pipe_lr.fit(
                        X_train2[train],
                        Y_train[train].predict_proba(X_train2[test])
                        fpr, tpr, thresholds = roc_curve(y_train[test],
                                                        probas[:, 1],
                                                        pos_label=1)
                        mean_tpr += interp(mean_fpr, fpr, tpr)
                        mean_tpr[0] = 0.0
                        roc_auc = auc(fpr, tpr)
                        plt.plot(fpr,
                                tpr,
                                label='ROC fold'))
```









<br><br>

