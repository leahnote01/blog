---
title: "Day102 Deep Learning Lecture Review - Lecture 18 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Data-Centric AI: Crowdsourcing, Neural Scaling Laws, and Active Learning 

<img src="/blog/images/2024-11-29-TIL24_Day102_DL/IMG_0298.JPG">

<br><Br>

### What is Data-Centric AI?

<I>Explanations from: [MIT - Introduction to Data-Centric AI](https://dcai.csail.mit.edu/)</I>

> Data-centric AI focuses on systematically improving datasets to enhance model performance. <u>Unlike Model-Centric AI</u>, which prioritizes algorithms and architectures, Data-Centric AI shifts attention to **curating, cleaning, and optimizing data**.

- Data-centric AI deals with systematically engineering data to build better AI systems.
- Real-world datasets are messy, and improving the dataset can significantly improve results.
- Data-centric AI thinks about AI in terms of the data and not the specific model.
- Data-centric AI Methods
  - Outlier detection and removal
  - Detecting and correcting label errors
  - Data augmentation
  - Feature engineering and selection
  - Establishing consensus labels from crowdsourced data.
  - Active learning to select the most informative data to label next.
  - Identifying the correct data and possibly data ordering to train models.<Br><br>

<center>
  <img src="/blog/images/2024-11-29-TIL24_Day102_DL/image-20241221170034955.png" width="80%"><br><br>
</center>



**Key Principles**:

1. **Systematic Data Engineering**:
   - Addresses noisy, redundant, and unbalanced datasets to improve model accuracy.
   - Implements methods like outlier detection, label correction, and data augmentation.
2. **Real-World Datasets are Messy**:
   - Many datasets have labeling errors, redundant samples, or poor-quality data points. Improving these can lead to better results than scaling model size or complexity.
3. **Iterative Dataset Improvements**:
   - Involves ongoing refinement of datasets, which includes labeling errors correction, balancing datasets, and focusing on high-quality data.

**Not Data-Centric AI**:

- Doubling dataset size indiscriminately without improving data quality.
- Hand-picking data points based on subjective criteria.





### Data Creation and Curation

- Data Creation is Expensive
  - Datasets may cost millions of dollars to create. 
    - Many companies spend much time getting prompts and dialogues to train LLMs.
  - The COCO dataset for semantic segmentation required over 85,000 annotator hours.<br><br>

- **Curating Datasets**

  - Major issues include:
    - Detecting erroneously labeled instances
    - Identifying inputs with quality problems 
      - Blurry Images
    - Data deduplication
    - Removing unwanted data
      - Toxic data for LLMs
    - Ensuring **no data leakage** between train and data sets
      - When scraping the web, many images may be identical or modified versions of the original
        - JPEG vs. PNG
        - Crops

  - **General Steps**
    - Acquire <u>large</u> amount of data
      - Most may be unlabeled
    - Acquire <u>labels</u> for the data
      - Usually from a human
    - Curate the dataset to remove errors and bad dta
    - Split dataset into appropriate partitions
    - Define metrics and study design for analysis
    - Version the dataset<br><br>

  

#### Crowdsourcing Labels

> Crowdsourcing involves gathering labels for datasets **from a distributed group of annotators**, often using platforms like Amazon Mechanical Turk. This approach is especially beneficial for large-scale datasets where manual labeling of all data is impractical.

- **Key Characteristics:**
  - Scalable: Can label vast amounts of data quickly.
  - Low Cost: Often cheaper than relying on domain experts.
  - Diverse Perspectives: Incorporates multiple viewpoints, which can improve label quality.<br><br>

- **Challenges**:
  1. Annotator Noise:
     - Not all annotators are equally skilled or diligent, leading to errors.
  2. Consistency:
     - Different annotators might interpret tasks differently, introducing variability.
  3. Quality Control:
     - Ensuring high-quality labels from non-expert annotators is challenging.
  4. Applications:
     - Text sentiment analysis, image recognition, medical data annotation.<br><br>



#### Estimating Annotator Quality

> To ensure reliable labels from crowdsourced data, it’s crucial <u>to estimate the quality of each annotator</u>. This allows identifying and weighing annotations based on annotator reliability.

**Estimating Inter-Annotator Agreement**

- We can quantify the confidence that a consensus label is correct via the agreement between annotations for an example:

  <center>
    $\text{Agreement}_i = \frac{1}{|\mathcal{J}_i|} \sum_{j \in \mathcal{J}_i} \left[ Y_{ij} == \hat{Y}_i \right]$ <br><br>
    </center>

- Where we are computing the average number of annotators who agreed on the label.

- We can estimate the quality of an anotator $j$ based on the fraction of their annotations that agree with the consensus label for the same example:

  <center>
    $\text{Quality}_j = \frac{1}{|\mathcal{I}_{j,+}|} \sum_{i \in \mathcal{I}_{j,+}} \left[ Y_{ij} == \hat{Y}_i \right]$<br><br>
  </center>

  - $\text{Quality}_j$: Measures the quality of annotator $j$.

  - $\vert \mathcal{I}_{j,+} \vert $: Total number of items labeled by annotator $j$ in the subset $\mathcal{I} _{j,+}$.   

  - $\sum_{i \in \mathcal{I} _{j,+}}$: Summation over all items $i$ labeled by annotator $j$ in the subset $\mathcal{I} _{j,+}$.     

  - $$\left[ Y _{ij}  = =  \hat{Y} _i \right]$$: Indicator function, which evaluates to 1 if annotator $j$'s label ($Y_{ij}$) matches the predicted/true label ($\hat{Y} _i$), and 0 otherwise.

    <br><br>

<b>Methods to Estimate Annotator Quality</b>

- **Accuracy Against Gold Standard**
  - **Gold Standard**: 
    - Provide annotators with data in <u>which you are 100% confident of the ground truth</u>.
    - If they poorly annotate this data, <u>you can evaluate</u> whether they are delivering appropriately high-quality data or merely providing you with poor results. 
    - Note that for certain issues, such as semantic segmentation, there is inherent subjective variability in the annotations provided.
    - Compare annotator's labels against a small set of verified labels.
    - $\text{Accuracy} = \frac{\text{Correct Annotations}}{\text{Total Annotations}}$

<br>

- **Agreement with Other Annotators**
  - Measure consistency between annnotators.
  - Common metric: **Cohen's Kappa** or **Fleiss' Kappa.**

<br>

- **Model-Based Estimation**
  - Use probabilistic models (e.g., Bayesian methods) to estimate annotator reliability based on their label history.

<br><Br>

**Problems with Majority Vote**

- Resolving ties is ambiguous.
- A bad annotator and good annotator have an equal impact on estimates.
- If you cannot enumerate a specific list of classes, e.g., natural language outputs, you
  need many annotators to get a consensus. <br><Br>



#### Estimating The Amount of Data We Need for Training

> Determining how much data is necessary to train a machine learning model is <u>a critical step in designing an efficient pipeline.</u> Too little data results in underfitting, while too much data wastes resources.

- For AI products, performance needs to be greater than some threshold.

  - Often these criteria are specified by product managers.

  - **Example**: Cancer detection recall over 99% with less than 5% false positives.

    <Br><br>

<center>
  <img src="/blog/images/2024-11-29-TIL24_Day102_DL/image-20241221192743901.png" width="80%"><br><br>
</center>





**Pareto Curve**

- A **Pareto Curve** is a graphical representation that <u>helps illustrate trade-offs between two competing objectives</u>, often in optimization problems or efficiency analysis. It is used to visualize the Pareto frontier, which is the set of points representing the best possible trade-offs.

**Key Features**:

1. **Pareto Frontier**:
   - The curve or boundary of the Pareto-efficient points where improving one objective leads to a degradation in the other.
   - Any point on the Pareto frontier is optimal in the sense that you cannot improve one objective without worsening the other.
2. **Dominated Points**:
   - Points below the Pareto curve are considered suboptimal because at least one objective can be improved without degrading the other.
3. **Applications**:
   - Data-Centric AI: Trade-offs between data quality and dataset size.
   - Economics: Trade-offs between cost and benefit.
   - Machine Learning: Trade-offs between model accuracy and computational cost.<br><Br>



#### **Error as a Function of Training Data Follows a Power Law**

In machine learning, the error (e.g., test error) of a model often decreases as the size of the training dataset increases. This relationship frequently follows a **power law**:

<center>
  $E(n) = \frac{A}{n^b}+C$<br><br>
</center>



where:

- $E(n)$: The error as a function of training data size $n$.
- $A$: A scaling constant that depends on the model and task complexity.
- $b$: The **power law exponent**, which indicates how quickly the error decreases as the data size grows.
- $C$: The irreducible error (Bayes error), representing the theoretical minimum error the model can achieve.



- Key Features of the **Power Law**

  - **Diminishing Returns**
    - As the size of training data $n$ increases, the error reduction becomes smaller.
    - For small dataset, adding more data significantly reduces error.
    - But, for larger datasets, the error reduction per additional sample diminishes.
  - **Scaling Laws**
    - The power law demonstrates the fundamental scaling behavior of model performance with data size.
    - For complex tasks (e.g., vision, NLP), the power law exponent $b$ is typically small (e.g., $b \approx 0.5$), indicating slower improvements with data scaling.
  - **Universal Behavior:**
    - The power law applies across many machine learning tasks and models, including surprised learning, deep learning, and even unsupervised learning tasks.<br><br>

  

  <center>
    <img src="/blog/images/2024-11-29-TIL24_Day102_DL/image-20241221211256644.png" width="80%"><br><br>
  </center>

  

<br><Br>

### Selecting Data to Label

#### Active Learning for Efficiently Labeling Datasets

> Active Learning is an iterative process to label data points that will maximize the model's performance. Instead of randomly labeling data, it identifies the most **informative samples** to label next.

- Method for determining which samples from a large unlabeled data pool to label that would maximally improve models.
- Active learning is a method for efficiently getting labels for datasets.<br><Br>

<center>
  <img src="/blog/images/2024-11-29-TIL24_Day102_DL/image-20241221212000077.png" width="80%"><br><br>
</center>



- Process:
  1. **Unlabeled Data Pool**
     - Begin with a large pool of unlabeled data.
  2. **Initial Labeling**
     - Label <u>a small subset of the data</u> to train an initial model.
  3. **Select Informative Samples**
     - Use the trained model <u>to rank unlabeled samples by uncertainty</u> (e.g., entropy or margin sampling).
     - <u>Choose the most uncertain samples</u> to label in the next round.
  4. **Iterate**
     - Retrain the model with the newly labeled data and repeat the process.

- **Advantages**
  - **Efficiency**: Saves resources by focusing on the most impactful samples.
  - **Cost Reduction**: Reduces the need to label large datasets.

- **Applications**
  - Annotating medical data, where expert labeling is expensive (e.g., radiology images).
  - Selecting rare concepts in large datasets using methods like **SEALS** (Similarity search for Efficient Active Learning).

<Br><br>
