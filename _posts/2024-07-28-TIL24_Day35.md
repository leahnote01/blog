---
title: "(editing) Day35 ML Review - Decision Tree (1) "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,decisionTree,TIL_24]
toc: true 
---

# Basic Concepts & Maximizing Information Gain- Gini Impurity, Entropy, Classification Error

<img src="/blog/images/2024-07-28-TIL24_Day35/68468A00-4BAB-4C8C-B42E-D789A35DF17A.jpeg"><br><br>

> Decision tree classifiers are highly valued for their ability to provide *interpretability*. This model operates by breaking down data through a series of decision-making questions. These questions can involve real numbers, allowing the model to ask binary questions such as "Is the sepal width greater than or equal to 2.8 cm?" This approach enables a clear and understandable process for decision-making based on the input data.



When using the decision algorithm, the process starts at the **root** of the tree by splitting the data based on the feature **that yields the highest information gain (IG)**. This splitting procedure is then iteratively **applied at each child node** <u>until the leaves contain data that is entirely homogeneous</u>, meaning that <u>all the training examples at each leaf belong to the <I>same class.</I></u> In practical terms, this may result in a deep tree with a large number of nodes, which increases the risk of overfitting the model to the training data. To address this, it is common practice to **prune** the tree **by setting a limit on the maximum depth**, ensuring a more generalized model.

<br>

### Maximizing IG: Getting the Best Possible Value with the Best Efficiency

To split the nodes effectively based on the most informative features, we must establish an <u>objective function to optimize through the tree learning algorithm.</u> Here is the objective function intended to maximize the information gain at each split.

<center>
  $IG(D_p, f) - I(D_p) - \sum_{j=1}^{m}\frac{N_j}{N_p}I(D_j)$<br><br>
</center>

The variable $f$ represents the feature used to perform the split; $D_p$ and $D_j$ are the datasets of the parent and $j^{th}$ child node; I is our impurity measure; $N_p$ is the total number of training examples at the parent node; and $N_j$ is the number of examples in the jth child node. Information gain is essentially **the difference between the impurity of the parent node and the sum of the child node impurities**. <u>The lower the impurities of the child nodes, the larger the information gain.</u> However, to simplify and reduce the search space, most libraries, including scikit-learn, implement binary decision trees.



This means that each parent node is split into two child notes, $D_{left}$ and $D_{right}$:

<center>
  $IG(D_p, f) = I(D_p)-\frac{N_{left}}{N_p}I(D_{left})-\frac{N_{right}}{N_p}I(D_{right})$<br><br>
</center>



The three impurity measures or splitting criteria  that are commonly used in binary decision trees are: **Gini Impurity($I_G$)**, **Entropy($I_H$)**, and **the classification error($I_E$)**. 

<br><br>

