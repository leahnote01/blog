---
title: "Day35 ML Review - Decision Tree (1) "
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,decisionTree,TIL_24]
toc: true 
---

# Basic Concepts & Maximizing Information Gain- Gini Impurity, Entropy, Classification Error

<img src="/blog/images/2024-07-28-TIL24_Day35/68468A00-4BAB-4C8C-B42E-D789A35DF17A.jpeg"><br><br>

> Decision tree classifiers are highly valued for their ability to provide *interpretability*. This model operates by breaking down data through a series of decision-making questions. These questions can involve real numbers, allowing the model to ask binary questions such as "Is the sepal width greater than or equal to 2.8 cm?" This approach enables a clear and understandable process for decision-making based on the input data.



When using the decision algorithm, the process starts at the **root** of the tree by splitting the data based on the feature **that yields the highest information gain (IG)**. This splitting procedure is then iteratively **applied at each child node** <u>until the leaves contain entirely homogeneous data, meaning that all the training examples at each leaf belong to the <I>same class.</I></u> In practical terms, this may result in a deep tree with many nodes, which increases the risk of overfitting the model to the training data. To address this, it is common practice to **prune** the tree **by limiting the maximum depth**, ensuring a more generalized model.

<br>

### Maximizing IG: Getting the Best Possible Value with the Best Efficiency

We must establish an objective function to optimize through the tree learning algorithm to <u>split the nodes effectively based on the most informative features.</u> Here is the objective function to maximize the information gain at each split.

<center>
  $IG(D_p, f) - I(D_p) - \sum_{j=1}^{m}\frac{N_j}{N_p}I(D_j)$<br><br>
</center>

The variable $f$ represents the feature used to perform the split; $D_p$ and $D_j$ are the datasets of the parent and $j^{th}$ child node; I is our impurity measure; $N_p$ is the total number of training examples at the parent node, and $N_j$ is the number of examples in the jth child node. Information gain is **the difference between the impurity of the parent node and the sum of the child node impurities**. <u>The lower the impurities of the child nodes, the larger the information gain.</u> However, most libraries, including scikit-learn, implement binary decision trees to simplify and reduce the search space.



This means that each parent node is split into two child notes, $D_{left}$ and $D_{right}$:

<center>
  $IG(D_p, f) = I(D_p)-\frac{N_{left}}{N_p}I(D_{left})-\frac{N_{right}}{N_p}I(D_{right})$<br><br>
</center>


The three impurity measures or splitting criteria  that are commonly used in binary decision trees are: **Gini Impurity($I_G$)**, **Entropy($I_H$)**, and **the classification error($I_E$)**. 

<br>

#### 1. Entropy ($I_H$)

Here is the definition of entropy for all **non-empty** class $P(i \mid t) \neq 0$

<center>
  $I_H(t)=-\sum_{i=1}^{c}p(i \mid t)log_2p(i \mid t)$ <br><br>
</center>

In the context of decision trees, the term $p(i\mid t)$ represents the <u>proportion</u> of examples belonging to the class $i$ at a specific node, denoted as $t$. The concept of entropy measures <u>the impurity or uncertainty at a node.</u> <u>It is 0 when all examples at a node belong to the same class and increases as the class distribution becomes more uniform.</u> 

Specifically, in a binary class scenario, 

- the entropy is 0 when $p(i=1 \mid t)=1$ or $p(i=0 \mid t)=0$. 
- Conversely, if the classes are evenly distributed with $p(i=1 \mid t)=0.5$ and $p(i=0 \mid t)=0.5$, the entropy is 1, indicating maximum uncertainty. 

In summary, the entropy criterion in decision tree construction aims to <u>maximize the mutual information within the tree.</u> This effectively reduces uncertainty and enhances the tree's overall effectiveness in classifying new instances.

<br>

#### 2. Gini Impurity ($I_G$)

The Gini impurity can be understood as a criterion to minimize the probability of misclassification:

<center>
  $I_G(t) = \sum_{i=1}^{c} p(i \mid t)(1-p(i \mid t)) = 1 - \sum_{i=1}^{c}p(i \mid t)^2$ <br><br><br>
</center>

Similar to entropy, the Gini impurity is maximal when the classes are perfectly mixed. This occurs, for example, in a binary class setting $(c=2)$.

<center>
  $I_G(t) = 1-\sum_{i=1}^{c}0.5^2=0.5$
<Br><Br><br>
</center>

In practice, <u>both Gini impurity and entropy usually yield similar results.</u> It's not worth spending much time on evaluating trees using different impurity criteria rather than experimenting with different pruning cut-offs.

<br>

#### 3. Classification Error

Classification error measurement is as follows.

<center>
  $I_E(t) = 1 - max\{ p(i \mid t) \} $
<br><br><Br>
</center>

This criterion is useful for pruning, but not recommended for growing a decision tree because it is less sensitive to changes in the class probabilities of the nodes.



<br><br>

