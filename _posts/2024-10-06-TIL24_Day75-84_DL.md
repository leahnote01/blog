---
title: "Day76-84 Introduction to Natural Language Processing using Deep Learning"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, CNN, TIL_24]
toc: true 
---

# Basic Machine Learning & Deep Learning, Word Embedding, CNNs, RNNs, LSTM and Transformer 

> Throughout my E2E Deep Learning course journey, I realized that my foundational knowledge could have been more robust. Facing this challenge pushed me to dive deeper into the core concepts of machine learning and NLP, ensuring I didnâ€™t just scrape the surface but truly understood the intricacies. This visual map became my go-to guide, helping me grasp complex ideas and connect theoretical principles with real-world applications. Studying these concepts took me three weeks as my semester started, so I've tried to spend at least 3 hours per day. 

Each section represents a stepping stone, helping me overcome obstacles and bridge knowledge gaps I hadnâ€™t anticipated. From fundamental machine learning concepts to the most advanced architectures, this process solidified my understanding and prepared me to tackle more sophisticated NLP challenges.

I studied these concepts in **Korean** ðŸ‡°ðŸ‡· as part of building my foundational knowledge, using the fantastic resource from [**Wikidocs**](https://wikidocs.net/book/2155). It was a challenging yet gratifying process that reminded me that facing obstacles is part of growth. Iâ€™m excited to continue deepening my knowledge in this field and applying these skills to real-world NLP challenges! ðŸ”¥ I will be studying further in English and posting those concepts, so please stay tuned! 



Hereâ€™s a breakdown of the topics I tackled during this deep dive:

### 1. Basic Machine Learning Concepts:

- Logistic Regression
- Linear Regression
- Gradient Descent (Batch, Stochastic, Mini-Batch)
- Cross-Entropy Loss and Cost Functions
- Overfitting, Regularization (L1, L2), and Generalization Techniques
- Bias-Variance Tradeoff
- Decision Trees and Random Forests

### 2. Deep Learning Architectures:

- Fully Connected Neural Networks
- Activation Functions (Sigmoid, ReLU, Leaky ReLU, Tanh)
- Backpropagation and Optimization Techniques (Adam, RMSprop, SGD)
- Weight Initialization and Batch Normalization
- Convolutional Neural Networks (CNNs):
  - Convolution Layers, Filters, and Pooling Layers
  - Applications of CNNs in Image Recognition and NLP tasks

### 3. Recurrent Neural Networks (RNNs) and Variants:

- Architectures of RNN, LSTM, and GRU models
- Sequence Processing and Time-Series Data
- Exploding and Vanishing Gradient Problems in RNNs

### 4. Word Embeddings:

- Architectures of Word2Vec, GloVe, ELMo, and FastText
- Embedding Layers for Dense Vector Representation
- Applications of Embeddings in NLP

### 5. Transformers:

- Self-Attention and Multi-Head Attention
- Sequence-to-Sequence Models with Attention
- Transformers and their Role in NLP
- Positional Encoding, Multi-Head Attention
- Position-wise FFNN, Residual Connections, and Layer Normalization



<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/2025C298-875F-448D-898B-32FB2EBEDE45_1_105_c.jpeg">

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/9BDBC069-5FF1-4855-A6E0-94B4309A57D2_1_105_c.jpeg">

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/BE078326-6FA1-4D2C-B3A0-0A62DDA2D95F_1_105_c.jpeg">

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/ADA54717-25C5-4696-9F2E-787B4CE7924E_1_105_c.jpeg">

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/88C2A7FC-61CB-4D31-BD0C-F138E824C232_1_105_c.jpeg">

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/8869678B-259E-4D3B-8A86-4B299FF2C031_1_105_c.jpeg">

<br><Br>



The Full Image file can be downloaded here on my GitHub! I have also added the link for each image below.



## Deep Learning Architecture

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/Drawing_9.jpg">

[ðŸ“š Down Load Here: Deep Learning Architecture Tree](https://github.com/leahnote01/blog/blob/978b8c0c6c31bd629f06f1b36ec723cffc54eb82/images/2024-10-06-TIL24_Day75-84_DL/Drawing_9.jpg)

<br><br>

## Word Embedding, RNN, and CNN

>  Word 2 BVec. Keras Embedding Layer, ELMo, Glove. FastText, and Keras application

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/Drawing_8.jpg">

![Drawing_11](/images/2024-10-06-TIL24_Day75-84_DL/Drawing_11.jpg)

![Drawing_12](/images/2024-10-06-TIL24_Day75-84_DL/Drawing_12.jpg)

![Drawing_13](/images/2024-10-06-TIL24_Day75-84_DL/Drawing_13.jpg)

[ðŸ“š Down Load Here: RNNs & CNNs](https://github.com/leahnote01/blog/blob/978b8c0c6c31bd629f06f1b36ec723cffc54eb82/images/2024-10-06-TIL24_Day75-84_DL/Drawing_8.jpg)



<br><Br>

## The architecture of RNN, Transformer, and Attention Mechanism

> Seq2sec, Attention, Dot-Product Attention, Transformer, Multi-head Attention, and Position-wise FFNN

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/Drawing_7.jpg">

![Drawing_14](/images/2024-10-06-TIL24_Day75-84_DL/Drawing_14.jpg)

![Drawing_15](/images/2024-10-06-TIL24_Day75-84_DL/Drawing_15.jpg)

![Drawing_16](/images/2024-10-06-TIL24_Day75-84_DL/Drawing_16.jpg)



[ðŸ“š Down Load Here: Transformers](https://github.com/leahnote01/blog/blob/978b8c0c6c31bd629f06f1b36ec723cffc54eb82/images/2024-10-06-TIL24_Day75-84_DL/Drawing_7.jpg)



<br><Br>

## The Basic Deep Learning

> Activation Function- Sigmoid, ReLU, Tanh, and Softmax, Perceptron, Loss Function, and Optimizers

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/Drawing_6.jpg">

[ðŸ“š Down Load Here: Deep Learning](https://github.com/leahnote01/blog/blob/978b8c0c6c31bd629f06f1b36ec723cffc54eb82/images/2024-10-06-TIL24_Day75-84_DL/Drawing_6.jpg)

<br><br>



## The Basic Machine Learning

> Linear Regression, Logistic Regression, Vector and Matrix, and Softmax Regression

<img src="/blog/images/2024-10-06-TIL24_Day75-84_DL/Drawing_5.jpg">

[ðŸ“š Down Load Here: Machine Learning](https://github.com/leahnote01/blog/blob/978b8c0c6c31bd629f06f1b36ec723cffc54eb82/images/2024-10-06-TIL24_Day75-84_DL/Drawing_5.jpg)

<br><br>



