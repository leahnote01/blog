---
title: "Day63 ML Review - Ensemble Method (2)"
layout: single
classes: wide
read_time: True
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, ensembleMethod, TIL_24]
toc: true 
---

# (editing) Code Structure of Combining Classifiers via Majority Vote

<img src="/blog/images/2024-08-27-TIL24_Day63/B22A3978-95B2-4AA4-896C-5EB24C90A19B.jpeg"><br><br>

## Combining Classifiers via Majority Vote

### Implementing a Simple Majority Vote Classifier

The algorithm we plan to implement will enable us to merge various classification algorithms with <u>specific weights to boost confidence.</u> We aim to create a more robust meta-classifier that offsets the individual classifiers' shortcomings on a particular dataset. To put it more precisely in mathematical terms, we can express the weighted majority vote in the following manner.

<center>
  $\hat{y} = \text{arg} \ \text{max}_i \sum^m_{j=1} W_j \chi_A \big( c_j(x)=i \big) $ <br><Br><br>
</center>

In the given formula, $W_j$ represents the weight connected with a base classifier, $C_j$, while $\hat{y}$ stands for the predicted class label of the ensemble. A denotes the set of unique class labels, and $\chi_A$ signifies the characteristic function or indicator function, which yields 1 if the predicted class of the $j$th classifier matches $i$ ($C_j(x)=i$. To simplify the equation for equal weights, we can express it as follows.

<center>
  $\hat{y} = \text{mode} \big\{ C_1(x), C_2(x), \dots, C_m(x) \big\}$ <br><br><br>
</center>



Let's assume have three base classifiers to predict the class label of a given example. Two out of three base classifiers predict class `0`, and one predicts class `1`. <u>When we weigh the predictions of each base classifier equally, the majority vote predicts that the example belongs to class 0.</u>

<center>
  $C_1(x) \rightarrow 0,\ C_2(x) \rightarrow 0, \ C_3(x) \rightarrow 1$ <br><br>
  $\hat{y} = \text{mode} \{ 0,0,1 \} = 0$ <br><br><br>
</center>

Let's assign a weight of 0.6 to $C_3$ and let's weight $C_1$ and $C_2$ by a coefficient of 0.2:

<center>
  $\hat{y} = \text{arg} \ \text{max}_i \sum^m_{j=1} W_j \chi_A \big( c_j(x)=i \big)$ <br>
  <Br>
  $= \text{arg} \ \text{max}_i[0.2 \times i_0 + 0.2 \times i_0 + 0.6 \times i_1] = 1$ <br><br><br>
</center>



More simply, since $3 \times 0.2 = 0.6$, we can say that the prediction made by $C_3$ has three times more weight than the predictions by $C_1$ or $C_2$. This can be expressed as follows.

<center>
    $\hat{y} = \text{mode} \{ 0,0,1,1,1 \} = 1$ <br><br><br>
</center>

We can use NumPy's `argmax` and `bincount` functions as following code for easier way to calculate.

```python
>>> import numpy as np
>>> np.argmax(np.bincount([0,0,1], weights[0.2, 0.2, 0.6]))
1
```

<br><Br>

If we would like to determine this with the probability of the predicted class, we can derive equations as follows. 

  <center>
    $\hat{y} = \text{arg} \ \text{max}_i \sum^m_{j=1} w_j p_{ij}$ <br><br><Br>
  </center>





Let's assume that we have a binary classification problem with class labels $i \in \{0,1 \}$ and an ensemble of three classifiers, $C_j (j \in \{ 1,2,3\})$. Let's assume that the classifiers $C_j$ return the following class membership probabilities for a particular example, $x$:

<center>
  $$
  C_1(x) \rightarrow [0.9, 0.1],\ C_2(x) \rightarrow [0.8, 0.2], \ C_3(x) \rightarrow [0.4, 0.6] 
  $$ <br><br><br>
</center>



Using the same weights, the probability of the individual class as follows.

<center>
  $p(i_0 \vert x) = 0.2 \times 0.9 + 0.2 \times 0.8 + 0.6 \times 0.4 = 0.58$ <Br><Br>
  $p(i_1 \vert x) = 0.2 \times 0.1 + 0.2 \times 0.2 + 0.6 \times 0.6 = 0.42$ <br><br>
  $\hat{y} = \text{arg} \ \text{max}_i [p(i_0 \vert x), p(i_1 \vert x)] = 0$ <br><Br><br>
</center>











<br>
