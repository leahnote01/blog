---
title: "Day107 Deep Learning Lecture Review - HW4 - Implementing Conformal Prediction and"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../../
tag: [dlReview, TIL_24]
toc: true 
---

# (Editing)

![110BCDEE-4EA5-47BD-877B-D706C51FB0B2_1_105_c](../../images/2024-12-20-TIL24_Day107_DL/110BCDEE-4EA5-47BD-877B-D706C51FB0B2_1_105_c.jpeg)<br><br>

Uncertainty quantification is essential to deep learning applications, especially in critical domains like healthcare and autonomous systems. I explored **Conformal Prediction (CP)**, a framework that provides prediction sets with reliable coverage guarantees. The object was implementing **Naive and Adaptive Prediction Sets Algorithms** using a pre-trained ResNet Model.  <br><br>



### Understanding Conformal Prediction

Traditional deep learning models provide a single-point prediction with a confidence score (e.g., softmax probability). However, these confidence scores can be **miscalibrated** and fail to deliver reliable uncertainty estimates. **Conformal Prediction** addresses this limitation by generating **multiple possible prediction sets** while guaranteeing a **confidence level.** 

Mathematically, conformal prediction ensures that:  

<center>
$1-\alpha \leq P(Y \in \tau(X)) \leq 1-\alpha + \frac{1}{(n+1)}$ <br><Br>  
</center>

Where:

- $\alpha$ is the significance level,
- $X$ is the input,
- $Y$ is the true label,
- $\tau (X)$ is the prediction set.

The main concept is <u>to utilize a scoring function</u> to assess how closely the model's predictions align with the actual labels and then establish **quantiles** to form prediction sets. <br><br>

#### Implementing the Naïve Prediction Set Algorithm

The naïve method constructs a prediction set by including classes until the cumulative probability surpasses a predefined threshold. 

**Implementation Steps**

1. **Prepare the datasets**

   - Used `softmax_outputs.npy` for predicted probabilities.
   - Used `correct_classes.npy` for ground-truth labels. 

2. **Split the data**

   - The first **2000 samples** are used for **calibration**.
   - The remaining samples are for **validation**.

3. **Calculate scores**

   - The score function is simply the probability assigned to the true class: 

     <center>
       $s(X,Y) = 1-\hat{f}(X)_Y$ <br><br>
     </center>

   - Compute the quantile threshold $\hat{q}$ using:
     <center>
       $\hat{q} = Quantile(s_1, \dots, S_n; 1- \alpha $
     </center>

4. **Generate prediction sets**

   - Include the top $k$ classes until their cumulative probability exceeds $1-\hat{q}$.
   - **Iterate through softmax outputs** until the **cumulative probability surpasses** the threshold.  <br><Br>

**Results**

- **Empirical coverage: 98.66%**
- **Key observation:** The **naïve method** produces **small prediction sets** but can occasionally miss the true class, affecting reliability. 
- Coverage slightly below **99**% meaning some samples were **miclassified** without correction. 
