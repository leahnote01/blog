---
title: "Day108 Deep Learning Lecture Review - HW5- Advanced Techniques in DL"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../../
tag: [dlReview, TIL_24]
toc: true 
---

# Out-of-distribution (OOD) Detection(Maximum Softmax Probability, ODIN, & SLDA) and Continual Learning

![B07A283A-6E97-4810-BFFA-6811C788A37A_1_105_c](../../images/2024-12-21-TIL24_Day108_DL/B07A283A-6E97-4810-BFFA-6811C788A37A_1_105_c.jpeg) <br><br>

As deep learning models are increasingly deployed in real-world applications, ensuring their **reliability and adaptability** is crucial. This posting focused on two key areas:

1. **Out-of-distribution (OOD) Detection** – Identifying whether an input sample belongs to a distribution that the model was trained on. 
2. **Continual Learning (CL)** – Developing models that *retain past knowledge* while learning from **new data streams**.

In this post, I will cover the methods and insights from my experiments with **Maximum Softmax Probability (MSP**), **ODIN(Out-of-Distribution detector for Neural Networks)**, and **Streaming Linear Discriminant Analysis (SLDA)**.<br><br>

## Out-of-distribution (OOD) Detection

Deep learning models are often **overconfident** in their predictions, **even when encountering unfamiliar data.** OOD detection aims to flag such inputs, preventing unreliable predictions.



### 1.1 Maximum Softmax Probability (MSP)

**MSP** is a <u>baseline approach</u> for OOD detection that relies on the idea that a well-calibrated model should assign **low confidence scores** to OOD samples. The **MSP score** is defined as :

<center>
  $max_y \sigma_y(f(x)/T)$ <br><br>
</center>

Where:

- $\sigma_y$ is the **softmax function,**
- $T$ is the **temperature scaling factor.**<br><br>



#### Implementation

1. **Loaded a pre-trained WideResNet model** on CIFAR-10.
2. **Created a test set** combining:
   - **In-distribution (ID)**: CIFAR-10 test set.
   - **Out-of-distribution (OOD)**: LSUN dataset resized to CIFAR-10 dimensions.
1. **Computed MSP scores** for each samples
1. **Generated an ROC curve** and evaluated detection performance.<br><Br>



#### Results

<center>
  <img src="/../../images/2024-12-21-TIL24_Day108_DL/image-20250131232353408.png" width="50%"><br><br>
</center>



| Metric                          | Value |
| ------------------------------- | ----- |
| AUROC ($\uparrow$)              | 0.76  |
| FPR@95 ($\downarrow$)           | 0.41  |
| AUPR ($\uparrow$)               | 0.52  |
| Detection Accuracy ($\uparrow$) | 0.50  |

- **AUROC** of 0.76 indicates **moderate OOD detection ability.**

- **High FPR@95(41%)** means the model falsely classifies many OOD samples as in distribution.

- **MSP is a weak baseline,** as it does not actively distinguish ID from OOD samples.

  ```python
  # ... Load Datasets
  
  # Step 3: Initialize the MaxSoftmax detector and OOD metrics
  detector = MaxSoftmax(model)
  metrics = OODMetrics()
  
  # Step 4: Perform OOD detection
  scores = []
  labels = []
  with torch.no_grad():
    for inputs, targets in test_loader:
      inputs = inputs.to(device)
      score = detector(inputs)
      scores.extend(score.cpu().numpy())
      labels.extend(targets.numpy())
  
      
  # Convert targets to binary labels: 0 for in-distribution, 1 for OOD
  labels_binary = [0 if label in range(10) else 1 for label in labels]
  
  
  # Step 5: Generate the ROC curve
  fpr, tpr, thresholds = roc_curve(labels_binary, scores)
  roc_auc = auc(fpr, tpr)
  
  # Calculate FPR@95
  tpr_95_index = next(i for i, x in enumerate(tpr) if x >= 0.95)
  fpr_at_95 = fpr[tpr_95_index]
  
  # Calculate AUPR
  precision, recall, _ = precision_recall_curve(labels_binary, scores)
  aupr = auc(recall, precision)
  
  # Detection Accuracy
  threshold = 0.5
  predictions = [1 if score > threshold else 0 for score in scores]
  detection_accuracy = sum([pred == true for pred,
                            true in zip(predictions,
                                        labels_binary)]) / len(labels_binary)
  ```

  <br><Br>

### 1.2 ODIN: Out-of-Distribution Detector

ODIN improves upon MSP by **perturbing the input** and applying **temperature scaling**, making OOD samples easier to detect. 

1. **Perturbation** - Modifies input $x$ using **gradient-based perturbation.**

   <center>
     $\hat{x} = x - \epsilon \cdot \text{sign}(\nabla_xL(f(x)/T,\hat{y}))$<br><bR>
   </center>

   Where $\epsilon$ is a small perturbation and $T$ is the temperature parameter.



2. **Temperature Scaling** - Amplifies softmax probabilities to **sharpen the confidence differences.** <br><Br>



#### Implementation

- **Used ODIN with default and tuned hyperparameters:****
  - Default: $\epsilon=0.0014,\ T=1000$
  - Tuned: $\epsilon = 0.002, \ T=1000$
- **Generated ROC curves** for different settings. <br><Br>



#### Results

- How does the perturbation process in ODIN relate to adversarial examples?

  - The perturbation process in ODIN involves adding a small, carefully designed input perturbation that aligns with the direction of the gradient of the loss for the input. **This makes OOD samples more distinguishable from in-distribution samples**. <u>This process is similar to the generation of adversarial examples crafted to maximize the loss.</u> In ODIN, the perturbation helps amplify the differences between in-distribution and OOD samples rather than fooling the model. 

- Metrics Results

  | Detector     | AUC ($\uparrow$) | FPR@95($\downarrow$) | Comments                     |
  | ------------ | ---------------- | -------------------- | ---------------------------- |
  | MSP          | 0.76             | 0.41                 | Baseline method              |
  | ODIN_Default | 0.86             | 0.26                 | Default ODIN hyperparameters |
  | ODIN_Tuned   | 0.89             | 0.18                 | Tuned hyperparameters        |

<center>
  <img src="/../../images/2024-12-21-TIL24_Day108_DL/image-20250201214441309.png"><br><br>
</center>



- 
