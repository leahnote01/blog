---
title: "Day46 ML Review - K-Nearest Neighbors (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, kNearestNeighbors, TIL_24]
toc: true 
---

# The Curse of Dimensionality 

<img src="/blog/images/2024-08-05-TIL24_Day46/IMG_1473.JPG">

<br><br>

> It is important to mention that KNN is very susceptible to overfitting due to the **curse of dimensionality**. The curse of dimensionality describes the phenomenon where the feature space becomes increasingly sparse for an increasing number of dimensions of a fixed-sized training dataset.



<I>DataCamp, "The Curse of Dimensionality in Machine Learning: Challenges, Impacts, and Solutions," accessed Aug. 6, 2024. [Online]. Available: https://www.datacamp.com/blog/curse-of-dimensionality-machine-learning</I> <br><br><br>



### Key Concepts

The **curse of dimensionality** refers to various phenomena that arise when analyzing and organizing data in <u>high-dimensional spaces (with a large number of features or variables)</u>. As the number of dimensions increases, the volume of the space increases exponentially, leading to a variety of challenges for machine learning algorithms and statistical methods.<br><Br>



### How does the curse of dimensionality occur?

As <u>we increase the number of dimensions in our dataset</u>, the space it occupies grows exponentially. This means that <u>the data becomes more spread ou</u>t. To illustrate, if we have a one-dimensional line, it's relatively easy to populate it with a few points. However, with a two-dimensional square, we would need more points to cover the area. Now, picture a three-dimensional cube - we'd require even more points to fill the space. This concept applies to higher dimensions, resulting in extremely sparse data.<br><br>



### What problems does it cause?

1. **Distance Measures Become Less Meaningful**:

   - <u>In high-dimensional spaces, the concept of "distance" between data points becomes less informative.</u> For example, in low-dimensional space, nearby points tend to be similar, and distant points tend to be different. However, in high dimensions, the distances between all points tend to converge, meaning that all points become nearly equidistant from each other.
   - This phenomenon can make algorithms like K-Nearest Neighbors (KNN) less effective, as the distinction between "near" and "far" neighbors becomes blurred.

   **Example**: If you have data in a 2D space, the difference in distances between the closest and farthest points might be significant. However, <u>as you move to higher dimensions, such as 1000D, the difference between the nearest and farthest points decreases</u>, making it harder to differentiate between them.

   <Br><Br>

2. **Data Sparsity**:

   - <u>As the number of dimensions increases</u>, data points become more sparse, meaning that the <u>density of data points in the space decreases</u>. This sparsity makes it difficult to find meaningful patterns because most of the space is empty.
   - In high dimensions, to have enough data to adequately represent the space, you would need an exponentially larger amount of data. However, in practice, collecting such a vast amount of data is often impractical or impossible.

   **Example**: Imagine a simple grid. In 1D, you might need only 10 points to cover a line. In 2D, you might need 100 points to cover a square. In 3D, you'd need 1,000 points to cover a cube. As you go to higher dimensions, the number of points required grows exponentially.

   <br><br>

3. **Overfitting**:

   - In high-dimensional spaces, <u>models can easily become overly complex</u> because there are more features available to explain the variance in the data. This complexity can lead to overfitting, where the model captures noise rather than the underlying patterns, performing well on training data but poorly on unseen data.
   - Overfitting is more likely because, with many dimensions, a model can find spurious correlations between features and the target variable that do not generalize.

   **Example**: In a high-dimensional space, a model might fit every single training data point perfectly by creating overly complex decision boundaries. However, this model would likely perform poorly on new data because it has learned to "memorize" the training data rather than generalize.

   <br><br>

4. **Increased Computational Complexity**:

   - The computational <u>cost of algorithms increases with the number of dimensions</u>. Operations that are straightforward in low dimensions, like searching for the nearest neighbors or calculating distances, become computationally expensive in high-dimensional spaces.
   - Many machine learning algorithms, like decision trees, support vector machines, or clustering algorithms, experience significant slowdowns or become intractable as the dimensionality increases.

   **Example**: A simple search for the nearest neighbor in a low-dimensional space might be fast, but in a high-dimensional space, it can become exponentially slower, requiring much more time and computational resources.

   <br><br>

5. **Dimensionality Reduction Techniques**:

   - To combat the curse of dimensionality, dimensionality reduction techniques are often employed. These techniques aim to reduce the number of dimensions while preserving as much of the relevant information as possible.
   - **Principal Component Analysis (PCA)**, **t-SNE**, and **Autoencoders** are common techniques used to reduce dimensionality and mitigate the curse.

   **Example**: PCA can reduce a 100-dimensional dataset to just a few dimensions by finding the principal components that explain most of the variance in the data. This reduction helps to simplify the model, reduce overfitting, and improve computational efficiency.

<br><br>
