---
title: "Day07 ML Review - Linear Regression"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, TIL_24]
toc: true
---

# Linear Regression Overview & Cost Function (1) (MSE)

<img src="/blog/images/2024-05-22-TIL24_Day7/8915EDEF-F6CE-4B6A-867B-A331A7853217.jpeg" alt="day07in">

### Overview

>Linear regression is a method for modeling the linear relationship between a dependent variable and one or more independent variables. Linear regression aims **to find a linear equation that best predicts the dependent variabl**e from the independent variables.

Linear Regression is a data analysis technique that <u>predicts the value of unknown data using known, related data values.</u> It creates a mathematical model using a linear equation to represent the relationship between a dependent variable and one or more independent variables.  <br>

For instance, if you have data on your expenses and income for the past year, linear regression can help determine if your expenses are half your income. This analysis allows you to predict future expenses by halving the projected income. Linear regression models are simple and offer straightforward mathematical formulas for making predictions. This statistical technique is widely used in software and computing applications.<br>

In Machine Learning, algorithms analyze large data sets and use that data to calculate linear regression equations. **Data Scientists first train an algorithm on a known or labeled data set and then use that algorithm to predict unknown values**. 

<br><Br>

### Simple Linear Regression

The model predicts the outcome based on a single independent variable. The relationship between the dependent variable $y$ and the independent variable $x$ is defined by the equation: <br><center>
$$
\\ y= \beta_0 + \beta_1x + \epsilon \\
$$
<br></center>



- $\beta_0$ & $\beta_1$ are named as 'regression **coefficient**'. 
- **$\beta_0$​** is the **intercept** of the line (the value of $y$ when $x=0$)
- $\beta_1$ is the **slope** of the line, which represents the effect of the independent variable on the dependent variable. (the change in $y$ for a one-unit change in $x$)
- $\epsilon$ is the **error** term which accounts for the variability in $y$ that cannot be explained by the linear relationship with $x$.

<br>

#### Estimating the Coefficients

The coefficients $\beta_0$ and $\beta_1$​ are estimated using the least squares method, which involves **minimizing the sum of the squared differences (residuals)** between <u>the observed values and those predicted by the model</u>. The residuals are given by: <br>

<center>$e_i = y_i - (\beta_0  + \beta_1 x_i) \\$</center>

<br>

where $e_i$ is the residual for the $i$-th observation.

<br>

The sum of squared residuals (SSR) is: <br>

<center>$SSR = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n(y_i - \beta_0 - \beta_1 x_i)^2$

</center>

<br>

To find the best-fit line, **we take the partial derivatives of SSR** with respect to $\beta_0$ and $\beta_1$, set them to **zero**, and solve for $\beta_0$ and $\beta_1$.

<br>

#### Solutions for $\beta_0$ and $\beta_1$.

The solutions for $\beta_1$ and $\beta_0$ from the normal equations are:<br>

<center> $\beta_1 = \frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n({x_i-\bar{x}})^2}$ </center>

<br>

<center> $\beta_0 = \bar{y} - \beta_1 \bar{x}$ </center>

<br>

where $\bar{x}$ and $\bar{y}$ are the **sample means** of $x$ and $y$, respectively (It is impossible to calculate population parameters, so we must select samples).<br> 

<img src="/blog/images/2024-05-22-TIL24_Day7/image-20240522161234928.png" alt="regression">

<center>(image from: https://blog.naver.com/mykepzzang/220935571198)</center> 

<br>



#### Multivariate Linear Regression

$$
\\ y=\beta_0+\beta_1x_1+\beta_2x_2+ \dots +\beta_nx_n+ \epsilon
\\
$$

<br>

The multiple linear regression model counts on the multiple independent variables. <br>

It is typically used for <u>predicting continuous values and forecasting</u>, but it can be used for classification tasks, such as <u>binary classification.</u> In such cases, a threshold is set, and *values below the threshold predict one class, while values above predict another.* <br>

Linear regression is a **parametric** model that assumes a linear relationship between the input and output variables. <br><br>



### Limitations of Linear Regression

- **Linearity Assumption**: The biggest limitation is the assumption of linearity. Linear Regression assumes the relationship between the dependent and independent variables is linear. This can be overly simplistic as real-world data often exhibits non-linear patterns.
- **Influence of Outliers**: Linear regression models are <u>highly sensitive to outlier values.</u>  Outliers can have a disproportionately large effect on the fit of the model, often skewing the entire regression line.
- **Homoscedasticity**: Linear regression assumes the variance of <u>residual errors is consistent across all levels of the independent variables</u> (homoscedasticity). The model's predictions become less reliable if the error variance changes (heteroscedasticity).
- **Independence**: The model assumes that the observations are independent of each other. In cases where there is autocorrelation between observations (as often found in time series data), the standard model fitting procedures can yield unreliable estimates.

<br>

### Cost Function of Linear Regression

> Linear regression aims to find the best-fitting line through a set of data points. This line is defined by its parameters (coefficients): the slope & the intercept.  The cost function helps determine how well the line fits the data by measuring the difference between the observed values and the values predicted by the model. 

<br>

In linear regression, the cost function is a critical component in evaluating the model's performance. Linear regression aims to find the best-fitting line through a set of data points. This line is defined by its parameters (coefficients): the slope and the intercept. The cost function helps determine how well the line fits the data by measuring the difference between the observed values and the values predicted by the model. The most commonly used cost function in linear regression is the Mean Squared Error (MSE).

<br><br>

#### Mean Squared Error (MSE)

The MSE is calculated by taking **the average of the squared differences** between the <u>observed values</u> (actual y-values of the data points) and the <u>predicted values</u> (predicted y-values obtained from the regression line). Mathematically, it can be expressed as: <br>

<center> $\text {MSE} = \frac{1}{n} \sum_{i=1}^n(y_i-\hat{y_i})^2   \\
\\$  </center>

<br>

Where : 

* $n$ is the number of data points,
* $y_i$ is the actual value at $i^{th}$ point,
* $\hat{y_i}$ is the predicted value at $i^{th}$ point.

<br>

##### Purpose of the MSE

1. **Performance Measurement**: MSE provides a quantifiable measure of how far the predictions deviate from the actual outcomes, which directly reflects the performance of the model.
2. **Model Fitting**: The aim of linear regression is to minimize the MSE to achieve the best fit. The model can find the line that best fits the data by adjusting the coefficients (slope and intercept) to reduce MSE.
3. **Gradient Descent**: MSE is particularly useful because it is differentiable, allowing techniques like gradient descent to be used for optimization. Gradient descent is an algorithm that iteratively adjusts the parameter to minimize the MSE.

<br>

##### Characteristics of MSE

* **Sensitive to Outliers**: MSE increases significantly when there are outliers in the data, as it squares the differences. This sensitivity can be beneficial for detecting outliers but can also skew the model if outliers are not addressed.
* **Always Non-negative**: MSE is always non-negative because it involves the squares of differences, and the square of any real number is non-negative. The best value (minimum) is 0 when all predictions perfectly match the actual values.



###### *References in Korean*

* Regression Coefficient: 회귀계수
* Residual: 잔차
  잔차는 실제 자료와 적합회귀선(fitted regression line or estimated regression line) 의 오차이다. 



<img src="/blog/images/2024-05-22-TIL24_Day7/665EDCFD-63C0-4EBF-BCDF-B2676ACFD55E.jpeg" alt="day07out">



