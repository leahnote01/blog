---
title: "Day34 ML Review - Support Vector Machine (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,supportVectorMachine,TIL_24]
toc: true 
---

# Solving Nonlinear Problems - Using a Kernal SVM

<img src="/blog/images/2024-07-25-TIL24_Day34/38847B5B-EBC1-4F8D-B527-A96BF2A90854_1_105_c.jpeg">

> Another reason SVMs are highly popular among machine learning practitioners is that they can be easily kernelized to solve nonlinear classification problems. 



### Kernel Methods for Non-Linearly Data

Kernel methods allow us to handle non-linearly separable data by creating nonlinear combinations of the original features. This is done by projecting the data onto a higher-dimensional space using a mapping function denoted as $\phi$, where the data becomes linearly separable. This process can transform a two-dimensional dataset into a new three-dimensional feature space, where the classes become separable via the following projection.

<center>
  $\phi(x_1, x_2) = (z_1, z_2, z_3) = (x_1, x_2, x_1^2+x_2^2)$<br><br>
</center>
<center>
  <img src="/blog/images/2024-07-25-TIL24_Day34/image-20240805150619081.png" width="60%"> <br>
  <I>(Python Machine Learning: Machine Learning and Deep Learning with Python, scikit-learn, and TensorFlow 2, 3rd Edition)</I> <br><br>
</center>

We can separate the two classes in the plot using a linear hyperplane, and this boundary becomes nonlinear when projected back onto the original feature space.

<br>

### Using the Kernal Trick to Find Separating Hyperplanes in a High-Dimensional Space

We transform training data into a higher-dimensional feature space using a mapping function $\phi$ and train a linear SVM model. Using the linear SVM model, we then use the same mapping function to transform new, unseen data for classification. However, creating new features is computationally expensive, especially with high-dimensional data. This is where the so-called "kernel trick" becomes crucial. 

Although we did not go into much detail about how to solve the quadratic programming task to train an SVM, in practice, we just need to replace the dot product $x^{(i)T}x^{(j)}$ by $\Phi(x^{(i)})^T \Phi(x^{(j)})$. In order to save the expensive step of calculating this dot product between two points explicitly, we define a so-called **Kernel Function**: 

<center>
  $\kappa(x^{(i)}, x^{(j)}) = \Phi(x^{(i)})^T \Phi(x^{(j)})$<br><Br>
</center>

One of the most widely used kernels is the **radial basis function (RBF)** kernel, which can simply be called the **Gaussian kernel**:

<center>
  $\kappa(x^{(i)}, x^{(j)}) = \exp\left(-\frac{\|x^{(i)} - x^{(j)}\|^2}{2\sigma^2}\right)$ <br>
  $\rightarrow $
  $\kappa(x^{(i)}, x^{(j)}) = \exp\left(-\gamma\|x^{(i)} - x^{(j)}\|^2\right)$
<br><br>
  Here, $\gamma = \frac{1}{2\sigma^2}$ is a free parameter to be optimized. <br><br><br>
</center>

The term "kernel" essentially represents a similarity function between two examples. The minus sign inverts the distance measure <u>to produce a similarity score</u>. As a result of the exponential term, the similarity score ranges between 1 (for exactly similar examples) and 0 (for dissimilar examples).



<center>
  <img src="/blog/images/2024-07-25-TIL24_Day34/image-20240805163336242.png" width="60%">
</center>



<br><br>

