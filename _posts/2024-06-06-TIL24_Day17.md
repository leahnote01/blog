---
title: "Day17 ML Review - Regularization"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,statReview,TIL_24]
toc: true 
---

# Concepts and Types of Regularization and How It Works

<img src="/blog/images/2024-06-06-TIL24_Day17 (copy)/C7923CD3-0060-448B-8B66-EAB650139961_1_105_c-7865625.jpeg"><br><br>

> Regularization in machine learning is a technique used to prevent overfitting. Overfitting happens when a model works well with the training data but performs poorly with new, unseen data. This usually occurs in complex models with too many parameters relative to the number of observations. Regularization solves this problem <u>by imposing a penalty on the size of the coefficients</u>. It prevents the model from learning the training data too well and reducing **the loss too quickly by adding a penalty term.** This helps to avoid overfitting by slowing down the reduction of loss. Regularization is any modification to a learning algorithm intended to reduce its generalization error but not its training error. 

Explanation from: https://deepdata.tistory.com/88<br><br>

## Underfitting & Overfitting

<img src="/blog/images/2024-06-06-TIL24_Day17 (copy)/image-20240608180009011.png">

- **Underfitting**: Not capturing enough patterns in the data. Perform <u>poorly on both train & test datasets.</u>

- **Overfitting**: occurs <u>when a model captures both the patterns and noise in the training data, which leads to poor performance on unseen data.</u> In simpler terms, **noise refers to the presence of irrelevant data**. In this context, the noise exists only in the training data. When a model trained with this noise encounters new test data, its performance suffers because the noise present in the training data does not exist in the test data. Various regularization techniques have been proposed to address overfitting. These techniques involve making minor adjustments to the model's learning algorithm to improve its generalization of new data.

  

## Types of Regularization

### **L1 Regularization (Lasso Regression)**

- Adds a penalty equivalent to the **absolute** value of the magnitude of coefficients.
- Formula: $\lambda \sum |w_i|$ where $\lambda$ is the regularization strength and
   $w_i$​ are the model coefficients.
- This can result in some coefficients being set to zero, leading to feature selection. L1 regularization helps reduce overfitting and select features by removing uninformative features from the model. It is used when we want to lessen the number of features in a model, especially if we think many features are not relevant or unnecessary.<br>



### **L2 Regularization (Ridge Regression)**

- Adds a penalty equivalent to the **square** **of the magnitude of coefficients.**
- Formula: $ \lambda \sum w_i^2 $
- All coefficients are shrunk by the same factor (none are zeroed out completely). This is used when we suspect many features influence the outcome, but their effect sizes are small or moderate.
- It **handles models affected by multicollinearity better** (where independent variables are highly correlated).<br><br>



## **How Regularization Works**

Regularization works by adding the regularization term to the loss function. The loss function of a machine learning algorithm is a sum of the prediction error term and the regularization term. The overall objective becomes:

- Minimize: Loss = Original Loss (e.g. MSE) + Regularization Term

Including the regularization term controls the complexity of the model, ensuring that the model coefficients do not fit the noise in the training data too closely. This is achieved by:

- Increasing $\lambda$: Leads to smaller coefficients, which can reduce overfitting but increase underfitting.
- Decreasing $\lambda$: Leads to larger coefficients, allowing the model to fit the data better but risking overfitting.<br><br>

### **Practical Implications:**

In practice, the choice of regularization technique and the tuning of the regularization parameter($\lambda$)  are critical. These are often selected through cross-validation, where different values are tried, and the one that performs best on a hold-out set is chosen.

Regularization is a foundational concept in machine learning, applicable across various models, including linear/logistic regression, neural networks, and more, helping improve model generalization.<br><br>



## Goal and Benefits of Regularization

- **Prevent Overfitting**
  - Regularization reduces the model’s complexity by constraining its coefficients, discouraging overly complex models that fit the noise in the training data.
- **Improve Model Generalization**
  - Simplified models perform better on new, unseen data because they capture the underlying trends rather than the specifics of the training set.
- **Handle High Dimensionality**
  - Regularization is particularly useful when the number of features (variables) exceeds the number of observations, making models unstable or prone to overfitting.
- **Feature Selection:** 
  - Some features can be eliminated from the model, especially with L1 regularization, helping identify the most important features for predicting the target variable.





#### High Dimensionality Issues

Several challenges arise in scenarios with more features (variables) than data points (observations).

- Overfitting: occurs when a model quickly learns the noise in the training data rather than the underlying pattern. This results in an outstanding performance on the training data, but the generalization needs to be improved to new, unseen data.
- Computational Complexity: more features increase the computational burden, as more parameters need to be estimated.
- Multicollinearity: high dimensionality often brings about multicollinearity, where some variables are highly correlated. This redundancy can destabilize the model, leading to unreliable and highly variable estimates of regression coefficients.
