---
title: "Day100 Deep Learning Lecture Review - Lecture 17 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Understanding Conformal Prediction: Concepts, Applications, Marginal Coverage and Recipes In Detail

<img src="/blog/images/2024-11-12-TIL24_Day100_DL/DA67E27A-A051-4870-9001-F7D4FA86977F_1_105_c.jpeg">

<br><br>

## Conformal Prediction

> Unlike traditional point predictions, conformal prediction generates **prediction regions**â€”sets of predictions for classification tasks and intervals for regression tasks. These regions are guaranteed to encompass the correct outcome with a user-specified confidence level, provided that the framework's conditions are satisfied.

- What is Conformal Prediction?
  - At its core, conformal prediction transforms traditional machine learning predictions into a format that includes uncertainty measures.
  - This framework builds on the assumption that data points in the <u>calibration set</u> (used for conformal prediction) and the test set are exchangeable, meaning they come from the same distribution. <br><br>



### Key Outputs

1. **Prediction Sets (Classification)**: For classification tasks, the model outputs a set of possible labels rather than a single label. For example, instead of predicting "cat," the prediction might include {"cat," "dog," "rabbit"}. This set is guaranteed to include the correct label with high probability.
2. **Prediction Intervals (Regression)**: For regression tasks, conformal prediction provides an interval (e.g., [4.2, 5.8]) that is statistically guaranteed to include the actual value with the specified confidence level (e.g., 90%).<br>

<center>
  <img src="/blog/images/2024-11-12-TIL24_Day100_DL/image-20241220135715934.png" width="80%"><br><br>
</center>



- Confidence Level ($\alpha$)

  - By setting $\alpha$, we <u>define the algorithm's error rate</u> we're willing to tolerate. 

    <center>
      $1-\alpha \leq \textbf{P}(Y_{test} \in C(X_{test})) \leq 1-\alpha +\frac{1}{n+1}$<br><br>
    </center>

  - For example

    - $\alpha = 0.05$: The system guarantees that the prediction will be correct <u>at least 95% of the time.</u>
    - $\alpha = 0.1$: The prediction is correct at least 90% of the time.

  - Smaller $\alpha$ values lead to **larger prediction regions**, which means <u>wider intervals</u> in regression or <u>larger sets</u> of labels in classification

  - This **trade-off** between specificity and certainty is key to understanding and applying confromal prediction effectively. <br><br>

### **How Does Conformal Prediction Work?**

- Two subsets of Conformal Prediction workflows
  - <u>Calibration</u>: A subset of the data, separate from the training and test sets, is used to compute thresholds. This calibration set ensures that the predictions align with the desired confidence level.
  - <u>Prediction</u>: Using the calibrated thresholds, <u>the model outputs prediction regions for unseen data.</u> These regions are dynamically adjusted based on the characteristics of the data and the uncertainty associated with each prediction.

- **Standard assumptions**
  - We have a pre-trained model for a prediction task.
    - Classification or regression
  - He have a heuristic notion of uncertainty from the pre-trained model.
  - We have a small amount of additional calibration data that was not used for training and is separate from test data.
  - We have a nonconformity measure used to produce prediction regions.<br><br>

- **Nonconformity measure**
  - A critical aspect of conformal prediction is the **nonconfomity measure**, which quantifies how "strange" or "unexpected" a new data point is relative to the calibration set.
  - Higher nonconfomity scores indicate greater uncertainty.
  - The choice of nonconformity measure can significantly affect the quality of prediction regions.<br><Br>

- **Quantiles and Thresholds**
  - Conformal prediction uses quantiles <u>to define the cut-off for prediction regions.</u> 
  - For instance, in <u>classification</u>, the algorithm includes all labels whose scores fall within a certain quantile threshold, ensuring that the prediction set captures the desired confidence level.<br>

- Empirical Quantile
  - An empirical quantile divides 1-D data into $m$ continuous intervals with equal probability $q$.
  - An empirical quantile function returns the cut-point for the interval corresponding to $q$, for qhich $(1-q)$% of the data are larger than the cut-point and $q$% are smaller than that value.

<center>
  <img src="/blog/images/2024-11-12-TIL24_Day100_DL/image-20241220142821639.png" width="80%"><br><Br>
</center>





<br>

### Applications of Conformal Prediction

Conformal prediction is a versatile framework with applications across domains:

1. **Medical Triage**: In critical healthcare scenarios, such as diagnosing diseases or interpreting medical images, high confidence is essential. For example, when analyzing a CT scan to determine the type of stroke a patient has, conformal prediction can ensure that the decision is statistically reliable. By setting a very low $\alpha$, the system can prioritize safety and minimize errors.
2. **Selective Classification**: Models can abstain from making a prediction when uncertainty is high. This is useful for routing difficult cases to human experts, improving overall system reliability.
3. **Active Learning**: In scenarios where labeled data is expensive to obtain, conformal prediction can identify the most uncertain samples (i.e., those with the largest prediction regions) for annotation. This approach accelerates learning while minimizing annotation costs.
4. **Out-of-Distribution (OOD) Detection**: When a test sample lies outside the distribution of the training data, conformal prediction can flag it as uncertain. This is especially useful in applications like autonomous driving or fraud detection.<br><br>

### Marginal vs. Conditional Coverage

1. **Marginal Coverage**: This is the standard guarantee offered by conformal prediction. It ensures that the correct prediction is included in the prediction region for at least $1-\alpha$ of all test samples, averaged across the dataset. For instance, at 90% confidence, marginal coverage ensures that <u>at least 90% of predictions are correct on average.</u>
2. **Conditional Coverage**: A stricter form of coverage, ensuring that predictions are correct across subgroups or classes. For example, in cancer diagnosis, we may want 99% accuracy both for patients with cancer and those without. Marginal coverage cannot guarantee fairness across such subgroups, whereas conditional coverage aims to addresss this limitation. <Br><Br>

### Rescipies for Conformal Prediction





