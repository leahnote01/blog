---
title: "Day54 ML Review - Dimensionality Reduction (5)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, dimensionalityReduction, TIL_24]
toc: true 
---

# Applying Kernal Principal Component Analysis(KPCA) to New Data Points

<img src="blog/images/2024-08-17-TIL24_Day54/EA0B6E3A-317F-4054-931E-F2787C19249E.jpeg"><br><br>

In the previous post, we considered applying KPCA to two datasets: one with half-moon shapes and the other with concentric circles. In **practical scenarios**, we often need to transform **multiple datasets**, such as training and test data, and <u>even new examples we gather after building and evaluating the model.</u> 

Remember that we can compute the following equation. An eigenvector($\alpha$) of the centered kernel matrix (not the covariance matrix), which means that those are the examples that are already projected onto the principal component axis, $v$. Thus, if we want to project a new example, $x'$, onto this principal component axis.  

<center>
    $\phi(x')^Tv$ <br><br>
</center>



KPCA (Kernel Principal Component Analysis) differs from standard PCA because it is a memory-based method. This means we need to use the original training dataset whenever we want to project new examples again. We have to calculate the pairwise RBF kernel (similarity) between each $i$th 

<br><br>

