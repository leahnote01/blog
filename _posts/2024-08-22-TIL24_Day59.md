---
title: "Day59 ML Review - Cross Validation (4)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, logisticRegression, TIL_24]
toc: true 
---

# Confusion Matrix, F1 score, and ROC Area Under the Curve (ROC AUC)

<img src="/blog/images/2024-08-22-TIL24_Day59/A590D840-2DB5-4A2A-9ADB-D19D7037B0BD.jpeg"><br><Br>





## Confusion Matrix

<font size=3pt><I>(Explanation from: https://www.evidentlyai.com/classification-metrics/confusion-matrix)</I></font><br>

> A confusion matrix is simply a square matrix that reports the count of the true positive(TP), true negative(TN), false positive(FP), and false negative(FN) predictions of the classifier, as shown below. 

<center>
  <img src="/blog/images/2024-08-22-TIL24_Day59/image-20240828161009879.png" width="50%"><Br><br>
</center>



The confusion matrix displays the number of correct and false predictions. However, absolute numbers may only sometimes be practical. We also need <u>relative metrics to compare models or track their performance over time.</u> You can derive such quality metrics directly from the confusion matrix.

**Accuracy** refers to <u>the proportion of all correct classifications</u>, regardless of whether they are positive or negative. It is mathematically defined as:

<center>
  <img src="/blog/images/2024-08-22-TIL24_Day59/image-20240828162042625.png" width="50%"><br><br>
</center>





However, accuracy can be misleading for imbalanced datasets when one class has significantly more samples. For example, let's assume we have many non-spam emails: 9100 out of 10000 are regular emails. <u>The overall model 'correctness' is heavily skewed</u> to reflect how well the model can identify those non-spam emails. The accuracy number could be more informative if we are interested in catching spam.



‍**Precision** is <u>the share of **true** positive predictions</u> in all **positive** predictions. In other words, it shows how often the model is suitable when it predicts the target class.

![image-20240828161952630](/images/2024-08-22-TIL24_Day59/image-20240828161952630.png)

Precision is an important metric <u>when the cost of false positives is high.</u> If we want to *avoid sending good emails to spam folders*, you might want to focus on precision.

**‍Recall, or true positive rate (TPR)**. Recall shows the share of true positive predictions made by the model out of all positive samples in the dataset. In other words, the recall shows how many instances of the target class the model can find.

![image-20240828162405493](/images/2024-08-22-TIL24_Day59/image-20240828162405493.png)

Recall is a helpful metric when the cost of <u>false negatives is high</u>. For example, we can optimize for recall if we do not want to miss any spam (even at the expense of falsely flagging some legitimate emails). 



### Choice of Metrics and Tradeoffs

<font size=3pt><I>Table from: https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall</I></font>

|           Metric            |                           Guidance                           |
| :-------------------------: | :----------------------------------------------------------: |
|          Accuracy           | Use as a rough indicator of model training progress/convergence for balanced datasets.For model performance, use only in combination with other metrics.Avoid for imbalanced datasets. Consider using another metric. |
| Recall (True positive rate) | Use when false negatives are more expensive than false positives. |
|     False positive rate     | Use when false positives are more expensive than false negatives. |
|          Precision          | Use when it's very important for positive predictions to be accurate. |



## F1 Score

<font size=3pt><I>(Explanation from: https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall)</I></font>

> In machine learning, the **F1 score**, also known as the F-measure, is a performance metric that measures a model's accuracy by combining its precision and recall scores into a single value. The F1 score is calculated using the harmonic mean of t<u>he precision</u> and <u>recall</u> scores, which encourages similar values for both. The F1 score ranges from 0–100%, with higher scores indicating better quality classifiers. <Br>



<br><br>

