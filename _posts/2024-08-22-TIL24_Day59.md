---
title: "Day59 ML Review - Cross Validation (4)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, logisticRegression, TIL_24]
toc: true 
---

# F1 score, Confusion Matrix, and ROC Area Under the Curve (ROC AUC)

<img src="/blog/images/2024-08-22-TIL24_Day59/A590D840-2DB5-4A2A-9ADB-D19D7037B0BD.jpeg"><br><Br>



## F1 Score

<font size=3pt><I>(Explanation from: https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall)</I></font>

> In machine learning, the **F1 score**, also known as the F-measure, is a performance metric that measures a model's accuracy by combining its precision and recall scores into a single value. The F1 score is calculated using the harmonic mean of t<u>he precision</u> and <u>recall</u> scores, which encourages similar values for both. The F1 score ranges from 0–100%, with higher scores indicating better quality classifiers. <Br>



*(Explanation from: https://www.evidentlyai.com/classification-metrics/confusion-matrix)*

### Confusion Matrix

A confusion matrix is simply a square matrix that reports the count of the true positive(TP), true negative(TN), false positive(FP), and false negative(FN) predictions of the classifier, as shown below. 

![image-20240828161009879](/images/2024-08-22-TIL24_Day59/image-20240828161009879.png)

The confusion matrix displays the number of correct and false predictions. However, absolute numbers may only sometimes be practical. To compare models or track their performance over time, you also need <u>relative metrics</u>. You can derive such quality metrics directly from the confusion matrix.

**Accuracy** refers to <u>the proportion of all correct classifications</u>, regardless of whether they are positive or negative. It is mathematically defined as:

<center>
  $\text{Accuracy} = \frac{\text{True Predictions}}{\text{All Predictions}} =\frac{TP+TN}{TP+TN+FP+FN}$ <br><br>
</center>



To calculate the F1 score, the harmonic mean of precision is used.

<center>
  <I>F1 score=2 × Precision x Recall/Precision+Recall</I><br><br>
</center>



<br><br>

