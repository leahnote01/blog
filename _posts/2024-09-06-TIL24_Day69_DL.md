---
title: "Day69 Deep Learning Lecture Review - Lecture 5"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, transformer, foundationModels, TIL_24]
toc: true 
---

# (Editing) Transformers and Foundation Models

<img src="/blog/images/2024-09-06-TIL24_Day69_DL/EB38BEDA-725B-4498-8CD8-1D0A3E6E4668.jpeg"><br><br>

> Foundation models are large-scale, pre-trained models used as the basis for a wide range of tasks in machine learning, especially in <u>Natural Language Processing (NLP), Computer Vision (CV), and other areas.</u> They are trained on diverse, extensive datasets, usually using unsupervised or self-supervised learning techniques. These models can be fine-tuned for specific tasks using smaller datasets. They are <u>huge, often containing billions of parameters, and can be applied to many different functions</u> without being re-trained completely.

Foundation models are <u>pre-trained</u> in a <u>self-supervised</u> manner, which means they learn representations without requiring labeled data. Following pre-training, they can be fine-tuned on a smaller, task-specific dataset. They mark a shift toward **building generalized AI systems** that can solve diverse problems without task-specific retraining. However, their development and deployment come with challenges, such as resource-intensive training, ethical considerations, and biases inherent in the training data.

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912180607808.png" width="90%"><br><br>
</center>




<br>

## GELU Activation Function

> **The GELU (Gaussian Error Linear Unit)** activation function is a smooth, non-linear function primarily used in neural networks. It is often used in models like BERT (Bidirectional Encoder Representations from Transformers) and other transformer-based architectures because of its <u>superior performance</u> compared to traditional activation functions such as ReLU (Rectified Linear Unit).

- Used in most transformer models, and now in many CNN and MLP architectures.
- Doesn't have a dying ReLU problem. <u>Smoother</u> Activation near zero, Probabilistic Behavior, Differentiable in all ranges, and <u>allows (small) gradient in the negative range.</u>
  - Unlike ReLU, which is piecewise linear and has discontinuities, GELU is a smooth and differentiable function, which is beneficial for optimization in deep learning models.
  - GELUâ€™s smoother behavior allows gradients to flow more easily during backpropagation, especially <u>for values near zero.</u> 
  - This reduces the chances of the model facing **"dead neurons" that can happen in ReLU (where negative values are zeroed out entirely).**<br>

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/1BF01C0A-3B38-4853-8C27-64CF9C44A6BB_1_201_a.jpeg" width="70%"><br><I><Font size="3pt">
  Image from: Kanan, C., "End-to-End Deep Learning," CSC477.01.FALL2024ASE Lecture Slides, University of Rochester, 2024.
  </Font></I><br><br><Br>
</center>




<br>

## Normalization Functions

![image-20240912182855444](/images/2024-09-06-TIL24_Day69_DL/image-20240912182855444.png)

- Layer Norm
  -  Normalizes across all features in a given layer. It applies normalization across the **channels (C)** and **merged spatial dimensions (H, W)**, for each individual sample in the mini-batch.
  - It works well <u>for non-sequential data</u> and has applications <u>in transformer architectures,</u> where mini-batch normalization isn't as effective.
- Batch Normalization
  - Normalizes over the **mini-batch samples (N)** and the **spatial dimensions (H, W)**, *but each channel (C) is normalized independently.*
  - This normalization is <u>applied per channel</u> for a mini-batch. It is commonly used in CNNs to reduce internal covariate shift, speeding up training and allowing for higher learning rates.

<br>

