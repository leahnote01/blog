---
title: "Day69 Deep Learning Lecture Review - Lecture 5"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, transformer, foundationModels, TIL_24]
toc: true 
---

# (Editing) Lecture 5: Transformers and Foundation Models

<img src="/blog/images/2024-09-06-TIL24_Day69_DL/EB38BEDA-725B-4498-8CD8-1D0A3E6E4668.jpeg"><br><br>

> Foundation models are large-scale, pre-trained models used as the basis for a wide range of tasks in machine learning, especially in <u>Natural Language Processing (NLP), Computer Vision (CV), and other areas.</u> They are trained on diverse, extensive datasets, usually using unsupervised or self-supervised learning techniques. These models can be fine-tuned for specific tasks using smaller datasets. They are <u>huge, often containing billions of parameters, and can be applied to many different functions</u> without being re-trained completely.

Foundation models are <u>pre-trained</u> in a <u>self-supervised</u> manner, which means they learn representations without requiring labeled data. Following pre-training, they can be fine-tuned on a smaller, task-specific dataset. They mark a shift toward **building generalized AI systems** that can solve diverse problems without task-specific retraining. However, their development and deployment come with challenges, such as resource-intensive training, ethical considerations, and biases inherent in the training data.

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912180607808.png" width="90%"><br><br>
</center>




<br>

## GELU Activation Function

> **The GELU (Gaussian Error Linear Unit)** activation function is a smooth, non-linear function primarily used in neural networks. It is often used in models like BERT (Bidirectional Encoder Representations from Transformers) and other transformer-based architectures because of its <u>superior performance</u> compared to traditional activation functions such as ReLU (Rectified Linear Unit).

- Used in most transformer models, and now in many CNN and MLP architectures.
- Doesn't have a dying ReLU problem. <u>Smoother</u> Activation near zero, Probabilistic Behavior, Differentiable in all ranges, and <u>allows (small) gradient in the negative range.</u>
  - Unlike ReLU, which is piecewise linear and has discontinuities, GELU is a smooth and differentiable function, which is beneficial for optimization in deep learning models.
  - GELU’s smoother behavior allows gradients to flow more easily during backpropagation, especially <u>for values near zero.</u> 
  - This reduces the chances of the model facing **"dead neurons" that can happen in ReLU (where negative values are zeroed out entirely).**<br>

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/1BF01C0A-3B38-4853-8C27-64CF9C44A6BB_1_201_a.jpeg" width="70%"><br><I><Font size="3pt">
  Image from: Kanan, C., "End-to-End Deep Learning," CSC477.01.FALL2024ASE Lecture Slides, University of Rochester, 2024.
  </Font></I><br><br><Br>
</center>




<br>

## Normalization Functions

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL/image-20240912182855444.png" width="90%"><br><br>
</center>



- Layer Norm
  -  Normalizes across all features in a given layer. It applies normalization across the **channels (C)** and **merged spatial dimensions (H, W)**, for each individual sample in the mini-batch.
  - It works well <u>for non-sequential data</u> and has applications <u>in transformer architectures,</u> where mini-batch normalization isn't as effective.
- Batch Normalization
  - Normalizes over the **mini-batch samples (N)** and the **spatial dimensions (H, W)**, *but each channel (C) is normalized independently.*
  - This normalization is <u>applied per channel for a mini-batch.</u> It is commonly used in CNNs <u>to reduce internal covariate shift,</u> speeding up training and allowing for higher learning rates.
- Group Normalization
  - <u>Splits channels into groups</u> and normalizes across each group and spatial dimensions.

- Instance Normalization: Normalizes for each sample and channel separately.
- Convolutional Weight: Shows how convolution kernels work across input and output channels.

<br>

- Layer Norm vs. Batch Norm
  - Layer Norm: Subtract <u>mean of each input vector</u> and divide by the **vector’s** standard deviation
  - Batch Norm: Subtract <u>channel mean</u> computed using all samples in batch, and then divide by <u>channel standard deviation computed using all samples in batch.</u>
  - Layer norm is invariant to batch size.
  - Layer norm tends to make training slower

<br>

### Batch Norm is a Frquent Problem.

- Batch normalization (Batch Norm) has been a popular technique in deep learning, primarily used to speed up training and stabilize neural network models. However, it also comes with some limitations and challenges

  1. Dependency on **Batch Size**: Batch Norm's performance is highly dependent on the batch size used during training. If the batch size is too small, the statistical estimates of the mean and variance may be inaccurate, leading to noisy gradients. It calculates the mean and variance of features <u>across the mini-batch</u>, and smaller batch sizes mean **less accurate estimates of these statistics, potentially hurting model performance.**

  2. Training vs. **Inference Discrepancy**: During training, batch normalization uses the statistics of the current mini-batch, but during inference (or testing), it uses <u>running estimates (moving averages) of the batch statistics</u>. These estimates might not perfectly match the real statistics at inference time. The difference between training and inference statistics can lead to a performance drop at inference time, particularly when the model is sensitive to the exact distribution of features.

  3. Not Effective **for Small Batches or Online Learning**: Batch normalization <u>struggles in settings where small batch sizes are required</u> (e.g., very large models, memory-constrained environments) or in online learning (streaming data) scenarios. When using small batch sizes, the statistics are often noisy and unstable, which reduces the effectiveness of normalization. In online learning, <u>since the data comes in a stream and mini-batch statistics are not available, applying Batch Norm becomes impractical.</u>

  4. Batch Norm Doesn't Work Well with **Recurrent Neural Networks (RNNs)**: Batch normalization is less effective in Recurrent Neural Networks (RNNs) and other sequential models. In RNNs, <u>the internal state is passed over many time steps, and normalizing these sequences using batch statistics can interfere with the temporal dependencies in the data.</u> Other normalization techniques, such as **Layer Normalization**, are typically preferred in RNNs and transformers because they work better in sequential models.

<br>

### Side Note: Layer Norm

>  **Layer Normalization (LayerNorm)** is a normalization technique used in neural networks, particularly effective <u>for models where the input is sequential,</u> such as Recurrent Neural Networks (RNNs) and Transformers. Unlike Batch Normalization (BatchNorm), which normalizes across the batch dimension, LayerNorm normalizes <u>across the features within each layer for each individual input, making it independent of the batch size.</u>

For each input instance (or sample), LayerNorm normalizes the activations across all features (neurons) in a laye<u>r by calculating the mean and variance for the entire layer’s output</u>, rather than across a mini-batch. This ensures that the activations have a **consistent distribution for each input**, which **helps stabilize and speed up the training process.**



<br><br>



## Transformers

> The **Transformer** is a deep learning model architecture introduced in the paper “Attention is All You Need” by Vaswani et al. in 2017. It revolutionized the field of Natural Language Processing (NLP) and is now the backbone of many state-of-the-art models, *including BERT, GPT, and T5.* The Transformer is unique because it is based entirely on attention mechanisms, **without relying on the recurrence** (as in Recurrent Neural Networks, RNNs) or convolutions (as in Convolutional Neural Networks, CNNs). This allows it to handle longer-range dependencies and parallelize training more efficiently.
