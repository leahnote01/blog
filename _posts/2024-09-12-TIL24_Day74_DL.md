---
title: "Day74 Deep Learning Lecture Review - Lecture 6 (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, NLP, BERT, GPT, LLM, TIL_24]
toc: true 
---

# (Editing) Large Language Model (1) - BERT, GPT,

<img src="/blog/images/2024-09-12-TIL24_Day74_DL/52666564-7D43-4606-97B3-3888A7DBC0C6_1_105_c.jpeg">



## BERT

> BERT, which stands for **Bidirectional Encoder Representations from Transformers**, is a groundbreaking deep learning model introduced by Google in 2018. It is primarily designed for Natural Language Processing (NLP) tasks and has revolutionized how machines understand and generate human language.





### 1. Key Features

- BERT takes a chunk of text (sentences, paragraphs) as input and produces an embedding for downstream tasks.
- Can fine-tune BERT and variants of BERT for classification or regression tasks.
- Can use embeddings in multi-modal systems, e.g., Visual Question Answering.

1. **Bidirectional Context Understanding**: BERT reads text in <u>both directions</u> (left-to-right and right-to-left) using the Transformer architecture, capturing a deeper, more nuanced understanding of words in context, unlike traditional models like GPT, which read text in a single direction.

2. **Transformer Architecture**: BERT is based on the Transformer model, which uses <u>self-attention mechanisms</u> to weigh the importance of each word and every other word in a sentence. This allows BERT to capture complex relationships between words. (But it is just a <u>transformer's encoder stack</u>.)

3. **Pre-trained on Large Corpora**: BERT is pre-trained on vast amounts of text data from sources like Wikipedia and BooksCorpus. Its training objectives are **masked language modeling** (predicting masked words in sentences) and **next-sentence prediction** (understanding relationships between sentences).

4. **Fine-tuning for Specific Tasks**: After <u>pre-training</u>, BERT can be fine-tuned on a specific dataset for a particular NLP task, such as sentiment analysis, question answering, text classification, etc. This transfer learning approach allows BERT to achieve state-of-the-art performance across a wide range of NLP tasks with relatively little task-specific data.<br><br>



## 2. How BERT works

1. **Tokenization**:

- **Step 1.1: Input Processing**: BERT first t<u>okenizes the input text</u>. It uses a WordPiece tokenizer, which splits words into subword units (e.g., “playing” becomes “play” + “##ing”). This allows BERT to handle out-of-vocabulary words and capture word morphology.
- **Step 1.2: Adding Special Tokens**: BERT expects unique tokens like:
  - **[CLS]**: Placed <u>at the beginning of every input sequence</u> to represent the overall sentence meaning (used for classification tasks).
  - **[SEP]**: Placed <u>at the end of each sentence</u> or to separate two sentences (used for tasks like Question Answering or Next Sentence Prediction).

![image-20241007201810921](/images/2024-09-12-TIL24_Day74_DL/image-20241007201810921.png)

Source: Lecture 6 - Large Language Model / E2E Deep Learning / Professor C. Kanan.

<br>

- **Step 1.3**: Token IDs and Input Embeddings

  : The tokenized text is converted into token IDs (numerical representation) that BERT understands. Each token is represented by:

  - **Token Embeddings**: Dense vector representations of words.
  - **Segment Embeddings**: Distinguish between different sentences.
  - **Position Embeddings**: Add positional information about the token’s location in the sequence.



### 2. **Pre-training (Masked Language Model and Next Sentence Prediction)**:

BERT is initially trained on large datasets using two tasks:

- Step 2.1: Masked Language Modeling (MLM)

  :

  - BERT randomly masks some percentage (typically 15%) of the tokens in the input sequence.
  - The task is to predict the masked words based on the context provided by the other words in the sentence.
  - This teaches BERT to understand bidirectional context, unlike traditional left-to-right language models.

- Step 2.2: Next Sentence Prediction (NSP)

  :

  - BERT is trained to predict whether two sentences are consecutive or not.
  - For each pair of sentences, 50% of the time, the second sentence is the actual next sentence, and 50% of the time, it is a random sentence from the corpus.
  - This task helps BERT understand relationships between sentences, useful for tasks like Question Answering and Natural Language Inference.





## 2. Training BERT

- BERT-base & BERT-large(12 Encoders / 24 Encoders) are trained on BooksCorpus (800M words) and English Wikipedia (2500M words).
- Trained to do language modeling, with 15% masking, and trained to classify if two text spans appeared sequentially in the training corpus. 

