---
title: "Day69 Deep Learning Lecture Review - Lecture 5"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, transformer, foundationModels, TIL_24]
toc: true 
---

# (Editing) Transformers and Foundation Models

<img src="/blog/images/2024-09-06-TIL24_Day69_DL /EB38BEDA-725B-4498-8CD8-1D0A3E6E4668.jpeg"><br><br>

> Foundation models are large-scale, pre-trained models that are used as the basis for a wide range of tasks in machine learning, especially in <u>Natural Language Processing (NLP), Computer Vision (CV), and other areas.</u> They are trained on diverse, extensive datasets, usually using unsupervised or self-supervised learning techniques. These models can be fine-tuned for specific tasks using smaller datasets. They are <u>incredibly large, often containing billions of parameters, and can be applied to many different tasks</u> without needing to be re-trained completely.

Foundation models are <u>pre-trained</u> in a <u>self-supervised</u> manner, which means they learn representations without requiring labeled data. Following pre-training, they can be fine-tuned on a smaller, task-specific dataset. They mark a shift toward **building generalized AI systems** that can solve diverse problems without task-specific retraining. However, their development and deployment come with challenges such as resource-intensive training, ethical considerations, and biases inherent in the training data.

![image-20240912180607808](/images/2024-09-06-TIL24_Day69_DL /image-20240912180607808.png)

<br>

## GELU Activation Function
