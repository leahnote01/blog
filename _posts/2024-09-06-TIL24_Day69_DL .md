---
title: "Day69 Deep Learning Lecture Review - Lecture 5"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, transformer, foundationModels, TIL_24]
toc: true 
---

# (Editing) Transformers and Foundation Models

<img src="/blog/images/2024-09-06-TIL24_Day69_DL /EB38BEDA-725B-4498-8CD8-1D0A3E6E4668.jpeg"><br><br>

> Foundation models are large-scale, pre-trained models that are used as the basis for a wide range of tasks in machine learning, especially in <u>Natural Language Processing (NLP), Computer Vision (CV), and other areas.</u> They are trained on diverse, extensive datasets, usually using unsupervised or self-supervised learning techniques. These models can be fine-tuned for specific tasks using smaller datasets. They are <u>incredibly large, often containing billions of parameters, and can be applied to many different tasks</u> without needing to be re-trained completely.

Foundation models are <u>pre-trained</u> in a <u>self-supervised</u> manner, which means they learn representations without requiring labeled data. Following pre-training, they can be fine-tuned on a smaller, task-specific dataset. They mark a shift toward **building generalized AI systems** that can solve diverse problems without task-specific retraining. However, their development and deployment come with challenges such as resource-intensive training, ethical considerations, and biases inherent in the training data.

![image-20240912180607808](/images/2024-09-06-TIL24_Day69_DL /image-20240912180607808.png)

<br>

## GELU Activation Function

> **The GELU (Gaussian Error Linear Unit)** activation function is a smooth, non-linear function primarily used in neural networks. It is often used in models like BERT (Bidirectional Encoder Representations from Transformers) and other transformer-based architectures because of its <u>superior performance</u> compared to traditional activation functions such as ReLU (Rectified Linear Unit).

- Used in most transformer models, and now in many CNN and MLP architectures.
- Doesn't have a dying ReLU problem. <u>Smoother</u> Activation near zero, Probabilistic Behavior, Differentiable in all ranges, and <u>allows (small) gradient in the negative range.</u>
  - Unlike ReLU, which is piecewise linear and has discontinuities, GELU is a smooth and differentiable function, which is beneficial for optimization in deep learning models.
  - GELUâ€™s smoother behavior allows gradients to flow more easily during backpropagation, especially <u>for values near zero.</u> 
  - This reduces the chances of the model facing **"dead neurons" that can happen in ReLU (where negative values are zeroed out entirely).**<br>

<center>
  <img src="/blog/images/2024-09-06-TIL24_Day69_DL /1BF01C0A-3B38-4853-8C27-64CF9C44A6BB_1_201_a.jpeg" width="70%"><br><I><Font size="3pt">
  Image from: Kanan, C., "End-to-End Deep Learning," CSC477.01.FALL2024ASE Lecture Slides, University of Rochester, 2024.
  </Font></I><br><br><Br>
</center>



