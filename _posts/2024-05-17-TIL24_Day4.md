---
title: "Day04 ML Statistics Review (4)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statReview, TIL_24]
toc: true
---

# Basic Mathematics Concepts (4) - Bayesian Statistics, Var-Cov Matrix

<img src ="/blog/images/2024-05-17-TIL24_Day4/69969D0B-C02B-4D43-82FD-54BC15A676C9.jpeg" alt="day04in">

<br>

## Bayesian Statistics

>Bayesian Statistics is a subset of statistics in which probability expresses a degree of belief in an event. This belief may change as new evidence is presented. Bayesian statistics uses the Bayes theorem to update the probability of a hypothesis as more evidence or information becomes available. 



In Bayesian statistics:

- **Prior Probability** represents what is initially believed before any evidence is introduced.
- **Likelihood** represents how probable new evidence is, given the prior.
- **Posterior Probability** is the updated belief after considering the new evidence. 

<br>

The mathematical formula of Bayes theorem is:
$$
P(H \mid E) = \frac{P(E \mid H) \times P(H)}{P(E)}
$$

- $P(H \mid E)$ is the <u>posterior probability</u> of the Hypothesis $H$ <u>given the evidence $E$.</u>
- $P(E \mid H)$ is the likelihood of observing the evidence $E$ <u>given that $H$ is true.</u>
- $P(H)$ is the prior probability of the hypothesis <u>before seeing the evidence.</u>
- $P(E)$ is the probability of the evidence <u>under all possible hypotheses</u>. 

<br><br>



## Var-Cov Matrix

> The **variance-covariance matrix** (often abbreviated as var-cov matrix is a key concept in statistics, particularly in the study of data distributions, linear regression, and multivariate analysis. This matrix provides a systematic way to capture the variances and covariances associated with multiple variables. 

<br>

For a set of variables $X_n$ :

- The **Variance** of a variable measures <u>the spread of its values from the mean</u>, indicating how much the values **differ from the mean**.
- The **Covariance** measures <u>how two variables change together</u>, indicating the extent to which **one variable increases as the other increases**.

<br>
$$
\begin{bmatrix}
\text{Var}(X_1) & \text{Cov}(X_1, X_2) & \dots & \text{Cov}(X_1, X_n) \\
\text{Cov}(X_2, X_1) & \text{Var}(X_2) & \dots & \text{Cov}(X_2, X_n) \\
\vdots & \vdots & \ddots & \vdots \\
\text{Cov}(X_n, X_1) & \text{Cov}(X_n, X_2) & \dots & \text{Var}(X_n)
\end{bmatrix}
$$

<br>

The Var-Cov Matrix for $n$ variables is a $n \times n$ symmetric matrix where:

* The diagonal entries are the variances of each variable.
* The off-diagonal entries are the covariances between pairs of variables.

<br><br>

The covariance matrix of a random vector $X$ is defined as: <br><br>
$$
cov(X) = E[(X-E[X])(X-E[X])^T]
$$
<br><br>

Where:

- $X$ is a matrix whose columns represent different variables (or random vectors).
- $E[X]$ is the expected value (mean) of $X$, usually calculated column-wise.
- $(X-E[X])$ represents the deviation of $X$ from its mean.
- $T$ denotes its transpose opration.

<br>

### **Calculation:**

1. Calculate $X-E[X]$: Each element of $X$ is subtracted by the mean of its respective column to get the deviation of each element from the mean.
2. **Transpose** and Matrix Multiplication: $(X-E[X])$ is multiplied by its transpose $(X-E[X])^T$. This operation results in a square matrix where each element represents the covariance between different pairs of variables.
3. Element-wise Computation in the Matrix: The matrix $E[(X-E[X])(X-E[X]^T)]$ is computed by taking the dot product of the rows of $X-E[X]$â€‹ with the columns of its transpose. This operation aggregates the product of deviations for each pair of variables across all observations. 
4. Taking Expectations (average): The expected value of the resulting matrix from step 3 is computed. Since expectation in the context of sample covariance is often approximated by the average, we divide the matrix element by the number of observations (or one less than the number of observations in some definitions, particularly for an unbiased estimate of sample covariance).

<br><br>

<img src= "/blog/images/2024-05-17-TIL24_Day4/image-20240517151032965.png" alt="covmatrix">

Therefore, for instance, the final matrix shown is:

- Diagonal entries (e.g., 504, 360, 720) represent the <u>variance of each individual variables</u>
- Off-diagonal entries (e.g., 360, 180, 0) represent the covariance between different pairs of variables, indicating how much two variables change together. **A covariance of zero suggests no linear relationship.**



<img src="/blog/images/2024-05-17-TIL24_Day4/C114EA6B-AB8A-48C1-9564-0AA182B5CDAA.jpeg" alt="day04out">

