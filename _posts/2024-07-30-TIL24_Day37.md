---
title: "Day37 ML Review - Decision Tree (3) & Random Forest (1)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview,classifier,decisionTree,TIL_24]
toc: true 
---

# Building a Decision Tree & Random Forest

<img src="/blog/images/2024-07-30-TIL24_Day37/A666E29A-ABD5-45CF-B051-FAED4B514823_1_105_c.jpeg">

<br><br>

Decision trees <u>can build complex decision boundaries by dividing the feature space into rectangles</u>. However, we must be careful since the deeper the decision tree, the more complex the decision boundary becomes, *which can easily result in overfitting*. We will now train a decision tree with a maximum depth of 4, using GIni impurity as a criterion for impurity. 

```python
from sklearn.tree import DecisionTreeClassifier
tree_model = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)
tree_model.fit(X_train, y_train)
```

* Parameters
  * `criterion`: specifies the function to measure the quality of a split.
    * `gini`, `entropy` can be used
  * `max_depth`: specifies the maximum depth of the tree. Limiting the depth of the tree helps prevent overfitting.
    * If `None`, the nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.
  * `min_samples_split`: The minimum number of samples required to split an internal node.
    * Any positive integer (default is 2).
    * A float between 0 and 1, represents a fraction of the samples.



Nicer visualizations can be obtained by model `tree` as below.

```python
from sklearn import tree
tree.plot_tree(tree_model)
```



<br>

## Combining Multiple Decision Trees via Random Forest

Let's talk about the **random forest** algorithm, which is based on decision trees and is known for its scalability and ease of use. A random forest is essentially a collection of decision trees (other methods will be discussed later). The concept behind a random forest is <u>to combine multiple (deep) decision trees</u>, each of which may have <u>high variance on its own, to create a more reliable model with better generalization performance and reduced risk of overfitting.</u>



### Following Steps

