---
title: "Day52 ML Review - Dimensionality Reduction (3)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [mlReview, classifier, dimensionalityReduction, TIL_24]
toc: true 
---

# Nonlinear Mappings with Kernel Principal Component Analysis

<img src="/blog/images/2024-08-13-TIL24_Day51/DAC0D3A8-1B6F-434F-9F4A-3ECBF74D915C.jpeg"><br><br>

> Kernel Principal Component Analysis (Kernel PCA) is an extension of Principal Component Analysis (PCA) that allows for the transformation of data that is not linearly separable. Unlike traditional PCA, which only handles linear relationships between features, **Kernel PCA can capture and extract features in a higher-dimensional space** where the data may become linearly separable.



In order to address complex, nonlinear problems, we employ a technique that involves mapping the data onto a **higher-dimensional feature space** <u>where the classes can be separated in a linear manner</u>. This process entails transforming the original examples, denoted $x \in \mathbb{R}^d $, onto a new $k$-dimensional subspace, where $k$ is significantly greater than $d$. To achieve this transformation, we utilize a nonlinear mapping function, represented by:

<center>
   $\phi : \mathbb{R}^d \rightarrow \mathbb{R}^k \ \ (k \gg d)$ <br><br>
</center>

Essentially, $\phi$ is a function that generates nonlinear combinations of the original features, thereby facilitating the translation of the initial $d$-dimensional dataset into a larger, $k$-dimensional feature space.

Put simply, we utilize Kernel Principal Component Analysis (KPCA) to nonlinearly map our data into a higher-dimensional space. In this augmented space, we then employ standard Principal Component Analysis (PCA) to reproject the data onto a lower-dimensional space, allowing us to separate the examples using a linear classifier, assuming they can be separated by density in the input space.

However, one drawback of this method is its high computational cost. To address this issue, we make use of the kernel trick, which enables us to compute the similarity between two high-dimensional feature vectors in the original feature space.
