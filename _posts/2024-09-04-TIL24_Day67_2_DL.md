---
title: "Day67 (2) Deep Learning Review - CNN (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, CNN, TIL_24]
toc: true 
---

# Convolutional Neural Network (1)

<img src="/blog/images/2024-09-04-TIL24_Day67_DL/E078B106-915B-47B6-B14F-816FCBFA5E13.jpeg"><br><br>

## Basic Concepts of CNNs

>  Certain neuron networks, such as CNNs, can automatically learn the features from raw data that are most useful for a particular task.

<br>

### Understanding CNNs and Feature Hierarchies

A Convolutional Neural Network (CNN) analyzes an input image <u>by breaking it down into smaller sections of pixels</u> and then computing **feature maps** based on these local patches.

<center>
  <img src="/blog/images/2024-09-04-TIL24_Day67_2_DL/image-20240905204902661.png" width="80%"><br><br>
</center>



This patch of pixels is known as the local receptive field. CNNs typically excel in image-related tasks, mainly due to two essential ideas:

- Sparse connectivity: A single element in the feature map is only connected to a small patch of pixels.
- Parameter sharing: The same weights are used for different input image patches. 

<br>

<center>
  <img src="/blog/images/2024-09-04-TIL24_Day67_2_DL/image-20240905210802353.png" width="90%"><br>
  <font size="3pt"><I>(https://encord.com/blog/convolutional-neural-networks-explained/)</I></font><br><br>
</center>





When we switch from a traditional, fully connected Multilayer Perceptron (MLP) to a convolutional layer, <u>a significant reduction in the network's weight (parameter) count is observed</u>. This modification improves the network's capacity to capture crucial features in the data. In the context of image data, it is reasonable <u>to assume that nearby pixels are more relevant to each other than pixels that are far apart</u>.

<br>

Convolutional Neural Networks (CNNs) are structured with multiple layers of **convolutional** and **subsampling** operations, **followed by one or more fully connected layers at the end**. The fully connected layers essentially form a Multi-Layer Perceptron (MLP), where each input unit, $i$, is connected to every output unit, $j$, using a weight denoted as $W_{ij}$.

<br>

The subsampling layers, also known as **pooling layers**, do not have any learnable parameters. This means they do not have adjustable weights or bias units during training. In contrast, both the convolutional and fully connected layers have weights and biases that are modified and optimized as the network is trained.

<br><br>

### The Architecture of CNNs

> The architecture of a Convolutional Neural Network (CNN) is designed to take advantage of the 2D structure of an input image. This is achieved through <u>convolutional layers, pooling layers, and fully connected layers.</u> Each type of layer performs a distinct function and contributes to the network's ability to perform complex image recognition tasks.

<br>

Image and explanations from: [Rubber-Tree blog](https://rubber-tree.tistory.com/116)

![image-20240906170957740](/images/2024-09-04-TIL24_Day67_2_DL/image-20240906170957740.png)



Convolutional Neural Networks (CNNs) are adept at leveraging the layered structure of an image to identify and extract features. They accomplish this by employing **Convolution Layers**, which apply different filters to the input image to identify and extract features, and **Pooling Layers**, which significantly reduce the data's dimensionality. For classification purposes, CNNs make use of the **Fully Connected Layer**. <u>Following feature extraction, the image is flattened before being fed into the Fully Connected Layer to finalize the classification process.</u>

<br><br>

### Convolution Layers

![image-20240906180257448](/images/2024-09-04-TIL24_Day67_2_DL/image-20240906180257448.png)

<u>Color image</u> data is typically represented as <u>a three-dimensional tensor</u> (3D tensor), allowing for encapsulation of the width, height, and channel dimensions. For instance, the color channels of an image in the RGB format consist of Red (R), Green (G), and Blue (B). <br><br>

#### Filter Application

The application of filters is a fundamental technique in image processing, particularly in the context of Convolutional Neural Networks (CNNs). Filters, which are <u>small matrices of weights</u>, are used to highlight or extract specific features from images by convolving these filters with the image data.

<br>

#### How Filters Work:

- **Convolution Process**: A filter (also known as a **kernel**) <u>slides over the image, and at each position, it performs element-wise multiplication with the part of the image it covers.</u> The results of these multiplications are then summed up to produce <u>a single output pixel in the feature map</u>. This operation is repeated across the entire image.
- **Feature Enhancement**: Each filter is designed to detect a specific type of feature at various positions in the image. For instance, some filters might detect edges, others detect texture, and some may highlight changes in color.<br>

![image-20240906174926229](/images/2024-09-04-TIL24_Day67_2_DL/image-20240906174926229.png)

- **The example**: The example above applies a 3x3 filter to a 4x4x1 segment of the image, resulting in a 2x2x1 tensor image.

- **Bias**: After the filter can be applied to an image, a bias term is typically added to the result of the convolution before it is passed <u>through a non-linear activation function</u> like ReLU (Rectified Linear Unit). This bias term allows for <u>an additional degree of freedom</u> in the model, enabling the activation function to be shifted to the right or left, which can be crucial for the learning process.

<br>

#### Stride

When applying a filter to an image during a convolution operation, the stride is the parameter <u>that controls how the filter moves across the imag</u>e. The stride determines the amount of shift the filter makes over the image and the final size of the output image after the stride has been applied.

**Example of Stride Usage:**

- **Stride = 1:** The filter moves <u>one pixel at a time</u>. This results in a <u>more detailed feature ma</u>p, capturing more information as the filter overlaps significantly with its previous position.
- **Stride = 2:** The filter jumps <u>two pixels at a time</u>. This results in a feature map that is <u>smaller in size,</u> effectively reducing the spatial dimensions more quickly and capturing broader features with less overlap between positions.

<br>

#### Padding

When we perform convolution operations, we often use **padding** <u>to maintain the dimensions of the image</u>. Specifically, when we apply a smaller filter to an image, <u>we may lose information around the edges</u>, resulting in a smaller output feature map. To prevent this, we can add padding around the image before applying the convolution.

Padding involves adding <u>layers of zeros (zero-padding) around the original image dimensions.</u> This allows the filter to be applied even at the edges of the image without reducing the size of the output. <u>By using padding, the output feature map can maintain the same dimensions as the input image</u>. For example, by applying one layer of zero-padding, a 4x4 image effectively becomes a 6x6 image, allowing the 4x4 filter to be applied to maintain the original image size.

Zero-padding not only helps <u>preserve</u> the dimensions of the image but also enables the model <u>to learn features at the edges</u> that would otherwise be lost or diminished.

<br>

#### Pooling

Before an image's feature map reaches the fully connected layer, where the final classification is performed, it's essential <u>to condense the image features.</u> This condensation is achieved by the pooling layer, which significantly r<u>educes the dimensions of the feature map</u>, allowing the network to manage computational resources more effectively and helping to prevent overfitting. The pooling layer operates by summarizing the features present in regions of the feature map.

There are several types of pooling:

- Max Pooling
- Average Pooling
- Min Pooling

In CNNs, <u>Max Pooling is commonly used</u>. This method involves selecting the largest element from the region of the feature map covered by the filter. For instance, a 2x2 Max Pooling operation with a stride of 2 reduces the size of the feature map by selecting the maximum value from every 2x2 block of the feature map, effectively downsizing it.
