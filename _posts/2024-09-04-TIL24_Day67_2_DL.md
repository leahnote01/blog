---
title: "Day67 (2) Deep Learning Review - CNN (1)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, neuralNetwork, CNN, TIL_24]
toc: true 
---

# Convolutional Neural Network (1)

<img src="/blog/images/2024-09-04-TIL24_Day67_DL/E078B106-915B-47B6-B14F-816FCBFA5E13.jpeg"><br><br>

## Basic Concepts of CNNs

>  Certain neuron networks, such as CNNs, can automatically learn the features from raw data that are most useful for a particular task.

<br>

### Understanding CNNs and Feature Hierarchies

A Convolutional Neural Network (CNN) analyzes an input image <u>by breaking it down into smaller sections of pixels</u> and then computing **feature maps** based on these local patches.

<center>
  <img src="/blog/images/2024-09-04-TIL24_Day67_2_DL/image-20240905204902661.png" width="80%"><br><br>
</center>



This patch of pixels is known as the local receptive field. CNNs typically excel in image-related tasks, mainly due to two essential ideas:

- Sparse connectivity: A single element in the feature map is only connected to a small patch of pixels.
- Parameter sharing: The same weights are used for different input image patches. 

<br>

<center>
  <img src="/blog/images/2024-09-04-TIL24_Day67_2_DL/image-20240905210802353.png" width="90%"><br>
  <font size="3pt"><I>(https://encord.com/blog/convolutional-neural-networks-explained/)</I></font><br><br>
</center>





When we switch from a traditional, fully connected Multilayer Perceptron (MLP) to a convolutional layer, <u>a significant reduction in the network's weight (parameter) count is observed</u>. This modification improves the network's capacity to capture crucial features in the data. In the context of image data, it is reasonable <u>to assume that nearby pixels are more relevant to each other than pixels that are far apart</u>.

<br>

Convolutional Neural Networks (CNNs) are structured with multiple layers of **convolutional** and **subsampling** operations, **followed by one or more fully connected layers at the end**. The fully connected layers essentially form a Multi-Layer Perceptron (MLP), where each input unit, $i$, is connected to every output unit, $j$, using a weight denoted as $W_{ij}$.

<br>

The subsampling layers, also known as **pooling layers**, do not have any learnable parameters. This means they do not have adjustable weights or bias units during training. In contrast, both the convolutional and fully connected layers have weights and biases that are modified and optimized as the network is trained.

<br><br>

### The Architecture of CNNs

> The architecture of a Convolutional Neural Network (CNN) is designed to take advantage of the 2D structure of an input image. This is achieved through <u>convolutional layers, pooling layers, and fully connected layers.</u> Each type of layer performs a distinct function and contributes to the network's ability to perform complex image recognition tasks.

<br>

Image and explanations from: [Rubber-Tree blog](https://rubber-tree.tistory.com/116)

![image-20240906170957740](/images/2024-09-04-TIL24_Day67_2_DL/image-20240906170957740.png)



Convolutional Neural Networks (CNNs) are adept at leveraging the layered structure of an image to identify and extract features. They accomplish this by employing **Convolution Layers**, which apply different filters to the input image to identify and extract features, and **Pooling Layers**, which significantly reduce the data's dimensionality. For classification purposes, CNNs make use of the **Fully Connected Layer**. <u>Following feature extraction, the image is flattened before being fed into the Fully Connected Layer to finalize the classification process.</u>

<br><br>

### Convolution Layers

![image-20240906180257448](/images/2024-09-04-TIL24_Day67_2_DL/image-20240906180257448.png)

<u>Color image</u> data is typically represented as <u>a three-dimensional tensor</u> (3D tensor), allowing for encapsulation of the width, height, and channel dimensions. For instance, the color channels of an image in the RGB format consist of Red (R), Green (G), and Blue (B). <br><br>

#### Filter Application

The application of filters is a fundamental technique in image processing, particularly in the context of Convolutional Neural Networks (CNNs). In a single convolution layer, **there are as many filters as the number of channels** in the input image. The output image of the convolution layer is created by applying the filter assigned to each channel.

<br>

#### How Filters Work:

- **Convolution Process**: A filter (also known as a **kernel**) <u>slides over the image, and at each position, it performs element-wise multiplication with the part of the image it covers.</u> The results of these multiplications are then summed up to produce <u>a single output pixel in the feature map</u>. This operation is repeated across the entire image.

<center>
  <img src="/blog/images/2024-09-04-TIL24_Day67_2_DL/image-20240906174926229.png" width="80%"><br><br>
</center>



The example above applies a 3x3 filter to a 4x4x1 segment of the image, resulting in a 2x2x1 tensor image.



- **Bias**: After the filter can be applied to an image, a bias term is typically added to the result of the convolution before it is passed <u>through a non-linear activation function</u> like ReLU (Rectified Linear Unit). This bias term allows for <u>an additional degree of freedom</u> in the model, enabling the activation function to be shifted to the right or left, which can be crucial for the learning process.

<br>

#### Stride

![image-20240906190653661](/images/2024-09-04-TIL24_Day67_2_DL/image-20240906190653661.png)

When applying a filter to an image during a convolution operation, the stride is the parameter <u>that controls how the filter moves across the imag</u>e. The stride determines **the amount of shift the filter makes over the image** and **the final size of the output image** after the stride has been applied.

- Example of Stride Usage:
  - **Stride = 1**: The filter moves <u>one pixel at a time</u>. This results in a <u>more detailed feature ma</u>p, capturing more information as the filter overlaps significantly with its previous position.
  - **Stride = 2**: The filter jumps <u>two pixels at a time</u>. This results in a feature map that is <u>smaller in size,</u> effectively reducing the spatial dimensions more quickly and capturing broader features with less overlap between positions.

<br>

#### Padding

When we perform convolution operations, we often use **padding** to <u>maintain the image's dimensions</u>.  Specifically, when we apply a smaller filter to an image, <u>we may lose information around the edges, resulting in a smaller output feature map.</u> To prevent this, we can add padding around the image before applying the convolution.

Padding involves adding <u>layers of zeros (zero-padding) around the original image dimensions.</u> This allows the filter to be applied even at the edges of the image without reducing the size of the output. <u>By using padding, the output feature map can maintain the same dimensions as the input image</u>. For example, by applying one layer of zero-padding, a 4x4 image effectively becomes a 6x6 image, allowing the 4x4 filter to be applied to maintain the original image size.

Zero-padding not only helps <u>preserve</u> the image's dimensions but also enables the model <u>to learn features at the edges</u> that would otherwise be lost or diminished.

<br>

#### Pooling

Before an image's feature map reaches the fully connected layer, where the final classification is performed, it's essential <u>to condense the image features.</u> This condensation is achieved by the pooling layer, which significantly r<u>educes the dimensions of the feature map</u>, allowing the network to manage computational resources more effectively and helping to prevent overfitting. The pooling layer operates by summarizing the features present in regions of the feature map.

There are several types of pooling:

- Max Pooling
- Average Pooling
- Min Pooling

In images, we can say that there is a <u>high degree of similarity between neighboring pixels</u>, **creating patterns across the image**. This similarity allows images to be represented not only at the individual pixel level but also at the level of <u>selected areas with specific features or characteristics.</u> The pooling layer is specifically designed to take advantage of this characteristic of image data. For example, in max-pooling, the highest value within a given area is selected to be the representative value for that area. <u>This process helps to capture the most important features within that area.</u> By incorporating these pooling layers, convolutional neural networks (CNNs) are endowed with a range of advantageous capabilities, allowing for efficient and effective image analysis and recognition.

- **Stability**: Within a specific area of selection, no matter how the pixels shift or rotate, the output from the pooling layer stays the same. This stability is really helpful because it <u>reduces the impact of changes like moving or rotating parts of an image on CNN's output</u>. It keeps the network's performance strong, even if the input images change a lot.
- **Reducing size**: The size of the images that CNN needs to process is <u>significantly reduced</u>, thus the number of model parameters in the artificial neural network also greatly <u>decreases</u>. Therefore, by using a pooling layer, the training time for CNN can be significantly saved, and the problem of overfitting can also be somewhat alleviated.

<br>
