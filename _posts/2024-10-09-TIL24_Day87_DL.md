---
title: "Day87 Deep Learning Lecture Review - Lecture 8 (2) & 9"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, LLM, NLP, TIL_24]
toc: true 
---

# Large Language Model - Fine-Tuning LLMs (2), 



<br><Br>

#### Speeding Up Self-Attention

- Grouped-Query Attention
  - The idea behind Grouped-Query Attention is <u>to group multiple queries together and compute attention</u> for these grouped queries, instead of performing attention for each query independently.
  
    ![image-20241009102342601](/images/2024-10-09-TIL24_Day87_DL/image-20241009102342601.png)
  
- Mixture-of-Expert(MoE) Models
  
  ![image-20241009102525802](/images/2024-10-09-TIL24_Day87_DL/image-20241009102525802.png) Source: https://huggingface.co/blog/moe
  
  - MoE (Mixture of Experts) models are a type of neural network architecture that dynamically selects and activates **different subsets of "expert" sub-models** for processing different parts of an input.
  - By selectively activating specific experts, MoE models can achieve high performance on diverse tasks.
  - MoE Layer
    - In a MoE Layer, for each input the router selects
      only one expert in the layer to perform computations.
    - Router determines which network to select, based on the selection made by a Gating Network.
  - Gating Network
    - The gating function typically outputs a sparse selection of experts, meaning only a small number of experts are activated for each input, minimizing computation
  - Sparse Activation
    - We want to pick just a subset of experts, so most
      have zero weight and donâ€™t need to be computed.
    - Only a few experts are chosen to process each input at a time.
  
  
  
  
  
  - 
  
    - 
  
    
  
    
  
    
  
    
