---
title: "Day02 ML Statistics Review"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statreview, TIL_24]
toc: true

---

# Basic Mathematics Concepts - Norm (2), Eigenvectors & Eigenvalues



<img src="/blog/images/2024-05-15-TIL24_Day2/0515in.jpeg" alt="0515in">



### Type of Norm

> The L1 Norm & L2 Norm are both ways to measure the size or length of a vector, and they are frequently used in various contexts with mathematics, computer science, and machine learning. 



- L1 Norm
  - The L1 norm of a vector (<u>Manhattan Norm</u>)
  - The Sum of **Absolute Value**
  - Can be visualized as the distance you would travel between two points in a grid-like path (like navigating city blocks), which accounts for its alternative name as the "Manhattan distance.â€



- L2 Norm
  - The L2 norm (<u>Euclidean Norm</u>)
  - The Sum of **Squares** of vectorâ€™s element
  - Represents <u>the straight-line distance from the origin to the point</u> in Euclidean space described by the vector. It corresponds to the length of the hypotenuse of a right-angled triangle, extending this concept to n-dimensional space.



Later, we will discuss the application of these norms further. FYI, in machine learning and computer science, the choice between L1 and L2 norms often depends on the specific requirements of the application, such as the need for robustness against outliers, preference for sparsity, or the mathematical properties required **by the optimization algorithm.**





#### **Eigenvector and eigenvalue**

> Eigenvectors and eigenvalues are fundamental concepts in linear algebra that have extensive applications across various fields, including physics, engineering, and machine learning, <u>particularly in methods like Principal Component Analysis (PCA) and spectral clustering.</u>

ï»¿![latexit-0.pdf](blob:file:///a8e13732-8e10-4e4c-ba78-ccf4ac6919b3)ï»¿ï»¿

- An eigenvalue is a scalar associated with a linear transformation represented by a matrix. It quantifies the factor by which the magnitude of an eigenvector is scaled during the transformation. 

  For a given square matrix ğ´, a scalar ğœ† is an eigenvalue if there exists a non-zero vector ğ‘£ such that: 
  ğ´ğ‘£ = ğœ†ğ‘£. Applying the matrix ğ´ to the vector ğ‘£ results in a new vector that is a scalar multiple of ğ‘£, where ğœ† is the scalar. 

  (í–‰ë ¬ Aë¥¼ ì„ í˜•ë³€í™˜ìœ¼ë¡œ ë´¤ì„ ë•Œ, ì„ í˜•ë³€í™˜ Aì— ì˜í•œ ë³€í™˜ ê²°ê³¼ê°€ ìê¸° ìì‹ ì˜ ìƒìˆ˜ë°°ê°€ ë˜ëŠ” 0ì´ ì•„ë‹Œ ë²¡í„°ë¥¼ ê³ ìœ ë²¡í„°(eigenvector)ë¼ í•˜ê³  ì´ ìƒìˆ˜ë°° ê°’ì„ ê³ ìœ ê°’(eigenvalue)ë¼ í•œë‹¤. ì¦‰, n x n ì •ë°©í–‰ë ¬(ê³ ìœ ê°’, ê³ ìœ ë²¡í„°ëŠ” ì •ë°©í–‰ë ¬ì— ëŒ€í•´ì„œë§Œ ì •ì˜ëœë‹¤) Aì— ëŒ€í•´ A**v** = Î»**v**ë¥¼ ë§Œì¡±í•˜ëŠ” 0ì´ ì•„ë‹Œ ì—´ë²¡í„° **v**ë¥¼ ê³ ìœ ë²¡í„°, ìƒìˆ˜ Î»ë¥¼ ê³ ìœ ê°’ì´ë¼ ì •ì˜í•œë‹¤.
  from https://darkpgmr.tistory.com/105)

  

- An eigenvector v is a non-zero vector of a linear transformation A that does not change its direction when this linear transformation A is applied to it.
  (ê³ ìœ  ë²¡í„°ë¥¼ ì„¤ëª…í•´ë³´ì. ê±°ì˜ ëª¨ë“  ë²¡í„°ë“¤ì€ Aì— ì˜í•´ì„œ ê³±í•´ì§ˆ ë•Œ, ë°©í–¥ì„ ë°”ê¾¼ë‹¤. íŠ¹ì •í•œ ì˜ˆì™¸ì ì¸ ë²¡í„° xëŠ” Axì™€ ë™ì¼í•œ ë°©í–¥ì´ë‹¤. ê·¸ëŸ¬í•œ ë²¡í„°ë“¤ì„ ê³ ìœ ë²¡í„°Eigenvectorë¼ê³  í•œë‹¤. Aë¥¼ ê³ ìœ ë²¡í„°ì— ê³±í•˜ê³ , ê·¸ë ‡ê²Œ ê³±í•´ì§„ ë²¡í„° AxëŠ” ê¸°ì¡´ì˜ xì— Î»ë¥¼ ê³±í•œ ê²ƒì´ ëœë‹¤.) 



<img src="/blog/images/2024-05-15-TIL24_Day2/image-20240516122853321.png" alt="image-20240516122853321">



* **Application and Importance**
  * **PCA (Principal Component Analysis)**: Eigenvectors are used to determine the directions of maximum variance in high-dimensional data, with the corresponding eigenvalues indicating the magnitude of these variances. This method is fundamental in reducing dimensions while retaining as much variability as possible.



