---
title: "Day02 ML Statistics Review"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statreview, TIL_24]
toc: true

---

# Basic Mathematics Concepts - Norm (2), Eigenvectors & Eigenvalues



<img src="/blog/images/2024-05-15-TIL24_Day2/0515in.jpeg" alt="0515in">



### Type of Norm

> The L1 Norm & L2 Norm are both ways to measure the size or length of a vector, and they are frequently used in various contexts with mathematics, computer science, and machine learning. 



- L1 Norm
  - The L1 norm of a vector (<u>Manhattan Norm</u>)
  - The Sum of **Absolute Value**
  - Can be visualized as the distance you would travel between two points in a grid-like path (like navigating city blocks), which accounts for its alternative name as the "Manhattan distance.”



- L2 Norm
  - The L2 norm (<u>Euclidean Norm</u>)
  - The Sum of **Squares** of vector’s element
  - Represents <u>the straight-line distance from the origin to the point</u> in Euclidean space described by the vector. It corresponds to the length of the hypotenuse of a right-angled triangle, extending this concept to n-dimensional space.



Later, we will discuss the application of these norms further. FYI, in machine learning and computer science, the choice between L1 and L2 norms often depends on the specific requirements of the application, such as the need for robustness against outliers, preference for sparsity, or the mathematical properties required **by the optimization algorithm.**





#### **Eigenvector and eigenvalue**

> Eigenvectors and eigenvalues are fundamental concepts in linear algebra that have extensive applications across various fields, including physics, engineering, and machine learning, <u>particularly in methods like Principal Component Analysis (PCA) and spectral clustering.</u>

﻿![latexit-0.pdf](blob:file:///a8e13732-8e10-4e4c-ba78-ccf4ac6919b3)﻿﻿

- An eigenvalue is a scalar associated with a linear transformation represented by a matrix. It quantifies the factor by which the magnitude of an eigenvector is scaled during the transformation. 

  For a given square matrix 𝐴, a scalar 𝜆 is an eigenvalue if there exists a non-zero vector 𝑣 such that: 
  𝐴𝑣 = 𝜆𝑣. Applying the matrix 𝐴 to the vector 𝑣 results in a new vector that is a scalar multiple of 𝑣, where 𝜆 is the scalar. 

  (행렬 A를 선형변환으로 봤을 때, 선형변환 A에 의한 변환 결과가 자기 자신의 상수배가 되는 0이 아닌 벡터를 고유벡터(eigenvector)라 하고 이 상수배 값을 고유값(eigenvalue)라 한다. 즉, n x n 정방행렬(고유값, 고유벡터는 정방행렬에 대해서만 정의된다) A에 대해 A**v** = λ**v**를 만족하는 0이 아닌 열벡터 **v**를 고유벡터, 상수 λ를 고유값이라 정의한다.
  from https://darkpgmr.tistory.com/105)

  

- An eigenvector v is a non-zero vector of a linear transformation A that does not change its direction when this linear transformation A is applied to it.
  (고유 벡터를 설명해보자. 거의 모든 벡터들은 A에 의해서 곱해질 때, 방향을 바꾼다. 특정한 예외적인 벡터 x는 Ax와 동일한 방향이다. 그러한 벡터들을 고유벡터Eigenvector라고 한다. A를 고유벡터에 곱하고, 그렇게 곱해진 벡터 Ax는 기존의 x에 λ를 곱한 것이 된다.) 



<img src="/blog/images/2024-05-15-TIL24_Day2/image-20240516122853321.png" alt="image-20240516122853321">



* **Application and Importance**
  * **PCA (Principal Component Analysis)**: Eigenvectors are used to determine the directions of maximum variance in high-dimensional data, with the corresponding eigenvalues indicating the magnitude of these variances. This method is fundamental in reducing dimensions while retaining as much variability as possible.



