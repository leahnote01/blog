---
title: "Day02 Basic Mathematics Review (2)"
layout: single
classes: wide
categories: TIL_24
typora-root-url: ../
tag: [statReview, TIL_24]
toc: true

---

# Norm(2), Eigenvectors & Eigenvalues



<img src="/blog/images/2024-05-15-TIL24_Day2/0515in.jpeg" alt="0515in">



## Type of Norm

> The L1 Norm & L2 Norm are both ways to measure the size or length of a vector, and they are frequently used in various contexts with mathematics, computer science, and machine learning. 



- ### L1 Norm
  
  - The L1 norm of a vector (<u>Manhattan Norm</u>)
  - The Sum of **Absolute Value**
  - It can be visualized as the distance you would travel between two points in a grid-like path (like navigating city blocks), which accounts for its alternative name, the "Manhattan distance.â€



- ### L2 Norm
  
  - The L2 norm (<u>Euclidean Norm</u>)
  - The Sum of **Squares** of the vectorâ€™s element
  - Represents <u>the straight-line distance from the origin to the point</u> in Euclidean space described by the vector. It corresponds to the length of the hypotenuse of a right-angled triangle, extending this concept to n-dimensional space.



Later, we will discuss the application of these norms further. FYI, in machine learning and computer science, the choice between L1 and L2 norms often depends on the application's specific requirements, such as the need for robustness against outliers, preference for sparsity, or the mathematical properties required **by the optimization algorithm.**



<br><br>

## **Eigenvector and eigenvalue**

> Eigenvectors and eigenvalues are fundamental concepts in linear algebra that have extensive applications across various fields, including physics, engineering, and machine learning, <u>particularly in methods like Principal Component Analysis (PCA) and spectral clustering.</u>



- An eigenvalue is a **scalar** associated with a linear transformation represented by a matrix. It quantifies the factor by which the magnitude of an eigenvector is scaled during the transformation. 

  For a given square matrix ğ´, a scalar ğœ† is an eigenvalue if there exists a non-zero vector ğ‘£ such that: 
  ğ´ğ‘£ = ğœ†ğ‘£. Applying the matrix ğ´ to the vector ğ‘£ results in a new vector that is a scalar multiple of ğ‘£, where ğœ† is the scalar. 

  

- An eigenvector v is a non-zero vector of a linear transformation A that does not change its direction when this linear transformation A is applied to it.
  
- That is, <u>after a linear transformation,</u> eigenvectors are **vectors that remain parallel** **(do not change their direction)** by an $n \times n$ square matrix $A$â€‹, and eigenvalues represent **the degree of change in length.**



- Geometrically, the eigenvectors of the matrix (linear transformation) $A$ represent direction vectors whose direction is preserved and only scale is changed by the linear transformation $A$, and the eigenvalues represent the degree of scale change of those eigenvectors.


<br>

<img src="/blog/images/2024-05-15-TIL24_Day2/image-20240516122853321.png" alt="image-20240516122853321">

<br>

* **Application and Importance**
  * **PCA (Principal Component Analysis)**: Eigenvectors are used to determine the *directions of maximum variance in high-dimensional data*, with the corresponding eigenvalues indicating *the magnitude of these variances*. This method is fundamental in reducing dimensions while retaining as much variability as possible.

<br><br>

### *Reference in Korean*

$Ax = \lambda x$ ì‹ì—ì„œ $\lambda$ëŠ” ê³ ìœ ê°’eigenvalues, $x$ëŠ” ê³ ìœ ë²¡í„°eigenvectorsë¥¼ ê°ê° ë‚˜íƒ€ë‚´ë©°, $n \times n$ ì •ë°©í–‰ë ¬ $A$ì— ì˜í•´ ì„ í˜• ë³€í™˜Linear Transformation ì„ ì‹œì¼œë„ ë³€í™˜ ì „ê³¼ í›„ê°€ í‰í–‰í•œparallel ë²¡í„°ë¥¼ ê³ ìœ ë²¡í„°eigenvectors, ê·¸ë¦¬ê³  ê¸¸ì´ ë³€í™”ëŸ‰ì˜ ì •ë„ë¥¼ ë‚˜íƒ€ë‚¸  ê²ƒì„ ê³ ìœ ê°’ì´ë¼ í•œë‹¤.  

Aí–‰ë ¬ì—ì„œ ê³±í•´ì§ˆ ë•Œ, ê±°ì˜ ëª¨ë“  ë²¡í„°ë“¤ì€ ë°©í–¥ì„ ë°”ê¾¼ë‹¤. íŠ¹ì •í•œ ì˜ˆì™¸ì ì¸ ë²¡í„° $x$ëŠ” $Ax$ì™€ ë™ì¼í•œ ë°©í–¥ì´ë‹¤. ê·¸ëŸ¬í•œ ë²¡í„°ë“¤ì„ ê³ ìœ ë²¡í„°eigenvectorsë¼ê³  í•œë‹¤. $A$ ë¥¼ ê³ ìœ ë²¡í„°eigenvectorsì— ê³±í•˜ê³ , ê·¸ë ‡ê²Œ ê³±í•´ì§„ ë²¡í„° $Ax$ëŠ” ê¸°ì¡´ì˜ $x$ì— $\lambda$â€‹ë¥¼ ê³±í•œ ê²ƒì´ ëœë‹¤. 

ê¸°í•˜í•™ì ìœ¼ë¡œ ë´¤ì„ ë•Œ, í–‰ë ¬(ì„ í˜• ë³€í™˜) Aì˜ ê³ ìœ ë²¡í„°ëŠ” ì„ í˜•ë³€í™˜ Aì— ì˜í•´ ë°©í–¥Direction ì€ ë³´ì¡´ë˜ê³  ìŠ¤ì¼€ì¼ë§Œ ë³€í™”ë˜ëŠ” ë°©í–¥ë²¡í„°ë¥¼ ë‚˜íƒ€ë‚´ê³ , ê³ ìœ ê°’ì€ ê·¸ ê³ ìœ ë²¡í„°ì˜ ë³€í™”ë˜ëŠ” ìŠ¤ì¼€ì¼scale ì •ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°’ì´ë‹¤.

<br><img src="/blog/images/2024-05-15-TIL24_Day2/D56E8AC6-ED9C-4C75-A73E-1226E78128FF.jpeg" alt="day02out">
