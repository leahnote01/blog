---
title: "Day97 Deep Learning Lecture Review - Lecture 15 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Bias Mitigation Strategies: Loss Reweighting, Sampling & Synthetic Samples, 

<img src="/blog/images/2024-11-02-TIL24_Day97_DL/F1047164-2FEB-42F5-B178-C7E18D669DA0_1_105_c-2891978-2891985.jpeg">



## Four Main Strategies for Bias Mitigation

> The lecture introduces several techniques aimed at mitigating bias in machine-learning models.

1. Loss Reweighting
1. Sampling
1. Synthetic Samples

<center>
  <img src="/blog/images/2024-11-02-TIL24_Day97_DL/image-20241209093955730.png"><br><br>
</center>



### 1. Sampling and Reweigting Methods

- **Loss Reweighting**
  - We can re-weight the rarer samples during training for imbalanced datasets to make the system more robust. 
    - Assign higher loss weight for smaller populations.
  - It requires to have co-variate labels.
  - Often used for output labels.
- **Simple Sampling Strategies**
  - For each epoch, one could use any of these:
    - Over-sample the smaller population.
    - Under-sample the larger population.

  - These methods allow the model to see samples from the minority population more often, so it is better learned.
  - Sampling strategies are generally preferred over loss-based methods, <u>especially in vision, where we use data augmentation.</u> 
  - Two Choices:
    - **<u>Epoch level</u>**: Identify the samples to use at the start of an epoch so that they are more balanced.
    - **<u>Batch level</u>**: Ensure each mini-batch has the desired sample distribution so rarer subpopulations are present at the frequency we specify for each mini-batch.

  - Hard Example Mining
    - Rare samples often need to be more effectively learned by the model. We can **leverage** this to create mini-batches that help <u>reduce bias without bias variables.</u>
    - Do a forward pass on $N$ samples and compute the loss on each one of them.
    - Choose the $B$ samples with the highest loss for backprop, where $B<N$.
      - Pottentially . such that high loss samples are more likely to be chosen.

- **Synthetic Minority Over-Sampling Technique (SMOTE)**
  - Only works on **embeddings / vector data.**
  - Problem with sampling the original data is that it isn't giving the system any real variety, so address that by creating synthetic (fake) embeddings. 
  - For minority samples, find their nearest neighbors from the same population.
  - Create new samples that are the average of nearest neighbor pairs.<br><br>




### 2. Robust Risk Minimization Methods for Bias Mitigations

> Reduces the model's reliance on spurious correlations and ensures it focuses on features generalizable across 

- Techniques

  - **Group DRO** (Distributionally Robust Optimization)
    - Used to find parameters that **minimize the empirical worst-group risk**.
    - Enumerate all of your groups and find the parameters that minimize the worst-group training loss across groups.
      - Method is very sensitive to hyperparameters.
  - **Spectral Decoupling**
    - Spectral decoupling does not require bias variables and it aims to overcome bias **by attacking gradient**
      **starvation.**
      - <u>Gradient Starvation</u>: The tendency of the network to <u>rely on statistically dominanat features</u> (e.g., spurious variables) rather than all possible features.
    - Penalizes the model for overly relying on features with high variance (spurious features).
    - Encourages robustness by focusing on stable, core features across groups.<br><br>

  <center>
    <img src="/blog/images/2024-11-02-TIL24_Day97_DL/image-20241209132753132.png" width="80%"><br><br><br>
  </center>

  



### 3. Architectural Changes

> Incorporates inductive biases into model design to favor simpler, more generalizable solutions.

- Examples:
  - OccamNets: Architectures designed to prioritize simpler solutions (aligns with Occam’s Razor).
  - Adversarial Training:
    - Adds adversarial examples to training to improve model robustness.
  - Domain-Adversarial Neural Networks (DANN):
    - Ensures features are invariant to specific domains or sensitive attributes like race or gender.

<br>

#### 1. OccamNets

- Principle
  - Inspired by *Occam's Razor*, which suggests that simpler solutions are preferable unless complexity is justified by necessity. OccamNets are architectures designed to prioritize simplicity while maintaining predictive accuracy.
- How it works:
  - **Simpler hypothesis**: OccamNets introduce constraints or priors in model architectures to discourage overfitting to complex patterns
  - **Regularization**:
    - Models incorporate penalties (e.g., $L_1$ or $L_2$ regularization) to limit the complexity of learned weights.
    - Introduces **inductive biases that favor simpler decision boundaries.**
  - Advantages:
    - <u>Enhances generalization</u>: Models avoid fitting noise or <u>spurious correlations</u> in data.
    - <u>Reduces overfitting</u>: Prevents models from relying on <u>unnecessary complexities.</u>
  - Use case:
    - Medical imaging: Ensures predictions rely on medically relevant features rather than noise or artifacts in images.
  - Implementation techniques:
    - <u>Sparse architectures with dropouts.</u>
    - <u>Regularization techniques like weight decay and sparsity constraints.</u><br><br>

#### 2. Adversarial Training

Image Source: [Adversarial Machine Learning Mitigation: *Adversarial Learning*](https://towardsdatascience.com/adversarial-machine-learning-mitigation-adversarial-learning-9ae04133c137)

![image-20241209141011664](/images/2024-11-02-TIL24_Day97_DL/image-20241209141011664.png)

![image-20241209141022856](/images/2024-11-02-TIL24_Day97_DL/image-20241209141022856.png)





- **Principle**:
  - Models can be susceptible to adversarial attacks—small perturbations in inputs that drastically change outputs. Adversarial training strengthens model robustness by incorporating adversarial examples in the training process.

- **How it works**:

  - **Adversarial example generation**

    - Small perturbations ($\delta$) are added to input data $x$ <u>to maximize model loss</u>


    <center>
      $x' = x + \delta, \quad \text{where } \delta = \epsilon \cdot \nabla_x \ell(f_\theta(x), y)$ <br><Br>
    </center>

    - Here, $\epsilon$ controls the perturbation size, $\nabla_x$is the gradient, and $\ell$ is the loss.

  - Training loop

    - During each epoch, adversarial examples $x'$ are generated and included in training, making the model robust to these attacks.



- **Benefits**:
  - Improves generalization <u>to unseen data.</u>
  - Increases robustness <u>to real-world noise and variations.</u>

- **Use case**:
  - Fraud detection: Models trained adversarially are less likely to be deceived by sophisticated fraud attempts.
- **Challenges**:
  - Computationally expensive: Adversarial example generation increases training time.
  - May not address all types of bias: Focused on robustness <u>rather than fairness</u>.<br><Br>



#### 3. Domain-Adversarial Neural Networks (DANN)

- Principle:
  - Ensures that features learned by a model are invariant across domains or sensitive attributes (e.g., race, gender). This is crucial for fair decision-making.

- How it works
  - Network architecture
    - Consists of three components:
      1. Feature extractor: Learns shared representations across domains.
      2. Label predictor: Predicts the primary target variable $Y$.
      3. Domain classifier: Predicts the domain or group $D$ from which the data originates.
    - Adversarial objective:
      - The feature extractor minimizes the loss for the label predictor while maximizing the loss for the domain classifier. This creates domain-invariant features:

<center>
  $\min_{\text{Feature Extractor}} \max_{\text{Domain Classifier}} \mathcal{L}_D - \lambda \mathcal{L}_Y$ <Br><Br> 
  $\text{Here, } \mathcal{L}_D \text{ is the domain classification loss, } \mathcal{L}_Y \text{ is the target classification loss, and } \lambda \text{ balances the tradeoff.} $
</center>

- Benefits

  - <u>Mitigates bias from sensitive attributes by learning features that are independent of those attributes.</u>
  - Enhances fairness and robustness in multi-domain tasks.

- Use case

  - Hiring algorithms: Prevents models from leveraging features correlated with sensitive attributes like gender or race.

- Challenges

  - Performance tradeoff: Striving for domain invariance can slightly reduce performance on the main task.<br><br>

    

***When to Use These Techniques?***

1. **OccamNets**: Use <u>when datasets have spurious correlations</u> or when simpler, interpretable models are preferred.
2. **Adversarial Training**: Use for applications <u>where data or decisions can be adversarially manipulated</u> (e.g., cybersecurity).
3. **DANN**: Use for fairness-critical applications where biases related <u>to sensitive domains need to be removed.</u>











<br><Br>







<br><br>
