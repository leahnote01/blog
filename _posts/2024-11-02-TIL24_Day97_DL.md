---
title: "Day97 Deep Learning Lecture Review - Lecture 15 (2)"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, TIL_24]
toc: true 
---

# Bias Mitigation Strategies: Loss Reweighting, Sampling & Synthetic Samples, 

<img src="/blog/images/2024-11-02-TIL24_Day97_DL/F1047164-2FEB-42F5-B178-C7E18D669DA0_1_105_c-2891978-2891985.jpeg">



## Four Main Strategies for Bias Mitigation

> The lecture introduces several techniques aimed at mitigating bias in machine-learning models.

1. Loss Reweighting
1. Sampling
1. Synthetic Samples

<center>
  <img src="/blog/images/2024-11-02-TIL24_Day97_DL/image-20241209093955730.png"><br><br>
</center>



### 1. Sampling and Reweigting Methods

- **Loss Reweighting**
  - We can re-weight the rarer samples during training for imbalanced datasets to make the system more robust. 
    - Assign higher loss weight for smaller populations.
  - It requires to have co-variate labels.
  - Often used for output labels.
- **Simple Sampling Strategies**
  - For each epoch, one could use any of these:
    - Over-sample the smaller population.
    - Under-sample the larger population.

  - These methods allow the model to see samples from the minority population more often, so it is better learned.
  - Sampling strategies are generally preferred over loss-based methods, <u>especially in vision, where we use data augmentation.</u> 
  - Two Choices:
    - **<u>Epoch level</u>**: Identify the samples to use at the start of an epoch so that they are more balanced.
    - **<u>Batch level</u>**: Ensure each mini-batch has the desired sample distribution so rarer subpopulations are present at the frequency we specify for each mini-batch.

  - Hard Example Mining
    - Rare samples often need to be more effectively learned by the model. We can **leverage** this to create mini-batches that help <u>reduce bias without bias variables.</u>
    - Do a forward pass on $N$ samples and compute the loss on each one of them.
    - Choose the $B$ samples with the highest loss for backprop, where $B<N$.
      - Pottentially . such that high loss samples are more likely to be chosen.

- **Synthetic Minority Over-Sampling Technique (SMOTE)**
  - Only works on **embeddings / vector data.**
  - Problem with sampling the original data is that it isn't giving the system any real variety, so address that by creating synthetic (fake) embeddings. 
  - For minority samples, find their nearest neighbors from the same population.
  - Create new samples that are the average of nearest neighbor pairs.<br><br>




### 2. Robust Risk Minimization Methods for Bias Mitigations

> Reduces the model's reliance on spurious correlations and ensures it focuses on features generalizable across 

- Techniques

  - **Group DRO** (Distributionally Robust Optimization)
    - Used to find parameters that **minimize the empirical worst-group risk**.
    - Enumerate all of your groups and find the parameters that minimize the worst-group training loss across groups.
      - Method is very sensitive to hyperparameters.
  - **Spectral Decoupling**
    - Spectral decoupling does not require bias variables and it aims to overcome bias **by attacking gradient**
      **starvation.**
      - <u>Gradient Starvation</u>: The tendency of the network to <u>rely on statistically dominanat features</u> (e.g., spurious variables) rather than all possible features.
    - Penalizes the model for overly relying on features with high variance (spurious features).
    - Encourages robustness by focusing on stable, core features across groups.

  <center>
    <img src="/blog/images/2024-11-02-TIL24_Day97_DL/image-20241209132753132.png" width="80%"><br><br><br>
  </center>

  



### 3. Architectural Changes

> Incorporates inductive biases into model design to favor simpler, more generalizable solutions.

- Examples:
  - OccamNets: Architectures designed to prioritize simpler solutions (aligns with Occamâ€™s Razor).
  - Adversarial Training:
    - Adds adversarial examples to training to improve model robustness.
  - Domain-Adversarial Neural Networks (DANN):
    - Ensures features are invariant to specific domains or sensitive attributes like race or gender.











<br><br>
