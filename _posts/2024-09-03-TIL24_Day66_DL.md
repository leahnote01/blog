---
title: "Day66 Deep Learning Lecture Review"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, mathematicsReview, matrix, linearAlgebra, TIL_24]
toc: true 
---

# Lecture 1: Review of Background Knowledge

<img src="/blog/images/2024-09-03-TIL24_Day66_DL/C0250DA4-3C36-45E5-BA8E-A60FEA5E2233.jpeg"><br><br>

> We will explore the intricate intricacies of "The End-to-End Deep Learning lecture of the 2024 Fall at the University of Rochester, diving into its potential and envisioning how it can revolutionize our understanding of the world. I expect to unravel the complexities and breakthroughs of this captivating field; we will navigate through the profound implications and transformative applications across various industries. Join me on this journey to review all concepts from the lectures.

<br>

### 1. Background Math

- Vector Norm $\Vert \mathbf{v} \Vert $
  - A vector norm is a function that takes a vector as input and outputs a single non-negative real number to show how significant that number is.
  - Used for many ML activities
    - Training systems(loss functions) / Regulation / Error Measurement
  - $L^p$ Vector Norm:  $\Vert \mathbf{x} \Vert_p = \big({\sum_{i=1}^n x_i^2} \big)^{\frac{1}{p}}$
    - $L^2$ Norm (Euclidean Norm):  $\Vert \mathbf{x} \Vert_2 = \sqrt{\sum_{i=1}^n x_i^2}$ (most common)
    - $L^1$ Norm (Manhattan Norm):  $\Vert \mathbf{x} \Vert_1 = \sum_{i=1}^n {\vert x_i \vert} $

<br>

* Matrices as Linear Transformation
  * Matrix multiplication can be used to transform vectors in useful ways, e.g., $\text{y=Ax}$
  * This is crucial to understand. Much of the class involves finding $\text{A}$ matrices to do a particular linear transformation.

<br>

* Tensors

  * Multi-dimensional arrays

  <center>
    <img src="/blog/images/2024-09-03-TIL24_Day66_DL/image-20240904150915892.png" width="80%"><br><br>
  </center>
  
  * Color images are typically stored as $n \times m \times 3$ tensors (arrays) - Red, Green, and Blue brightness values. 

<br>

* Exponential Function exp(x)

  * $e^x = \exp(x)$

  <center>
    <img src="/blog/images/2024-09-03-TIL24_Day66_DL/image-20240904152926650.png" width="70%">
  </center>

  

  * The exponential function $\exp(x)$ is defined as the power to which the number $e$ (approximately 2.71828, and known as Euler's number) must be raised to yield $x$. Thus, $e^x = \exp(x)$.

  * Differentiability and Continuity:

    * Continuous and differentiable across the entire real line.
       - The derivative of $\exp(x)$ is $\exp(x)$ itself, which is a unique property and makes it extremely useful in differential equations and calculus.
       - $K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$ <br>

    

<br>

- Multivariate Gaussian / Normal Distribution
  - $m$ \indicates the mean of the $d$ -dimensional Gaussian}
  - $\Sigma$ indicates the covariance matrix
    - $f(\mathbf{x}) = \frac{1}{\sqrt{(2 \pi)^d \vert \Sigma \vert}} \exp\left(-\frac{1}{2} (\mathbf{x} - \mathbf{m})^T \Sigma^{-1} (\mathbf{x} - \mathbf{m})\right)$

<br>

- Vectorization for Making Code Fast
  - Avoid for-loops if possible
  - Try to directly implement algorithms using matrix and vector operations
  - Parallelization is always important

<br><br>

### 2. Supervised Machine Learning

- After seeing a bunch of examples (Input space- $\mathbf x$, Output Space-y), pick a mapping $F:\mathbf x \rightarrow y$ that accurately replicates the input-output pattern of examples



