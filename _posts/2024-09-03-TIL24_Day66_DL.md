---
title: "Day66 Deep Learning Lecture Review"
layout: single
classes: wide
categories: TIL_24
read_time: True
typora-root-url: ../
tag: [dlReview, mathematicsReview, matrix, linearAlgebra, TIL_24]
toc: true 
---

# Lecture 1: Review of Background Knowledge

<img src="/blog/images/2024-09-03-TIL24_Day66_DL/C0250DA4-3C36-45E5-BA8E-A60FEA5E2233.jpeg"><br><br>

> We will explore the intricate intricacies of "The End-to-End Deep Learning lecture of the 2024 Fall at the University of Rochester, diving into its potential and envisioning how it can revolutionize our understanding of the world. I expect to unravel the complexities and breakthroughs of this captivating field; we will navigate through the profound implications and transformative applications across various industries. Join me on this journey to review all concepts from the lectures.

<br>

### 1. Background Math

- Vector Norm $\Vert \mathbf{v} \Vert $
  - A vector norm is a function that takes a vector as input and outputs a single non-negative real number to show how significant that number is.
  - Used for many ML activities
    - Training systems(loss functions) / Regulation / Error Measurement
  - $L^p$ Vector Norm:  $\Vert \mathbf{x} \Vert_p = \big({\sum_{i=1}^n x_i^2} \big)^{\frac{1}{p}}$
    - $L^2$ Norm (Euclidean Norm):  $\Vert \mathbf{x} \Vert_2 = \sqrt{\sum_{i=1}^n x_i^2}$ (most common)
    - $L^1$ Norm (Manhattan Norm):  $\Vert \mathbf{x} \Vert_1 = \sum_{i=1}^n {\vert x_i \vert} $

<br>

* Matrices as Linear Transformation
  * Matrix multiplication can be used to transform vectors in useful ways, e.g., $\text{y=Ax}$
  * This is crucial to understand. Much of the class involves finding $\text{A}$ matrices to do a particular linear transformation.

<br>

* Tensors

  * Multi-dimensional arrays

    <img src="/blog/images/2024-09-03-TIL24_Day66_DL/image-20240904150915892.png" width="60%"><br>

    
